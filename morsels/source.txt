:d:A list of game design concepts.
:k:Game Design, Game Mechanics, Game Narratives, Technical Game Design, Game AI, Gameplay Systems, Game Development, Mechanical Narratives, Gameplay Algorithms, Gameplay Semantics, Game Logic
:l:2024-07-26

[Game Design Concepts] June 27, 2024 - July 26, 2024

This is a collection of game design concepts.

@@<hr>
@@<div class="l_spacer"></div>
<001>
@@<h3><b>1. Mechanical Narratives</b></h3>

A game is usually a combination of two factors - mechanics and narratives.

Mechanics represent the systematic aspects of the game, such as physics, AI, crafting rules, progression curves, and others which rely on the knowledge of math/engineering.

Narratives represent the volitional aspects of the game, such as lores, stories, personalities, goals, and others which rely on the knowledge of arts/humanities.

A problem we often experience is that it is too tricky to fit these two together in one place. Many ambiguous cases are prone to arise, such as:

(1) Having a cool story (narrative) for a video game, but not knowing how to turn it into gameplay (mechanic).
(2) Knowing how to implement a clever enemy AI (mechanic), but not knowing how to design an appropriate enemy character for it (narrative).
(3) Having an interesting NPC with a unique personality (narrative), but not knowing which in-game role it ought to play (mechanic).

A solution is to start designing the game with building blocks which can both be considered mechanics and narratives at the same time (aka "mechanical narratives").

If we start by laying out the game's narratives first, it will be difficult to devise mechanics which are compatible with the given narratives. If we start by laying out the game's mechanics first, on the other hand, it will be difficult to devise narratives which are compatible with the given mechanics.

Such a dilemma can be bypassed by simply collecting a number of "mechanical narratives" and assembling them together.

@@<hr>
@@<div class="l_spacer"></div>
<002>
@@<h3><b>2. Sensors, Filters, and Motors</b></h3>

In general, a gameplay agent can be constructed by assembling 3 types of modules - sensors, filters, and motors.

Sensors collect data from the environment, filters process the data (for analysis), and motors trigger the agent to take actions based upon the filtered data.

Here is an example. Imagine that there is an anti-aircraft gun whose role is to shoot down enemy airplanes in range. We can model this gun as a composition of:

(1) A sensor (i.e. radar) which detects anything within the specified range,
(2) A filter (i.e. computing module) which looks up the database to see if the detected object is an enemy airplane, and
(3) A motor (i.e. the gun itself) which fires bullets at any object which is identified as an enemy airplane.

The gun detects an aircraft, checks to see if it is an enemy, shoots at it if so, and then detects another aircraft, checks to see if it is an enemy, shoots at it if so, and then detects another aircraft... and so on.

This is just a simple example. For more complex behaviors, you may as well equip it with multiple sensors (e.g. radar, camera, antenna), multiple filters (e.g. distance filter, look-angle filter), multiple motors (e.g. gun, horizontal rotor, vertical rotor, ammunition reloader), etc.

Such a multitude of modules can then communicate with one another by means of series connections, parallel connections, or both.

@@<hr>
@@<div class="l_spacer"></div>
<003>
@@<h3><b>3. Psychological Force Field</b></h3>

Psychological force field is a useful concept in game design.

We typically imagine each individual game character as an observer, equipped with its own independent mind. It continuously scans the environment and makes decisions off of it.

Each observer carries its own psychological force field which permeates the entirety of the game world. Every entity other than the observer itself essentially "warps" the field nearby, modifying the magnitudes and directions of the force vectors which guide the observer's movement.

A repulsive entity adds outward forces to the field, whereas an attractive entity adds inward forces to the field. This makes the observer move away from repulsive entities and closer to attractive entities.

Whether an entity is repulsive or attractive is determined by the observer's personal preference.

The force that is to be applied to the observer can be computed by the function "PsyForce(id, X, Y)", where "id" is the unique ID of the observer and (X, Y) is the observer's current position. This function returns the psychological force vector which gets added to the observer's net force.

A real life example of psychological force field is Feng Shui, where architectural patterns are believed to shape the field in which the residents of the house reside, thereby influencing the way they behave.

@@<hr>
@@<div class="l_spacer"></div>
<004>
@@<h3><b>4. Parallel-Attention Timeline</b></h3>

A parallel-attention timeline is a pretty useful tool to use in gameplay AI.

It starts from a simple analogy.

When I am stirring a pot of spaghetti while also heating a plate of frozen garlic bread in the oven, you can say that I must be multitasking.

However, it should be noted that I am paying way more attention to the pot of spaghetti than to the oven because the latter basically takes care of itself and only demands occasional supervision.

When designing a game, it is convenient to assume that each game character carries its own schedule in its mind, filled with tasks which start and end at their own designated points in time.

It is sometimes even more desirable, though, to model a schedule not as a one-dimensional queue of tasks, but as a two-dimensional grid in which each column represents a time slot and each row represents what could be referred to as "attention priority".

A character which follows this two-dimensional schedule always runs the task which occupies the highest row (i.e. highest attention priority) among all the tasks which occupy the current time slot.

Whenever an "attention trigger" (i.e. any object which demands attention) tries to register a new task to the schedule, the schedule will either ignore this new task if it happens to collide with an existing one, or proceed to register it if not.

Each task possesses its own intrinsic attention priority (i.e. row) which cannot be modified.

@@<hr>
@@<div class="l_spacer"></div>
<005>
@@<h3><b>5. Partial Movement Constraints</b></h3>

Both too much freedom and too little freedom are undesirable in gameplay.

Suppose that there are enemy characters whom the player must defeat in order to finish the level.

If the enemies are able to move in any direction, it will be a bit too bland because it reduces the functional significance of the level's peculiar spatial configuration (e.g. If you are being chased by an enemy who can only move horizontally, you can take refuge in a vertical space).

If the enemies are completely immobile, on the other hand, it will be pretty bland as well because it makes gameplay less dynamic.

A nice middle ground which leverages the benefits of both is to confine each enemy's locomotive freedom to a set of simple pathways, as though it is a train following a railway.

This mixed approach has 3 main advantages:

(1) It makes it easy to diversify enemy behaviors and introduce a wide variety of in-game strategies.
(2) Its pathfinding algorithm is easier to implement and way less computationally expensive than, say, the A-Star algorithm.
(3) It prevents extreme gameplay scenarios, such as letting the player be completely surrounded by a truckload of enemies because they always keep chasing the player without a pause. By limiting each enemy's maximum range of movement, we can spread out the distribution of enemies throughout the level and prevent them from concentrating too much in one place.

@@<hr>
@@<div class="l_spacer"></div>
<022>
@@<h3><b>6. Emotional State Machine</b></h3>

When it comes to designing gameplay systems, it is often useful to model each game character as a state machine (i.e. an object with a set of possible states). There are more advanced models indeed, but state machines usually suffice for fairly simple purposes.

Such a state machine typically comprises the character's action states, such as "idle", "attacking", "stunned", "wandering", and so on. However, this representation does not reflect the character's inner psychological states such as emotions.

In order to give depth to the game's narratives, we must let game characters have their own emotions. And this can be done by designing each character as an emotional state machine.

An emotional state machine resides in a 3D space called "emotion space", whose 3 spatial axes represent the character's happiness, excitement, and confidence.

The characters's emotional state is a point in the emotion space, and a change in the emotion is the same thing as a displacement of the point from one location to another.

When happiness, excitement, and confidence are all low, the character is depressed and timid. When happiness, excitement, and confidence are all high, the character is fascinated and arrogant. When happiness is high but excitement and confidence are low, the character is quietly happy in its selfless devotion. And when happiness and excitement are low but confidence is high, the character is in the mood of Squidward.

You can change the emotion's current state by applying a "psychological force" to it. The way you do it is to put the emotion's position under a psychological force field.

@@<hr>
@@<div class="l_spacer"></div>
<023>
@@<h3><b>7. Deities of the Game World</b></h3>

Designing a game is essentially the same thing as designing your own virtual world.

A game is a world which is governed by its own set of natural laws. And the manner in which these laws are being enforced largely depends on the manner in which the developer has initially configured the world.

Game worlds come in different types, each of which pertains to a particular theistic worldview.

Inside a monotheistic game world, a single control module regulates the feedback loops of the game characters.

Inside a polytheistic game world, multiple control modules regulate the feedback loops of the game characters.

Inside a pantheistic game world, the game characters' feedback loops are not being regulated by any external module; the characters themselves are their own regulators.

One of the main advantages of using either a monotheistic or polytheistic system is that it allows the gamemaster to occasionally override the game's default laws by means of admin privilege. In computer science, it is called "miracle".

@@<hr>
@@<div class="l_spacer"></div>
<025>
@@<h3><b>8. Modular Behavior Tree</b></h3>

Behavior trees are useful in game AI. You can implement a wide range of complex decision-making agents based upon them.

Sometimes, however, we feel overwhelmed by the urge to make a behavior tree unreasonably enormous when there are way too many types of items with which the agent is supposed to interact.

For example, imagine that a game character has a behavior tree which involves a task called "eat". In order to tell the character exactly what to do when it runs this task, the "eat" node must be able to handle all possible ways of eating, such as how to eat a salmon, how to eat a donut, how to eat a taco, and so on.

Thus, it is oftentimes convenient to make behavior trees modular, so that a tree of tasks that is specific to the object of interaction can simply reside within the object, temporarily stick itself to the character's "eat" task during the interaction, and then depart once the interaction is over.

@@<hr>
@@<div class="l_spacer"></div>
<026>
@@<h3><b>9. Brand-Building</b></h3>

One thing I learned while developing and publishing indie games, was that a game needs to have its own brand in order to be truly successful.

Creating a brand is such a monumental challenge. One cannot just put a bunch of buzzwords together and expect it to leave a lasting impact upon the heart of the audience. In my opinion, a great brand requires 3 major elements: Purpose, Logic, and Mystery.

A brand needs a purpose because otherwise the customer won't be encouraged to partake in its narrative. Without a purpose, there is no need to support the brand's growth by any means.

A brand cannot flourish without any logic either. It must have a logically sound solution to fulfill the said purpose. Our reason must be able to grasp it, for otherwise everybody will be left confounded.

However, a brand also ought to preserve a decent amount of mystery in it. Even a perfectly logical solution, guaranteed to solve the most urgent problem in the most quintessential way, won't attract the audience if they are not given their own roles to participate in a journey to freely explore the unknown and discover hidden treasures.

In general, I believe that the most appealing brand must have its own purpose, logic, and mystery in equal measures. This corresponds to the center point of the circular diagram I have shown here (denoted by the Sun).

@@<hr>
@@<div class="l_spacer"></div>
<043>
@@<h3><b>10. Image as a Game Map</b></h3>

Using an image to generate a game map is sometimes a good idea.

Suppose that your game comprises one vast terrain, modeled as a 2D voxel grid.

All you need to do is create an image and simply assume that its pixels represent to the terrain's voxels.

Each pixel's brightness may indicate the terrain's height. This can be graphically implemented by rendering the terrain as a grid of quads and letting the vertex shader set the y-coordinate of each vertex based on the corresponding pixel's brightness.

Each pixel's saturation may indicate the density of props, where low saturation means low probability of spawning a prop at the pixel's location, and high saturation means high probability of spawning a prop at the pixel's location.

Lastly, each pixel's hue may indicate the terrain's biome type, where "green" means forest/meadow, "orange" means village, "magenta" means palace, and so on.

Additional data-embedding is possible, too, as long as your image is transparent. The opacity of each pixel can be used to specify some other physical properties of the terrain, such as temperature, humidity, etc. Or it may be used for metadata, in which the game's trigger zones, waypoints, and spawn hotspots can be encoded.

With this image-based map generation system, you can even manipulate the terrain while playing the game because it is just a matter of image processing (run by fragment shaders). The overall erosion of the terrain is the same thing as blurring the image, and hitting the terrain with a meteorite is the same thing as painting the impact zone with a semi-transparent black brush.

@@<div class="l_spacer"></div>
@@<hr>
@@<div class="l_spacer"></div>
(To be continued...)






:d:A mathematical model of our own thoughts, feelings, and the sense of curiosity.
:k:Curiosity, Motivation, Motive Force, Curiosity Force, Conceptual Space, Physical Space, Katarina Gyllenbäck, Feature Space, Idealism, Ontology, Epistemology
:l:2024-07-27

[Model of the Mind] June 28, 2024 - July 27, 2024

This is a collection of mathematical concepts designed to express the nature of human mind in a rational manner.

The ideas shown below are inspired by Katarina Gyllenbäck's articles. For more information, visit <a href="https://thingspool.net/read-rec/page-2.html">Here</a>.

@@<hr>
@@<div class="l_spacer"></div>
<006>
@@<h3><b>1. Physical Space and Conceptual Space</b></h3>

There are two different spaces in our universe - physical space and conceptual space.

Physical space is the space we usually refer to whenever we are employing the word "space". It consists of spatial and temporal axes (e.g. X, Y, Z, and Time), and serves as a frame of reference when it comes to describing physical entities such as rigid bodies.

Conceptual space, on the other hand, is the space of qualitative features. For example, a color is a position in the RGB color space, where the axes (R, G, B) represent the intensity levels of the three primary color components (i.e. Red, Green, Blue). The RGB color space is one of many types of conceptual spaces we can imagine.

Each object has its own position in physical space as well as a position in conceptual space. The former tells us where the object is, and the latter tells us what the object looks like.

@@<hr>
@@<div class="l_spacer"></div>
<007>
@@<h3><b>2. Particulars and Universals</b></h3>

There are two types of objects - particulars and universals.

A particular is an object which exists at a specific point in space and time. All tangible objects, such as a table we can touch, a sandwich we can eat, a cup we can hold, and countless other "real" things, can be classified as particulars.

A universal is different from a particular in the sense that we cannot directly perceive it, since it exists only in conceptual space and not in physical space. It is a pure idea which resides only in the spiritual realm; it does not belong to anywhere in our material world.

In computer science, a class is a universal. It is purely spiritual because it lives in the static memory, which is fixed and therefore "eternal" in the sense that it spans the entirety of the application's runtime.

An instance of a class, which is dynamically allocated in memory, is a particular. It is a mortal being which gets created at some point in time and gets destroyed at some other point in time. It is never eternal because it does not span the entirety of the application's runtime.

@@<hr>
@@<div class="l_spacer"></div>
<008>
@@<h3><b>3. Contiguity and Resemblance</b></h3>

Since an object exists in two different spaces (namely, "physical space" and "conceptual space"), a distance between a pair of objects can possess either one of two meanings depending on the type of space in which it is measured.

Physical proximity implies contiguity. When two things are very close to each other in physical space, we say that they are contiguous because looking at one of them lets us look at the other much more easily.

Conceptual proximity implies resemblance. When two things are very close to each other in conceptual space, we say that they resemble each other because thinking of one of them lets us think of the other much more easily.

Proximity can be measured by taking the reciprocal of the distance. Less distance means more proximity, and more distance means less proximity.

@@<hr>
@@<div class="l_spacer"></div>
<009>
@@<h3><b>4. Surprise</b></h3>

How to measure the amount of surprise between two objects?

Each object has two distinct positions - one in physical space, and the other one in conceptual space. Therefore, we are able to compute two separate distances between a pair of objects - physical distance and conceptual distance.

The amount of surprise between two objects is proportional to their conceptual distance because the more qualitatively different they are, the more surprised they will be when they see each other.

On the other hand, the amount of surprise between two objects is inversely proportional to their physical distance because the closer they are, the more vividly they will observe each other's qualitative differences.

Thus, we can measure the amount of surprise by measuring the conceptual distance and then dividing it by the physical distance.

@@<hr>
@@<div class="l_spacer"></div>
<010>
@@<h3><b>5. Curiosity Force</b></h3>

Curiosity often drives us to move from familiar places to unfamiliar places. It is possible to devise a mathematical formula which tells us exactly how the force of curiosity will initiate such a movement.

Suppose that there is an observer somewhere in physical space. We can first compute the observer's "surprise vectors", each of which represents the amount of surprise that the observer feels in regard to an external object.

If you take the sum of all the surprise vectors which involve the observer and multiply the resulting "net surprise vector" by the amount of the observer's curiosity, you will obtain the "curiosity force vector" which is responsible for pushing the observer to venture into the unknown.

@@<hr>
@@<div class="l_spacer"></div>
<016>
@@<h3><b>6. How to Compute Curiosity</b></h3>

Can we represent curiosity as a numerical quantity?

The amount of curiosity reaches its peak value when the person is feeling a sense of surprise which is neither too intense nor too dim.

If you are surrounded by an environment which is not surprising at all, you will lose curiosity due to boredom.

If you are surrounded by an environment which is too overwhelmingly surprising, you will be anxious. As a result, you will suppress your curiosity in order to protect yourself from potential dangers.

It is only when you are surrounded by a moderately surprising environment (i.e. neither too familiar nor too unfamiliar) that you will be able to feel a vivid sense of curiosity.

A halfway mixture between familiar and unfamiliar elements creates a "Goldilocks zone of motivation" which will encourage you to uncover partially hidden secrets.

If everything is already uncovered, there will be no secret to uncover and so you won't feel the necessity to start an adventure. If everything is veiled in darkness, you will feel clueless and thus not even dare to start an adventure.

@@<hr>
@@<div class="l_spacer"></div>
<046>
@@<h3><b>7. Uniformity</b></h3>

It is possible to measure the amount of uniformity between two objects.

In order for a pair of objects to comprise one uniform body, they must satisfy two conditions.

First, they must be physically close to each other. If they are too far apart, we will clearly be able to see that there is a significant spatial gap between them, showing that they are discontinuous and thus not uniform.

Secondly, they must resemble each other (i.e. similar in characteristics). If they look too drastically different, we will be able to tell that their mutual contrast is too huge to ensure that they are part of one smooth, uniform body.

Therefore, the amount of uniformity can be computed by taking the inverse of the product between their physical and conceptual distances.

@@<hr>
@@<div class="l_spacer"></div>
<047>
@@<h3><b>8. Force of Adaptation</b></h3>

If you measure the amount of uniformity between the observer and a nearby object, you will obtain a number which tells you how familiar the observer is with the object.

And if you represent this number as the magnitude of a vector which starts at the observer's conceptual location and points itself to the object's conceptual location, you will get a vector which can be referred to as a "uniformity vector".

Take the sum of all uniformity vectors which originate from the observer, and you will acquire the net uniformity vector. This vector informs us the strength and direction of the surrounding environment's familiarity with respect to the observer.

Scale this net uniformity vector by the observer's adaptivity (which is a scalar value) and you will obtain the force vector which is currently pushing the observer's viewpoint in conceptual space. This force gradually "adapts" the observer to the surroundings, making them become more and more familiar as time passes by.

@@<hr>
@@<div class="l_spacer"></div>
<048>
@@<h3><b>9. Dynamics of Exploration</b></h3>

The force of curiosity pushes the observer's body in physical space, toward areas which are unfamiliar to him.

Meanwhile, the force of adaptation pushes the observer's viewpoint in conceptual space, toward areas which are as familiar to him as possible. This lets him quickly adapt himself to his surroundings.

These two forces work together in parallel, continuously propelling both the observer's body (location in physical space) and viewpoint (location in conceptual space) in the direction of spontaneous exploration.

The back-and-forth interaction between these two forces creates a feedback system. The force of curiosity puts the observer in unfamiliar places, which in turn compels the observer to adapt his viewpoint to the unfamiliar. Once his surroundings become too familiar to him, his curiosity then drives him to search for other unfamiliar territories to explore.







:d:A mathematical interpretation of David Hume's philosophy.
:k:David Hume, Empiricism, Empirical Philosophy, Metaphysics, Epistemology, Ontology, Discrete Math, Philosophy Of Mind, Computational Psychology
:l:2024-07-23

[Mathematical Interpretation of Hume's Philosophy] July 2, 2024 - July 23, 2024

This is a mathematical interpretation of David Hume's philosophy. It is mostly based upon my personal analysis, so please take it with a grain of salt.

I drew most of my inspirations from his book, "An Enquiry Concerning Human Understanding". For more information, visit <a href="https://thingspool.net/read-rec/page-6.html">Here</a>.

@@<hr>
@@<div class="l_spacer"></div>
<011>
@@<h3><b>1. Introduction</b></h3>

Hume's empiricist worldview begins with a hierarchy of concepts.

There are two major types of components in his domain of knowledge. One is "perceptions", and the other one is "objects of reason". Perceptions are the atoms of the mind, whereas objects of reason are molecules which can be formed by bonding those atoms together.

A perception is any "thought" we can conceive in our minds. It belongs to either one of the two categories - impressions and ideas.

An impression is a direct stimulus received by our sense organs. Things we directly see, hear, smell, taste, and feel are all impressions.

In contrast, an idea is an afterthought on the impressions we received. For example, what we see is an impression, but the recollection of what we just saw is an idea. Impressions are vivid, while ideas are dim.

Objects of reason can be subdivided into two categories - "relations of ideas" and "matters of fact".

A relation of ideas is a result of pure logic, such as a mathematical theorem which manages to prove itself based upon a set of ideas only, without relying on the presence of external stimuli. A matter of fact, on the other hand, requires a considerable amount of empirical data to validate itself (like the law of gravitation).

@@<hr>
@@<div class="l_spacer"></div>
<012>
@@<h3><b>2. Impressions and Ideas</b></h3>

Hume's definition of "impressions" and "ideas" provides us with a rudimentary ground of logic, upon which we can formulate a model of how we sense and recall our own thoughts.

Impressions are what we typically refer to as "sensory stimuli". These are the most immediate and piercing kind of perceptions which we feel with the utmost degree of intensity.

Ideas, on the other hand, are byproducts of impressions. They linger in our minds like ghosts, which occasionally touch our feelings but to a much lesser degree. Unlike impressions which are momentary, ideas stay in the person's memory and get recalled on demand.

In a way, therefore, ideas are Lego bricks which can fit one another nicely in our minds, whereas impressions are just raw plastic ingredients which are yet to be molded into such bricks.

@@<hr>
@@<div class="l_spacer"></div>
<013>
@@<h3><b>3. Generation of Ideas</b></h3>

According to Hume's philosophy, each impression generates its corresponding idea when it enters our domain of cognition.

However, he also mentions that the relation between impressions and their ideas is not necessarily one-to-one. There may as well be cases in which a wide spectrum of ideas manage to emerge from a relatively few impressions.

An impression of "light blue" and an impression of "dark blue", for example, may allow us to imagine an intermediate shade of blue (by mixing the qualities of light blue and dark blue) and remember it as a distinct idea. This lets us picture the full spectrum of blue without having to directly sense every single one of its variants via external stimuli.

@@<hr>
@@<div class="l_spacer"></div>
<014>
@@<h3><b>4. Relations between Ideas</b></h3>

We assign meaning to our ideas by establishing relations between them. In Hume's model of the human mind, there are three fundamental types of such relations - resemblance, contiguity, and causality (aka "cause and effect").

Resemblance tells us that two ideas are qualitatively similar to each other (such as two slightly different shades of blue). This is something we can immediately tell from our perceptions.

Contiguity tells us how close two ideas are to each other in spacetime. This, too, is something we can immediately tell from our perceptions.

Causality, however, is not something we can identify in such a straightforward manner. We must derive it from the other two types of relations (i.e. resemblance and contiguity).

@@<hr>
@@<div class="l_spacer"></div>
<015>
@@<h3><b>5. Causal Relations</b></h3>

An idea alone does not have the word "cause" or "effect" written on its face, and the only way for us to prove that an idea "causes" another idea is that one of them frequently precedes (or succeeds) the other in spacetime.

Also, since it is almost impossible for us to reproduce the same exact idea over and over in our material world (which is plagued with all sorts of random noises such as acoustic waves, electromagnetic disturbances, thermal noise, etc), we must rely on our belief that similar causes are likely to produce similar effects.

This leads us to the conclusion that, if a set of mutually resembling ideas are contiguous with another set of mutually resembling ideas, we can say that these two sets are causally related to each other.

@@<hr>
@@<div class="l_spacer"></div>
<017>
@@<h3><b>6. Facts</b></h3>

What is a "fact", really?

There are many ideas in our domain of reason, yet the boundary between ones that are supposed to be "facts" and ones that are mere personal feelings often looks a bit fuzzy.

Hume's philosophy tells us that an idea can be considered a "fact" if it is caused by a sufficiently large number of causes (e.g. preceded by a sufficiently long chain of causal relations).

Each cause works as a proof of the idea's truthfulness. The more causes there are, the more confidently we are able to say that the idea is true. If it is so true that its truthfulness is hardly questionable, we say that the idea is a "fact".

People who know how a blockchain works (such as Bitcoin or Ethereum) will easily come to the realization that ideas are reminiscent of blockchain transactions, and that a "fact" is basically a transaction which has been mined (verified) and is given credit by a long chain of preceding blocks.

@@<hr>
@@<div class="l_spacer"></div>
<027>
@@<h3><b>7. Simple and Complex Ideas</b></h3>

In "A Treatise of Human Nature", Hume makes a clear distinction between simple and complex ideas.

Based upon his rejection of the infinite divisibility of our sense-data, he says that "simple ideas" are ideas which cannot be further divided into its component parts, meaning that they are the "atoms" of our mind.

Examples of simple ideas include the primary color components (e.g. red, green, and blue), musical notes, and other irreducible units of sensation.

"Complex ideas", on the other hand, are groups of simple ideas. Each complex idea is a product of synthesis among two or more simple ideas. It is also possible to group complex ideas to form even more complex ideas.

A simple impression (i.e. external stimulus) gives birth to a simple idea. A multitude of simple ideas, then, give birth to complex ideas based on the way in which they are related to each other.

@@<hr>
@@<div class="l_spacer"></div>
<028>
@@<h3><b>8. Abstraction and Combination</b></h3>

A group of simple ideas are able to produce a complex idea. A group of complex ideas, too, are able to produce yet another complex idea which is even more complex than the preceding ones.

In general, therefore, we can say that a group of ideas with appropriate relations, whether they are simple or complex, are capable of generating a complex idea whose level of complexity is slightly higher than them.

There are two ways in which a complex idea may emerge - abstraction and combination.

When you see a group of slightly different shades of blue, you will recognize that they closely resemble each other. From this, you can conclude that they all belong to a class of objects called "blue". This is an example of abstraction.

When you see a lump of various colors, on the other hand, you will recognize that the individual colored spots do not necessarily resemble each other but are nevertheless very closely packed together in physical space. From this, you can conclude that they all belong to the same object. This is an example of combination.

@@<hr>
@@<div class="l_spacer"></div>
<029>
@@<h3><b>9. Relations between Complex Ideas</b></h3>

There are two primary types of relations between ideas - resemblance and contiguity. Resemblance indicates conceptual proximity, and contiguity indicates physical proximity.

Just like resemblance and contiguity can connect simple ideas, they are able to connect complex ideas as well.

An object is a complex idea which is based off of a set of contiguous ideas. When two objects have sufficiently many pairs of resembling ideas between them, they resemble each other.

A class is a complex idea which is based off of a set of resembling ideas. When two classes have sufficiently many pairs of contiguous ideas between them, they are contiguous to each other.

A contiguity between two classes may also be referred to as "causality" (i.e. One of them is either the "cause" or "effect" of the other).

@@<hr>
@@<div class="l_spacer"></div>
<030>
@@<h3><b>10. Data Structure of an Idea</b></h3>

According to Hume's argument in "A Treatise of Human Nature", every idea (or impression) must possess its own quality and quantity.

An idea's quality denotes the category of sensation to which it belongs, such as "brightness", "musical pitch", or "temperature".

An idea's quantity denotes its quality's intensity level. For instance, if there is an idea whose quality is "temperature" and whose quantity is "30", we may consider this idea as a sensation of warmth which is 30 degrees in temperature.

Hume says that an impression is basically the same thing as an idea (i.e. It has its own quality and quantity), except that it is much higher in "force and vivacity".

Therefore, I personally find that it is much more convenient to consider an impression as an idea whose level of vivacity is sufficiently high.

When expressed in the language of computer science, both impressions and ideas are mere instances of the same data structure called "Idea", which contains 3 integers - Quality, Quantity, and Vivacity.

@@<hr>
@@<div class="l_spacer"></div>
<031>
@@<h3><b>11. Metaphor</b></h3>

Metaphors are everywhere. They appear in poetry, novels, songs, and many other forms of media. However, they are often accused of being irrational.

Hume's philosophy suggests otherwise. It is possible to define a metaphor on a rational basis.

An idea has a location in conceptual space, which is composed of two dimensions called "quality" and "quantity". Quality indicates the category of sensation (e.g. brightness, loudness, temperature, etc), and quantity indicates its level of intensity.

We say that there is a resemblance between two ideas when they are close to each other in conceptual space. There are two types of resemblances: (1) Resemblance in quality, and (2) Resemblance in quantity.

Coldness and hotness do not resemble each other quantitatively, since their intensity levels (i.e. temperature) differ significantly. However, they resemble each other qualitatively because they both belong to the same category called "temperature".

On the contrary, red and hotness do not resemble each other qualitatively because they belong to two drastically different categories of sensation. However, they resemble each other quantitatively because the high emotional intensity of red is similar in magnitude to the high temperature of hotness. This is what we call "a metaphor".

@@<div class="l_spacer"></div>
@@<hr>
@@<div class="l_spacer"></div>
(To be continued...)







:d:Possible usage of Peter Gärdenfors' two-vector event model in the design of gameplay systems.
:k:Peter Gärdenfors, Cognitive Science, Causality, Causation, Causal Loops, Systems Thinking, System Dynamics, Game Design, Game Mechanics, Game Systems, Philosophy, Epistemology, Russell, Hume, Gameplay Systems Design, Technical Design, Lund University
:l:2024-07-28

[Game Design using Gärdenfors' Event Model] July 28, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Introduction</b></h3>

When developing a game, one of the biggest challenges that the developer often encounters is the problem of implementing complex gameplay systems.

A large-scale game project often involves various in-game events, such as instantaneous actions (e.g. malee attack), firing of projectiles, casting of spells (i.e. status conditions), area effects, upgrades, and many others. It is not so easy to make sure that all these events will coexist in harmony, due to undesired side effects which might be caused by factors such as: (1) Race condition, (2) Combination of multiple events, (3) Criteria for deciding whether an event should be applied or not, and so on.

Professor Gärdenfors, who is both a cognitive scientist and a philosopher at the University of Lund (Sweden), has introduced a structurally elegant model of events and their internal causal relations. It is called the "Two-Vector Model", and it leverages vector quantities as means of specifying the cause and effect of an event. In other words, his model defines an event as an instance of vector transformation.

What I have personally noticed is that his event model works beautifully in the context of game development. Many of the complexities which are prone to arise in gameplay systems can easily be mitigated (or even avoided) by applying the two-vector model, due to the fact that it lets us nicely encapsulate each gameplay event's causal relation as a simple algebraic operation.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>The Two-Vector Event Model</b></h3>

<049>

According to Professor Gärdenfors' definition of cause and effect, an event can be thought of as a vector function which takes a vector as the input and returns another vector as the output.

In the two-vector event model, the input is a "force vector" and the output is a "result vector". Imagine that there are two separate entities in the world - agent and patient. The agent is the one who causes the event, and the patient is the one who is being affected by the event. Once the event kicks in, the patient receives the force vector (aka "cause") that was emitted by the agent, and yields its own result vector (aka "effect").

The result vector should be able to represent any type of change, but the easiest way to understand it (from the point of view of classical mechanics) is to simply assume that it represents an offset in the patient's position. So if you (i.e. agent) hit a ball (i.e. patient) with the force of 1N and let it move by 1m, you may say that the force vector is (1N, 0N, 0N) and the result vector is (1m, 0m, 0m) in 3D space.

Gärdenfors, however, does not necessarily confine the force and result vectors solely to the physical domain. In his model of events, a "position" may as well refer to the quality of the patient (e.g. color, temperature, emotional state, etc), and a "force" may as well be considered a force which modifies the quality. For instance, an act of painting can be considered an application of a "color-changing force" to the patient, which subsequently pushes the patient's position in color space (e.g. RGB) to the desired color location.

<050>

Gärdenfors' event model differs from more traditional models of causation due to its nature of self-encapsulation. It has widely been presumed that the so-called "causation" is simply a relation between events, and that each event is more or less just a "snapshot" of how things look like at each moment. In Gärdenfors' model, on the other hand, each event contains its own cause-and-effect relation, defined in terms of agent, patient, force, and result. The agent and the force it emits can altogether be considered the "cause" of the event, while the patient and its result of receiving the emitted force can altogether be considered the "effect" of the event.

I think the most prominent advantage of modeling an event this way is that it allows us to apply the notion of causation inside the event itself rather than in terms of its relation with other events. In computer science, such a form of conceptualization nicely fits the OOP (Object-Oriented Programming) paradigm, where each object defines its own behaviors within its own body. So for example, in a typical object-oriented programming language such as Java or C#, it is oftentimes convenient to define "Event" as a class, and assume that its force-to-result vector transformation procedure (i.e. causal relation) will be implemented as the class's member function.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>An Alternative Interpretation</b></h3>

<051>

As a side note, I would like to briefly introduce yet another mathematical interpretation of an event. In his paper on event structure and force dynamics (See Fig 11 of <a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X">"Event structure, force dynamics and verb semantics"</a>), Gärdenfors explains Croft's alternative definition of causality. According to Croft, a cause-and-effect relation between forces can be explicated as the propagation of a "causal signal" across the dimension of causality. What's really interesting in this worldview is that it imagines "causality" as yet another dimension in spacetime, via which various worldly phenomena (i.e. events) establish causal links with one another.

This philosophically fascinating design, however, requires both the force vector and the result vector to reside in the same set of dimensions, thereby disallowing the result vector from identifying itself as part of its own hypothetical space which is of a different type from that of the force vector. This apparent lack of expressive freedom, I think, is one of the reasons why this unified spatial representation of causality is not so widely used.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>A Linguistic Interpretation</b></h3>

<052>

In <a href="https://www.researchgate.net/publication/352868530_Causal_Reasoning_and_Event_Cognition_as_Evolutionary_Determinants_of_Language_Structure">"Causal Reasoning and Event Cognition as Evolutionary Determinants of Language Structure"</a>, as well as the latter half of <a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X">"Event structure, force dynamics and verb semantics"</a>, Gärdenfors suggests a possible usage of his two-vector event model as a device for explaining sentence structures in our language. An "event", according to him, can be expressed as an English sentence because it has its own subject (agent), verb (force), and object (patient).

Both the agent and patient can be described by nouns, yet adjectives and prepositions can also be leveraged as "filters" for specifying them more precisely. For example, in conceptual space (aka "feature space" in machine learning and artifical intelligence), a noun can be imagined as a voluminous region (which encloses a cluster of data points that are associated with that noun) and an adjective can be imagined as a thin plane which partially intersects such a region. A combination between a noun and an adjective (e.g. "black cat", "white rose", or "wooden jar"), therefore, indicates the intersection (i.e. a plane segment) between the noun's region and the adjective's plane.

Similarly, a preposition may as well function as a filter because it specifies a region in physical space with respect to the physical location (and direction) of the observer's point of reference. It "sorts out" any object which does not fall within the specified region.

The force and result vectors are expressible in terms of verbs. Here, a verb (or a "verb phrase" in general) can be defined as a vector transformation which maps a region in space to another region in space. These regions are specified by the sentence's subject and object, respectively.

Based upon these observations, Professor Gärdenfors suggests that we formulate our language in terms of events and their force-to-result (i.e. cause-and-effect) relations. In other words, our language is based on the way we cogitate causal relations.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Application in Game Development</b></h3>

A potential application of Peter Gärdenfors' event model can be found in the development of video games. When designing a gameplay system, a developer often finds it hard to construct the cause-and-effect relations of various in-game events (e.g. Attack, Heal, Stun, Knockback, Poison, Teleport, etc) without introducing too many layers of complexity. Gärdenfors' nicely encapsulated model of events solves this problem, and I am here to demonstrate why.

First of all, we need to take a look at the generalized form of the two-vector model in order to be able to leverage it for game design purposes. It is illustrated below.

<053>

Previously, I have shown that an event can be summarized as a combination of a cause (i.e. agent and the force it emits) and an effect (i.e. patient and the result it generates from the received force). In general, however, an event does not necessarily have to involve exactly one agent, one patient, and one force vector.

Whenever I walk on my own, I am both the one who exerts the force of movement (agent) and the one who is being moved by that force (patient). And whenever I happen to be pushed by two people simultaneously, I should consider both of them as the agents of the "push" event. Their force vectors will have to be added up to yield the net force vector, which will then be used by the event to compute the result vector.

Let me show you a simple gameplay scenario to explain why the concept shown so far is useful for gameplay systems design. Suppose that there is a role-playing game in which the player is a fantasy warrior traveling in a dungeon. There are currently 3 characters nearby, one of them attacking the player (i.e. Attacker) and the other two healing the player (i.e. Healer A and Healer B). The player has a health bar which shows his current health. Each attacker decreases the health, and each healer increases the health.

<054>

In the two-vector model, it is necessary to represent this simultaneous presence of attacking/healing effects as a combination of force vectors. Imagine that there is a hypothetical space called "force space" in which all the contributing forces of the event reside (The idea of representing the force/result vectors in their own conceptual spaces is illustrated in <a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X">"Event structure, force dynamics and verb semantics"</a>). When an event kicks in, these contributing forces all add up to yield a single net force vector. This net force vector, then, gets mapped into its corresponding result vector. The result vector exists in another hypothetical space called "result space".

<055>

In the case of the player's health-changing event, we should consider the force space as the spectrum of all health-changing force values. So if the force is 0, you are doing nothing to the player's health. If the force is 1, you are increasing the player's health with the strength of 1 (This is what "healing" does). If the force is -1, you are decreasing the player's health with the strength of 1 (This is what "attacking" does). And so on.

The attacker applies the health-changing force of -1 to the player, while each of the two healers applies the health-changing force of 1 to the player. The net force is (-1) + 1 + 1 = 1, so we will conclude that the overall health-changing force that the player receives must be 1.

<056>

The health-changing event system, then, should be expected to take this net force vector (= 1) and transform it into its corresponding result vector which characterizes the change in the player's health (aka "ΔHealth"). Mathematically, such a process of transformation can be carried out by plugging the net force vector (as the input parameter) into the function called "transfer function", which basically shows us the one-to-one correspondence between force vectors and their result vectors.

Once the transformation part is complete, the only task remaining is to add the result vector (ΔHealth) to the player's current health. This is essentially what the player's health-changing event does whenever it executes itself.

But of course, one might be confused and say, "Dude, why do you overthink it? Just keep it simple. Simple is best. All you need to do is increase the player's health by 1 whenever a healer heals, and decrease it by 1 whenever an attacker attacks. You don't need such a fancy framework to do that!"

I am pretty sure that this is the exact kind of response which will be asserted a thousand times by a group of parrots unless I come up with a slightly more advanced example to show you the complexity of the issue. So here is an additional example.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Status Conditions</b></h3>

<057>

Suppose that there is also a wizard who is casting a spell on the player. This spell is called "heal-blocker spell", and while it is affecting the player, it prevents him from being healed. How shall we implement this this?

A naive approach is to put a conditional statement inside the the gameplay logic, such as: "IF (the player is being affected by a heal-blocker spell), THEN (do not heal the player)". This might be a decent solution for small games. If the game happens to involve a hundred (or even more) types of spells, however, a decent developer will agree that hard-coding their effects using a bunch of conditional statements is not an okay way to do it.

A much more scalable way of implementing a spell (aka "status condition") is to define it as a modifier of an event's transfer function.

<058>

The default transfer function of the player's health-changing event is the identity function ("f(x) = x"). It gracefully handles both the force of heal and the force of attack because, whenever the force is a positive number (heal), the health will change in the positive direction with the rate that is proportional to the magnitude of the force, and whenever the force is a negative number (damage), the health will change in the negative direction with the rate that is proportional to the magnitude of the force. This is exactly what we would expect the health-changing event to do every time it receives a force.

When the player is under the influence of the health-blocker spell, however, such a transfer function is no longer valid because the player shouldn't be healed when he receives a healing force. Therefore, we must zero out the right half of the transfer function to enforce such a condition. And how do we do that? There are multiple ways, but the easiest one is to "add" another function to the transfer function which, after the addition, will cancel out the healing behavior of the original transfer function.

This additive approach is quite elegant because it is incredibly easy to undo the process of addition. Whenever we add the spell, we simply add the spell's modifier function to the player's current transfer function. Whenever we remove the spell, we simply remove (subtract) the spell's modifier function from the player's current transfer function. Since subtraction is the exact inverse of addition, no information will be lost and all the external factors (i.e. anything that is not part of the spell) will be preserved no matter how many times we add/remove the spell to/from the player.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Thresholding</b></h3>

<059>

Another application of the two-vector event model can be found in success-or-fail (aka "binary") scenarios, such as trying to let the character jump up a steep hill in order to proceed to the next stage. Imagine that there is a hill right in front of the player, and that the player is trying to reach the top of the hill by jumping. The player's current altitude is 0, and it will be shifted up to 1 once he successfully reaches the top.

<060>

Just like we did in the previous example, we can use the two-vector event model for the problem of jumping. Unlike in the case of attacking and healing, though, we will now begin to assume that the force space refers to the range of "jump forces" (where high magnitudes denote powerful jumps and low magnitudes denote weak jumps), and that the result space refers to the change in the player's altitude after the jump.

The jump event has its own transfer function which is not an expression of proportionality between two variables, but a "threshold condition" which tells us how strong the player's jump must be in order to let him reach the top of the hill. In this example, at least the force of magnitude 2 is required to accomplish such a goal.

The main benefit of threshold-oriented gameplay scenarios (where you either CAN or CANNOT do something, not somewhere in between) is that it allows you to impose upon the player a specific set of keys which must be utilized in order to unlock his/her way out of the obstacle. If the hill were a smooth surface, for example, the player would've been able to climb it up by paying just a bit more effort and time. Under a strict yes-or-no condition, on the other hand (e.g. locked door, unreachable height, uncrossable river), it becomes possible to force the player to follow an absolute requirement such as: "You MUST have this item in your inventory in order to finish this task". This prevents the player from completing the whole game based solely upon brute-force and enough patience.

<061>

In his article on force dynamics (See <a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X">"Event structure, force dynamics and verb semantics"</a>), Gärdenfors shows us that an event's force vector can be classified into one of the following categories under the presence of a goal - "Enable", "Help", "Prevent", and "Despite". The "Enable" force, when added to the patient's current force vector, allows him/her to achieve the desired result which was unachievable before. The "Help" force is similar to the "Enable" force, except that its presence is not absolutely necessary because the patient is already able to achieve the desired result (with just a bit of additional time and effort). The "Prevent" force is the opposite of the "Enable" force because it disables the patient from achieving the goal which would have been achievable otherwise, and the "Despite" force is the opposite of the "Help" force.

Such categorization of forces is definitely feasible in a threshold-based scenario, such as the problem of jumping to reach the top of the steep hill. For example, if the player's initial jump force is only 1 and there is a "booster" item in the inventory which he/she can consume in order to add an extra boost of 1 to the jump force (which will achieve enough level of force to reach the top of the hill), we will be able to tell that this item is the "enabler" of the player's hill-mounting event. This way, we are able to sort various items, abilities, spells, and other numerous in-game factors into the four major categories (i.e. Enable, Help, Prevent, and Despite) and implement them appropriately based on how their presence will affect the progression of the game in binary (i.e. threshold-driven) gameplay scenarios.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Data-Driven Design</b></h3>

Many of you who have implemented large-scale gameplay systems may have heard of the term, "data-driven". It is one of the most popular design philosophies in game development, in which the game's rules are specified in the form of declarative statements (e.g. data tables, English sentences, block diagrams, etc) instead of being hard-coded as part of the game's script itself.

Gärdenfors' event model nicely fits the spirit of data-driven gameplay design, due to the fact that it allows us to fully describe an event and its causal relation in the form of a plain English sentence (i.e. a declarative statement), instead of a bunch of conditional and iterative statements which are intertwined with one another (See <a href="https://www.researchgate.net/publication/352868530_Causal_Reasoning_and_Event_Cognition_as_Evolutionary_Determinants_of_Language_Structure">"Causal Reasoning and Event Cognition as Evolutionary Determinants of Language Structure"</a>). Engineers who have studied a logic programming language (e.g. Prolog) will instantly grasp the beauty of this, as well as how neatly it is going to mitigate many of the design complexities which tend to arise in gameplay engineering.

As long as we manage to express gameplay events as English sentences, we will be able to summarize all gameplay rules simply as a list of sentences and hardly anything else.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Event as a List Processor</b></h3>

The examples I have shown so far are only one-dimensional - that is, each force space or result space is only a single number line, made up of a single variable (e.g. "change in health", "change in altitude", etc). However, this is just for the ease of visualization (because it is easier to draw 1D and 2D graphs than ones which are 3D, 4D, etc). In general, each force space or result space should be allowed to possess any number of dimensions, which may be spatial (x, y, z), temporal (t), or qualitative (e.g. color, temperature, health, mana, dexterity, experience, anger, happiness, attack strength, defense strength, and so forth).

Designing a transfer function which maps a multidimensional force vector to a multidimensional result vector is indeed a difficult thing to do. If you consider each vector as just an array of numbers (e.g. "int[]"), however, you will be able to tell that the two-vector event model is nothing more than a "list-mapping process" - a generic system which takes a list of numbers as the input, and generates another list of numbers as the output. One of the easiest ways of designing such a system is to treat each element of the output list as a linear combination of the elements of the input list. This lets the system's transfer function be constructed as a simple matrix multiplication, which is something your graphics card (GPU) can do extremely well.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Special Thanks</b></h3>

<062>

I would really like to thank Katarina Gyllenbäck for introducing the works of Professor Gärdenfors. I would have not had a chance to delve into his profound insights in the field of cognitive science, if she did not introduce his papers in her articles.

Katarina Gyllenbäck, who is both a narrative designer and a researcher of interactive media, has shown me a narrative-driven interpretation of Gärdenfors' two-vector event model. It is most thoroughly illustrated in her description of conceptual space in the article, <a href="https://katarinagyllenback.com/2023/03/16/part-11-the-meaning-makers-space/">"Part 11, The Meaning-Maker's Space"</a>.

To learn more about her areas of insight, please visit <a href="https://thingspool.net/read-rec/page-2.html">Here</a> to see my review of her writings on narrative design. Or, you may want to visit her website and read her vast collection of articles <a href="https://katarinagyllenback.com">Here</a>.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Bibliography</b></h3>

1. <a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X">Event structure, force dynamics and verb semantics</a> by Peter Gärdenfors (This article most accurately summarizes the mathematical pattern behind the causal relations of the two-vector event model.)

2. <a href="https://www.researchgate.net/publication/352868530_Causal_Reasoning_and_Event_Cognition_as_Evolutionary_Determinants_of_Language_Structure">Causal Reasoning and Event Cognition as Evolutionary Determinants of Language Structure</a> by Peter Gärdenfors (This one most thoroughly describes the linguistic interpretation of the two-vector event model, explaining why an event can be considered a sentence and each element of the event can be considered a phrase such as a "noun phrase", "verb phrase", etc)

3. <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.584017/full">Primary Cognitive Categories Are Determined by Their Invariances</a> by Peter Gärdenfors (This is a great introductory text to the idea of "Conceptual Space" - a hypothetical space which expresses the qualitative attributes of an object as a point in geometry. It also explains how a set of invariances in our domain of cognition (i.e. a dense cluster of sense-data) eventually manifest themselves in the form of a discrete entity called "object". This is one of the most foundational ideas in the study of artificial intelligence and machine learning (often referred to as "pattern recognition").)

4. <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.00630/full">Events and Causal Mappings Modeled in Conceptual Spaces</a> by Peter Gärdenfors (This is a general overview of how the force and result vectors are related to one another in an event. In this paper, Professor Gärdenfors tells us various subleties that are involved in the dynamics of causality, such as the capacity of the human mind to perform interpolation between two force vectors (which means that the total domain of forces which can be formed by a set of basis force vectors is their convex hull), etc.)

5. <a href="https://www.researchgate.net/publication/322314237_From_Sensations_to_Concepts_a_Proposal_for_Two_Learning_Processes">From Sensations to Concepts: a Proposal for Two Learning Processes</a> by Peter Gärdenfors (This article introduces some of the experimental results which show us that, during early childhood development, young children do manage to learn how much an object (i.e. a cluster of data points) differs from another object (i.e. another cluster of data points), but not necessarily the direction (i.e. dimension) in which they differ. It is only later stages in life during which they acquire the ability to break down each object as a product of multiple dimensions and make comparisons based upon individual dimensions, not only in terms of the overall distance between two clusters of data).