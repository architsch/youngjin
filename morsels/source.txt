:d:A list of game design concepts.
:k:Game Design, Game Mechanics, Game Narratives, Technical Game Design, Game AI, Gameplay Systems, Game Development, Mechanical Narratives, Gameplay Algorithms, Gameplay Semantics, Game Logic
:l:2024-07-26

[Game Design Concepts] June 27, 2024 - July 26, 2024

This is a collection of game design concepts.

@@<hr>
@@<div class="l_spacer"></div>
<001>
@@<h3><b>1. Mechanical Narratives</b></h3>

A game is usually a combination of two factors - mechanics and narratives.

Mechanics represent the systematic aspects of the game, such as physics, AI, crafting rules, progression curves, and others which rely on the knowledge of math/engineering.

Narratives represent the volitional aspects of the game, such as lores, stories, personalities, goals, and others which rely on the knowledge of arts/humanities.

A problem we often experience is that it is too tricky to fit these two together in one place. Many ambiguous cases are prone to arise, such as:

(1) Having a cool story (narrative) for a video game, but not knowing how to turn it into gameplay (mechanic).
(2) Knowing how to implement a clever enemy AI (mechanic), but not knowing how to design an appropriate enemy character for it (narrative).
(3) Having an interesting NPC with a unique personality (narrative), but not knowing which in-game role it ought to play (mechanic).

A solution is to start designing the game with building blocks which can both be considered mechanics and narratives at the same time (aka "mechanical narratives").

If we start by laying out the game's narratives first, it will be difficult to devise mechanics which are compatible with the given narratives. If we start by laying out the game's mechanics first, on the other hand, it will be difficult to devise narratives which are compatible with the given mechanics.

Such a dilemma can be bypassed by simply collecting a number of "mechanical narratives" and assembling them together.

@@<hr>
@@<div class="l_spacer"></div>
<002>
@@<h3><b>2. Sensors, Filters, and Motors</b></h3>

In general, a gameplay agent can be constructed by assembling 3 types of modules - sensors, filters, and motors.

Sensors collect data from the environment, filters process the data (for analysis), and motors trigger the agent to take actions based upon the filtered data.

Here is an example. Imagine that there is an anti-aircraft gun whose role is to shoot down enemy airplanes in range. We can model this gun as a composition of:

(1) A sensor (i.e. radar) which detects anything within the specified range,
(2) A filter (i.e. computing module) which looks up the database to see if the detected object is an enemy airplane, and
(3) A motor (i.e. the gun itself) which fires bullets at any object which is identified as an enemy airplane.

The gun detects an aircraft, checks to see if it is an enemy, shoots at it if so, and then detects another aircraft, checks to see if it is an enemy, shoots at it if so, and then detects another aircraft... and so on.

This is just a simple example. For more complex behaviors, you may as well equip it with multiple sensors (e.g. radar, camera, antenna), multiple filters (e.g. distance filter, look-angle filter), multiple motors (e.g. gun, horizontal rotor, vertical rotor, ammunition reloader), etc.

Such a multitude of modules can then communicate with one another by means of series connections, parallel connections, or both.

@@<hr>
@@<div class="l_spacer"></div>
<003>
@@<h3><b>3. Psychological Force Field</b></h3>

Psychological force field is a useful concept in game design.

We typically imagine each individual game character as an observer, equipped with its own independent mind. It continuously scans the environment and makes decisions off of it.

Each observer carries its own psychological force field which permeates the entirety of the game world. Every entity other than the observer itself essentially "warps" the field nearby, modifying the magnitudes and directions of the force vectors which guide the observer's movement.

A repulsive entity adds outward forces to the field, whereas an attractive entity adds inward forces to the field. This makes the observer move away from repulsive entities and closer to attractive entities.

Whether an entity is repulsive or attractive is determined by the observer's personal preference.

The force that is to be applied to the observer can be computed by the function "PsyForce(id, X, Y)", where "id" is the unique ID of the observer and (X, Y) is the observer's current position. This function returns the psychological force vector which gets added to the observer's net force.

A real life example of psychological force field is Feng Shui, where architectural patterns are believed to shape the field in which the residents of the house reside, thereby influencing the way they behave.

@@<hr>
@@<div class="l_spacer"></div>
<004>
@@<h3><b>4. Parallel-Attention Timeline</b></h3>

A parallel-attention timeline is a pretty useful tool to use in gameplay AI.

It starts from a simple analogy.

When I am stirring a pot of spaghetti while also heating a plate of frozen garlic bread in the oven, you can say that I must be multitasking.

However, it should be noted that I am paying way more attention to the pot of spaghetti than to the oven because the latter basically takes care of itself and only demands occasional supervision.

When designing a game, it is convenient to assume that each game character carries its own schedule in its mind, filled with tasks which start and end at their own designated points in time.

It is sometimes even more desirable, though, to model a schedule not as a one-dimensional queue of tasks, but as a two-dimensional grid in which each column represents a time slot and each row represents what could be referred to as "attention priority".

A character which follows this two-dimensional schedule always runs the task which occupies the highest row (i.e. highest attention priority) among all the tasks which occupy the current time slot.

Whenever an "attention trigger" (i.e. any object which demands attention) tries to register a new task to the schedule, the schedule will either ignore this new task if it happens to collide with an existing one, or proceed to register it if not.

Each task possesses its own intrinsic attention priority (i.e. row) which cannot be modified.

@@<hr>
@@<div class="l_spacer"></div>
<005>
@@<h3><b>5. Partial Movement Constraints</b></h3>

Both too much freedom and too little freedom are undesirable in gameplay.

Suppose that there are enemy characters whom the player must defeat in order to finish the level.

If the enemies are able to move in any direction, it will be a bit too bland because it reduces the functional significance of the level's peculiar spatial configuration (e.g. If you are being chased by an enemy who can only move horizontally, you can take refuge in a vertical space).

If the enemies are completely immobile, on the other hand, it will be pretty bland as well because it makes gameplay less dynamic.

A nice middle ground which leverages the benefits of both is to confine each enemy's locomotive freedom to a set of simple pathways, as though it is a train following a railway.

This mixed approach has 3 main advantages:

(1) It makes it easy to diversify enemy behaviors and introduce a wide variety of in-game strategies.
(2) Its pathfinding algorithm is easier to implement and way less computationally expensive than, say, the A-Star algorithm.
(3) It prevents extreme gameplay scenarios, such as letting the player be completely surrounded by a truckload of enemies because they always keep chasing the player without a pause. By limiting each enemy's maximum range of movement, we can spread out the distribution of enemies throughout the level and prevent them from concentrating too much in one place.

@@<hr>
@@<div class="l_spacer"></div>
<022>
@@<h3><b>6. Emotional State Machine</b></h3>

When it comes to designing gameplay systems, it is often useful to model each game character as a state machine (i.e. an object with a set of possible states). There are more advanced models indeed, but state machines usually suffice for fairly simple purposes.

Such a state machine typically comprises the character's action states, such as "idle", "attacking", "stunned", "wandering", and so on. However, this representation does not reflect the character's inner psychological states such as emotions.

In order to give depth to the game's narratives, we must let game characters have their own emotions. And this can be done by designing each character as an emotional state machine.

An emotional state machine resides in a 3D space called "emotion space", whose 3 spatial axes represent the character's happiness, excitement, and confidence.

The characters's emotional state is a point in the emotion space, and a change in the emotion is the same thing as a displacement of the point from one location to another.

When happiness, excitement, and confidence are all low, the character is depressed and timid. When happiness, excitement, and confidence are all high, the character is fascinated and arrogant. When happiness is high but excitement and confidence are low, the character is quietly happy in its selfless devotion. And when happiness and excitement are low but confidence is high, the character is in the mood of Squidward.

You can change the emotion's current state by applying a "psychological force" to it. The way you do it is to put the emotion's position under a psychological force field.

@@<hr>
@@<div class="l_spacer"></div>
<023>
@@<h3><b>7. Deities of the Game World</b></h3>

Designing a game is essentially the same thing as designing your own virtual world.

A game is a world which is governed by its own set of natural laws. And the manner in which these laws are being enforced largely depends on the manner in which the developer has initially configured the world.

Game worlds come in different types, each of which pertains to a particular theistic worldview.

Inside a monotheistic game world, a single control module regulates the feedback loops of the game characters.

Inside a polytheistic game world, multiple control modules regulate the feedback loops of the game characters.

Inside a pantheistic game world, the game characters' feedback loops are not being regulated by any external module; the characters themselves are their own regulators.

One of the main advantages of using either a monotheistic or polytheistic system is that it allows the gamemaster to occasionally override the game's default laws by means of admin privilege. In computer science, it is called "miracle".

@@<hr>
@@<div class="l_spacer"></div>
<025>
@@<h3><b>8. Modular Behavior Tree</b></h3>

Behavior trees are useful in game AI. You can implement a wide range of complex decision-making agents based upon them.

Sometimes, however, we feel overwhelmed by the urge to make a behavior tree unreasonably enormous when there are way too many types of items with which the agent is supposed to interact.

For example, imagine that a game character has a behavior tree which involves a task called "eat". In order to tell the character exactly what to do when it runs this task, the "eat" node must be able to handle all possible ways of eating, such as how to eat a salmon, how to eat a donut, how to eat a taco, and so on.

Thus, it is oftentimes convenient to make behavior trees modular, so that a tree of tasks that is specific to the object of interaction can simply reside within the object, temporarily stick itself to the character's "eat" task during the interaction, and then depart once the interaction is over.

@@<hr>
@@<div class="l_spacer"></div>
<026>
@@<h3><b>9. Brand-Building</b></h3>

One thing I learned while developing and publishing indie games, was that a game needs to have its own brand in order to be truly successful.

Creating a brand is such a monumental challenge. One cannot just put a bunch of buzzwords together and expect it to leave a lasting impact upon the heart of the audience. In my opinion, a great brand requires 3 major elements: Purpose, Logic, and Mystery.

A brand needs a purpose because otherwise the customer won't be encouraged to partake in its narrative. Without a purpose, there is no need to support the brand's growth by any means.

A brand cannot flourish without any logic either. It must have a logically sound solution to fulfill the said purpose. Our reason must be able to grasp it, for otherwise everybody will be left confounded.

However, a brand also ought to preserve a decent amount of mystery in it. Even a perfectly logical solution, guaranteed to solve the most urgent problem in the most quintessential way, won't attract the audience if they are not given their own roles to participate in a journey to freely explore the unknown and discover hidden treasures.

In general, I believe that the most appealing brand must have its own purpose, logic, and mystery in equal measures. This corresponds to the center point of the circular diagram I have shown here (denoted by the Sun).

@@<hr>
@@<div class="l_spacer"></div>
<043>
@@<h3><b>10. Image as a Game Map</b></h3>

Using an image to generate a game map is sometimes a good idea.

Suppose that your game comprises one vast terrain, modeled as a 2D voxel grid.

All you need to do is create an image and simply assume that its pixels represent to the terrain's voxels.

Each pixel's brightness may indicate the terrain's height. This can be graphically implemented by rendering the terrain as a grid of quads and letting the vertex shader set the y-coordinate of each vertex based on the corresponding pixel's brightness.

Each pixel's saturation may indicate the density of props, where low saturation means low probability of spawning a prop at the pixel's location, and high saturation means high probability of spawning a prop at the pixel's location.

Lastly, each pixel's hue may indicate the terrain's biome type, where "green" means forest/meadow, "orange" means village, "magenta" means palace, and so on.

Additional data-embedding is possible, too, as long as your image is transparent. The opacity of each pixel can be used to specify some other physical properties of the terrain, such as temperature, humidity, etc. Or it may be used for metadata, in which the game's trigger zones, waypoints, and spawn hotspots can be encoded.

With this image-based map generation system, you can even manipulate the terrain while playing the game because it is just a matter of image processing (run by fragment shaders). The overall erosion of the terrain is the same thing as blurring the image, and hitting the terrain with a meteorite is the same thing as painting the impact zone with a semi-transparent black brush.







:d:A mathematical model of our own thoughts, feelings, and the sense of curiosity.
:k:Curiosity, Motivation, Motive Force, Curiosity Force, Conceptual Space, Physical Space, Katarina Gyllenbäck, Feature Space, Idealism, Ontology, Epistemology
:l:2024-07-27

[Model of the Mind] June 28, 2024 - July 27, 2024

This is a collection of mathematical concepts designed to express the nature of human mind in a rational manner.

The ideas shown below are inspired by Katarina Gyllenbäck's articles. For more information, visit {%a href="https://thingspool.net/read-rec/page-2.html"%}Here{%/a%}.

@@<hr>
@@<div class="l_spacer"></div>
<006>
@@<h3><b>1. Physical Space and Conceptual Space</b></h3>

There are two different spaces in our universe - physical space and conceptual space.

Physical space is the space we usually refer to whenever we are employing the word "space". It consists of spatial and temporal axes (e.g. X, Y, Z, and Time), and serves as a frame of reference when it comes to describing physical entities such as rigid bodies.

Conceptual space, on the other hand, is the space of qualitative features. For example, a color is a position in the RGB color space, where the axes (R, G, B) represent the intensity levels of the three primary color components (i.e. Red, Green, Blue). The RGB color space is one of many types of conceptual spaces we can imagine.

Each object has its own position in physical space as well as a position in conceptual space. The former tells us where the object is, and the latter tells us what the object looks like.

@@<hr>
@@<div class="l_spacer"></div>
<007>
@@<h3><b>2. Particulars and Universals</b></h3>

There are two types of objects - particulars and universals.

A particular is an object which exists at a specific point in space and time. All tangible objects, such as a table we can touch, a sandwich we can eat, a cup we can hold, and countless other "real" things, can be classified as particulars.

A universal is different from a particular in the sense that we cannot directly perceive it, since it exists only in conceptual space and not in physical space. It is a pure idea which resides only in the spiritual realm; it does not belong to anywhere in our material world.

In computer science, a class is a universal. It is purely spiritual because it lives in the static memory, which is fixed and therefore "eternal" in the sense that it spans the entirety of the application's runtime.

An instance of a class, which is dynamically allocated in memory, is a particular. It is a mortal being which gets created at some point in time and gets destroyed at some other point in time. It is never eternal because it does not span the entirety of the application's runtime.

@@<hr>
@@<div class="l_spacer"></div>
<008>
@@<h3><b>3. Contiguity and Resemblance</b></h3>

Since an object exists in two different spaces (namely, "physical space" and "conceptual space"), a distance between a pair of objects can possess either one of two meanings depending on the type of space in which it is measured.

Physical proximity implies contiguity. When two things are very close to each other in physical space, we say that they are contiguous because looking at one of them lets us look at the other much more easily.

Conceptual proximity implies resemblance. When two things are very close to each other in conceptual space, we say that they resemble each other because thinking of one of them lets us think of the other much more easily.

Proximity can be measured by taking the reciprocal of the distance. Less distance means more proximity, and more distance means less proximity.

@@<hr>
@@<div class="l_spacer"></div>
<009>
@@<h3><b>4. Surprise</b></h3>

How to measure the amount of surprise between two objects?

Each object has two distinct positions - one in physical space, and the other one in conceptual space. Therefore, we are able to compute two separate distances between a pair of objects - physical distance and conceptual distance.

The amount of surprise between two objects is proportional to their conceptual distance because the more qualitatively different they are, the more surprised they will be when they see each other.

On the other hand, the amount of surprise between two objects is inversely proportional to their physical distance because the closer they are, the more vividly they will observe each other's qualitative differences.

Thus, we can measure the amount of surprise by measuring the conceptual distance and then dividing it by the physical distance.

@@<hr>
@@<div class="l_spacer"></div>
<010>
@@<h3><b>5. Curiosity Force</b></h3>

Curiosity often drives us to move from familiar places to unfamiliar places. It is possible to devise a mathematical formula which tells us exactly how the force of curiosity will initiate such a movement.

Suppose that there is an observer somewhere in physical space. We can first compute the observer's "surprise vectors", each of which represents the amount of surprise that the observer feels in regard to an external object.

If you take the sum of all the surprise vectors which involve the observer and multiply the resulting "net surprise vector" by the amount of the observer's curiosity, you will obtain the "curiosity force vector" which is responsible for pushing the observer to venture into the unknown.

@@<hr>
@@<div class="l_spacer"></div>
<016>
@@<h3><b>6. How to Compute Curiosity</b></h3>

Can we represent curiosity as a numerical quantity?

The amount of curiosity reaches its peak value when the person is feeling a sense of surprise which is neither too intense nor too dim.

If you are surrounded by an environment which is not surprising at all, you will lose curiosity due to boredom.

If you are surrounded by an environment which is too overwhelmingly surprising, you will be anxious. As a result, you will suppress your curiosity in order to protect yourself from potential dangers.

It is only when you are surrounded by a moderately surprising environment (i.e. neither too familiar nor too unfamiliar) that you will be able to feel a vivid sense of curiosity.

A halfway mixture between familiar and unfamiliar elements creates a "Goldilocks zone of motivation" which will encourage you to uncover partially hidden secrets.

If everything is already uncovered, there will be no secret to uncover and so you won't feel the necessity to start an adventure. If everything is veiled in darkness, you will feel clueless and thus not even dare to start an adventure.

@@<hr>
@@<div class="l_spacer"></div>
<046>
@@<h3><b>7. Uniformity</b></h3>

It is possible to measure the amount of uniformity between two objects.

In order for a pair of objects to comprise one uniform body, they must satisfy two conditions.

First, they must be physically close to each other. If they are too far apart, we will clearly be able to see that there is a significant spatial gap between them, showing that they are discontinuous and thus not uniform.

Secondly, they must resemble each other (i.e. similar in characteristics). If they look too drastically different, we will be able to tell that their mutual contrast is too huge to ensure that they are part of one smooth, uniform body.

Therefore, the amount of uniformity can be computed by taking the inverse of the product between their physical and conceptual distances.

@@<hr>
@@<div class="l_spacer"></div>
<047>
@@<h3><b>8. Force of Adaptation</b></h3>

If you measure the amount of uniformity between the observer and a nearby object, you will obtain a number which tells you how familiar the observer is with the object.

And if you represent this number as the magnitude of a vector which starts at the observer's conceptual location and points itself to the object's conceptual location, you will get a vector which can be referred to as a "uniformity vector".

Take the sum of all uniformity vectors which originate from the observer, and you will acquire the net uniformity vector. This vector informs us the strength and direction of the surrounding environment's familiarity with respect to the observer.

Scale this net uniformity vector by the observer's adaptivity (which is a scalar value) and you will obtain the force vector which is currently pushing the observer's viewpoint in conceptual space. This force gradually "adapts" the observer to the surroundings, making them become more and more familiar as time passes by.

@@<hr>
@@<div class="l_spacer"></div>
<048>
@@<h3><b>9. Dynamics of Exploration</b></h3>

The force of curiosity pushes the observer's body in physical space, toward areas which are unfamiliar to him.

Meanwhile, the force of adaptation pushes the observer's viewpoint in conceptual space, toward areas which are as familiar to him as possible. This lets him quickly adapt himself to his surroundings.

These two forces work together in parallel, continuously propelling both the observer's body (location in physical space) and viewpoint (location in conceptual space) in the direction of spontaneous exploration.

The back-and-forth interaction between these two forces creates a feedback system. The force of curiosity puts the observer in unfamiliar places, which in turn compels the observer to adapt his viewpoint to the unfamiliar. Once his surroundings become too familiar to him, his curiosity then drives him to search for other unfamiliar territories to explore.







:d:A mathematical interpretation of David Hume's philosophy.
:k:David Hume, Empiricism, Empirical Philosophy, Metaphysics, Epistemology, Ontology, Discrete Math, Philosophy Of Mind, Computational Psychology
:l:2024-07-31

[Mathematical Interpretation of Hume's Philosophy] July 2, 2024 - July 31, 2024

This is a mathematical interpretation of David Hume's philosophy. It is mostly based upon my personal analysis, so please take it with a grain of salt.

I drew most of my inspirations from his book, "An Enquiry Concerning Human Understanding". For more information, visit {%a href="https://thingspool.net/read-rec/page-6.html"%}Here{%/a%}.

@@<hr>
@@<div class="l_spacer"></div>
<011>
@@<h3><b>1. Introduction</b></h3>

Hume's empiricist worldview begins with a hierarchy of concepts.

There are two major types of components in his domain of knowledge. One is "perceptions", and the other one is "objects of reason". Perceptions are the atoms of the mind, whereas objects of reason are molecules which can be formed by bonding those atoms together.

A perception is any "thought" we can conceive in our minds. It belongs to either one of the two categories - impressions and ideas.

An impression is a direct stimulus received by our sense organs. Things we directly see, hear, smell, taste, and feel are all impressions.

In contrast, an idea is an afterthought on the impressions we received. For example, what we see is an impression, but the recollection of what we just saw is an idea. Impressions are vivid, while ideas are dim.

Objects of reason can be subdivided into two categories - "relations of ideas" and "matters of fact".

A relation of ideas is a result of pure logic, such as a mathematical theorem which manages to prove itself based upon a set of ideas only, without relying on the presence of external stimuli. A matter of fact, on the other hand, requires a considerable amount of empirical data to validate itself (like the law of gravitation).

@@<hr>
@@<div class="l_spacer"></div>
<012>
@@<h3><b>2. Impressions and Ideas</b></h3>

Hume's definition of "impressions" and "ideas" provides us with a rudimentary ground of logic, upon which we can formulate a model of how we sense and recall our own thoughts.

Impressions are what we typically refer to as "sensory stimuli". These are the most immediate and piercing kind of perceptions which we feel with the utmost degree of intensity.

Ideas, on the other hand, are byproducts of impressions. They linger in our minds like ghosts, which occasionally touch our feelings but to a much lesser degree. Unlike impressions which are momentary, ideas stay in the person's memory and get recalled on demand.

In a way, therefore, ideas are Lego bricks which can fit one another nicely in our minds, whereas impressions are just raw plastic ingredients which are yet to be molded into such bricks.

@@<hr>
@@<div class="l_spacer"></div>
<013>
@@<h3><b>3. Generation of Ideas</b></h3>

According to Hume's philosophy, each impression generates its corresponding idea when it enters our domain of cognition.

However, he also mentions that the relation between impressions and their ideas is not necessarily one-to-one. There may as well be cases in which a wide spectrum of ideas manage to emerge from a relatively few impressions.

An impression of "light blue" and an impression of "dark blue", for example, may allow us to imagine an intermediate shade of blue (by mixing the qualities of light blue and dark blue) and remember it as a distinct idea. This lets us picture the full spectrum of blue without having to directly sense every single one of its variants via external stimuli.

@@<hr>
@@<div class="l_spacer"></div>
<014>
@@<h3><b>4. Relations between Ideas</b></h3>

We assign meaning to our ideas by establishing relations between them. In Hume's model of the human mind, there are three fundamental types of such relations - resemblance, contiguity, and causality (aka "cause and effect").

Resemblance tells us that two ideas are qualitatively similar to each other (such as two slightly different shades of blue). This is something we can immediately tell from our perceptions.

Contiguity tells us how close two ideas are to each other in spacetime. This, too, is something we can immediately tell from our perceptions.

Causality, however, is not something we can identify in such a straightforward manner. We must derive it from the other two types of relations (i.e. resemblance and contiguity).

@@<hr>
@@<div class="l_spacer"></div>
<015>
@@<h3><b>5. Causal Relations</b></h3>

An idea alone does not have the word "cause" or "effect" written on its face, and the only way for us to prove that an idea "causes" another idea is that one of them frequently precedes (or succeeds) the other in spacetime.

Also, since it is almost impossible for us to reproduce the same exact idea over and over in our material world (which is plagued with all sorts of random noises such as acoustic waves, electromagnetic disturbances, thermal noise, etc), we must rely on our belief that similar causes are likely to produce similar effects.

This leads us to the conclusion that, if a set of mutually resembling ideas are contiguous with another set of mutually resembling ideas, we can say that these two sets are causally related to each other.

@@<hr>
@@<div class="l_spacer"></div>
<017>
@@<h3><b>6. Facts</b></h3>

What is a "fact", really?

There are many ideas in our domain of reason, yet the boundary between ones that are supposed to be "facts" and ones that are mere personal feelings often looks a bit fuzzy.

Hume's philosophy tells us that an idea can be considered a "fact" if it is caused by a sufficiently large number of causes (e.g. preceded by a sufficiently long chain of causal relations).

Each cause works as a proof of the idea's truthfulness. The more causes there are, the more confidently we are able to say that the idea is true. If it is so true that its truthfulness is hardly questionable, we say that the idea is a "fact".

People who know how a blockchain works (such as Bitcoin or Ethereum) will easily come to the realization that ideas are reminiscent of blockchain transactions, and that a "fact" is basically a transaction which has been mined (verified) and is given credit by a long chain of preceding blocks.

@@<hr>
@@<div class="l_spacer"></div>
<027>
@@<h3><b>7. Simple and Complex Ideas</b></h3>

In "A Treatise of Human Nature", Hume makes a clear distinction between simple and complex ideas.

Based upon his rejection of the infinite divisibility of our sense-data, he says that "simple ideas" are ideas which cannot be further divided into its component parts, meaning that they are the "atoms" of our mind.

Examples of simple ideas include the primary color components (e.g. red, green, and blue), musical notes, and other irreducible units of sensation.

"Complex ideas", on the other hand, are groups of simple ideas. Each complex idea is a product of synthesis among two or more simple ideas. It is also possible to group complex ideas to form even more complex ideas.

A simple impression (i.e. external stimulus) gives birth to a simple idea. A multitude of simple ideas, then, give birth to complex ideas based on the way in which they are related to each other.

@@<hr>
@@<div class="l_spacer"></div>
<028>
@@<h3><b>8. Abstraction and Combination</b></h3>

A group of simple ideas are able to produce a complex idea. A group of complex ideas, too, are able to produce yet another complex idea which is even more complex than the preceding ones.

In general, therefore, we can say that a group of ideas with appropriate relations, whether they are simple or complex, are capable of generating a complex idea whose level of complexity is slightly higher than them.

There are two ways in which a complex idea may emerge - abstraction and combination.

When you see a group of slightly different shades of blue, you will recognize that they closely resemble each other. From this, you can conclude that they all belong to a class of objects called "blue". This is an example of abstraction.

When you see a lump of various colors, on the other hand, you will recognize that the individual colored spots do not necessarily resemble each other but are nevertheless very closely packed together in physical space. From this, you can conclude that they all belong to the same object. This is an example of combination.

@@<hr>
@@<div class="l_spacer"></div>
<029>
@@<h3><b>9. Relations between Complex Ideas</b></h3>

There are two primary types of relations between ideas - resemblance and contiguity. Resemblance indicates conceptual proximity, and contiguity indicates physical proximity.

Just like resemblance and contiguity can connect simple ideas, they are able to connect complex ideas as well.

An object is a complex idea which is based off of a set of contiguous ideas. When two objects have sufficiently many pairs of resembling ideas between them, they resemble each other.

A class is a complex idea which is based off of a set of resembling ideas. When two classes have sufficiently many pairs of contiguous ideas between them, they are contiguous to each other.

A contiguity between two classes may also be referred to as "causality" (i.e. One of them is either the "cause" or "effect" of the other).

@@<hr>
@@<div class="l_spacer"></div>
<030>
@@<h3><b>10. Data Structure of an Idea</b></h3>

According to Hume's argument in "A Treatise of Human Nature", every idea (or impression) must possess its own quality and quantity.

An idea's quality denotes the category of sensation to which it belongs, such as "brightness", "musical pitch", or "temperature".

An idea's quantity denotes its quality's intensity level. For instance, if there is an idea whose quality is "temperature" and whose quantity is "30", we may consider this idea as a sensation of warmth which is 30 degrees in temperature.

Hume says that an impression is basically the same thing as an idea (i.e. It has its own quality and quantity), except that it is much higher in "force and vivacity".

Therefore, I personally find that it is much more convenient to consider an impression as an idea whose level of vivacity is sufficiently high.

When expressed in the language of computer science, both impressions and ideas are mere instances of the same data structure called "Idea", which contains 3 integers - Quality, Quantity, and Vivacity.

@@<hr>
@@<div class="l_spacer"></div>
<031>
@@<h3><b>11. Metaphor</b></h3>

Metaphors are everywhere. They appear in poetry, novels, songs, and many other forms of media. However, they are often accused of being irrational.

Hume's philosophy suggests otherwise. It is possible to define a metaphor on a rational basis.

An idea has a location in conceptual space, which is composed of two dimensions called "quality" and "quantity". Quality indicates the category of sensation (e.g. brightness, loudness, temperature, etc), and quantity indicates its level of intensity.

We say that there is a resemblance between two ideas when they are close to each other in conceptual space. There are two types of resemblances: (1) Resemblance in quality, and (2) Resemblance in quantity.

Coldness and hotness do not resemble each other quantitatively, since their intensity levels (i.e. temperature) differ significantly. However, they resemble each other qualitatively because they both belong to the same category called "temperature".

On the contrary, red and hotness do not resemble each other qualitatively because they belong to two drastically different categories of sensation. However, they resemble each other quantitatively because the high emotional intensity of red is similar in magnitude to the high temperature of hotness. This is what we call "a metaphor".

@@<hr>
@@<div class="l_spacer"></div>
<032>
@@<h3><b>12. Unity between Two Positions</b></h3>

An idea must have its own quality and quantity. However, we ought to be aware that it must also have its own physical location in order for the mind to conceive it.

For example, when you are imagining a colored point, you cannot do it without locating it somewhere inside your imaginary field of vision.

Hume suggests that, in order for an idea to "exist", we must be able to perceive it either through external senses or our own imagination. Therefore, an idea is required to have a pair of positions - one in conceptual space, and the other one in physical space.

It may theoretically be insisted that it is also feasible to formulate "pure ideas" which belong to either physical space or conceptual space but not both. Such constructs, however, are not directly sensible and thus do not belong to the domain of cognition.

@@<hr>
@@<div class="l_spacer"></div>
<033>
@@<h3><b>13. Position of a Complex Idea</b></h3>

How to compute the position of an idea?

The position of a simple (atomic) idea does not have to be computed. It is simply given to our faculty of senses the very moment it is perceived.

The position of a complex idea, on the other hand, must be computed based upon those of its component ideas.

An object in physical space appears to be a single point when viewed from a sufficiently long distance. The physical position of such a point is the center of mass of the object's components in physical space.

Meanwhile, the object's individual color spots "average out" when they are condensed into a single point. This means that its conceptual position is the center of mass of the object's components in conceptual space.

The "mass" of an idea is its vivacity.

@@<hr>
@@<div class="l_spacer"></div>
<034>
@@<h3><b>14. Word and Symbol</b></h3>

Whenever we communicate, we use words and symbols to reference our ideas.

The difference between a word and a symbol can be found in the two fundamental relations between ideas - contiguity (i.e. proximity in physical space) and resemblance (i.e. proximity in conceptual space).

When you open up a children's book intended to teach basic English words, you will see that there are lots of images with words next to them. For example, if there is an image of fire, the book will also put the word "fire" right next to it in order to teach the child that the word "fire" is associated with the appearance of fire.

Here, the word "fire" and the appearance of fire do not visually resemble each other, but the child nevertheless learns that the former indicates the latter because these two things have been displayed contiguously in physical space (i.e. page of the book).

What about a symbol? Here is an example. An airplane's safety manual typically uses an icon of fire (aka pictogram) to reference real fire.

In this case, the icon is a symbol of fire, not because we have necessarily seen this icon appearing right next to real fire in physical space, but because we know that these two things visually resemble each other.

Therefore, a word is an idea which references another idea by means of proximity in physical space, whereas a symbol is an idea which references another idea by means of proximity in conceptual space.

@@<hr>
@@<div class="l_spacer"></div>
<035>
@@<h3><b>15. Rate of Change</b></h3>

When there is a moving car, how do we measure its speed?

Suppose that we have two snapshots available, one of them showing the car's position at one point in time, and the other one showing the car's position at another point in time.

Let us refer to the first snapshot's position and time as (p1, t1), and the second snapshot's position and time as (p2, t2). Since p1 and t1 appear together in the same snapshot, we know that they are contiguous. And since p2 and t2 appear together in the same snapshot, we know that they, too, are contiguous. Furthermore, since the two snapshots are juxtaposed right next to each other, they are contiguous as well.

And since they (p1, t1, p2, t2) all make up a single body of contiguities, we can tell that they all belong to the same context.

t1 and t2, although they are not physically nearby, belong to the same "quality" in conceptual space (because they are both time values). From this coincidence, we are able to tell that their quantities are comparable. The result of their comparison is the change in time (Δt = t2 - t1).

Similarly, p1 and p2 belong to the same "quality" in conceptual space (because they are both position values), so we are able to tell that their quantities are comparable as well. The result of their comparison is the change in position (Δp = p2 - p1).

The car's speed according to the two snapshots, then, is the ratio between Δt and Δp (i.e. ratio between the lengths of the vertical line segments in conceptual space which are associated with the two snapshots).

@@<hr>
@@<div class="l_spacer"></div>
<036>
@@<h3><b>16. Cause and Effect</b></h3>

Ideas often represent events, and we know that the human mind is prone to associate events with one another in terms of cause and effect (i.e. causality).

Hume says that, by making a sufficient number of observations, we are able to notice a "constant conjuction" between two classes of objects. Hence, he explains that this "constant conjuction" is the data from which we draw the notion of causality.

In order to decide which event is the cause (or effect) of the other, however, we should make an additional analysis. It is the concept of "precedence" to which we ought to pay attention, since it is what gives a direction to the causal relation.

If there is a relation of causality between X and Y, we say that X must be the cause of Y if X precedes Y in time, and vice versa.

This endows the chain of events with a fixed direction of progress, flowing from the past to the future.

@@<hr>
@@<div class="l_spacer"></div>
<037>
@@<h3><b>17. Belief</b></h3>

It is possible to compute the amount of belief in an idea.

Our minds are populated by various ideas. Some of them are considered fictitious, while others are considered real. We make such a distinction based upon the amount of belief in each idea.

Imagine that an idea is preceded by a chain of causes (i.e. causal relations). The longer and more vivid the chain is, the more we are convinced that the idea is "real". And the shorter and less vivid the chain is, the more we are convinced that the idea is "fake".

Thus, the total amount of belief in an idea is the sum of vivacities of its preceding ideas, plus its own vivacity.

An impression (i.e. external stimulus) is "real" because, although it is not preceded by a chain of causes, its own vivacity is so sufficiently high that it easily conjures up a strong sense of belief.

@@<hr>
@@<div class="l_spacer"></div>
<038>
@@<h3><b>18. Conditional Probability</b></h3>

When a cause is given, how do we measure the probability of occurrence of each of its effects?

Our minds are filled with ideas, and ideas usually represent events. Our belief in the occurrence of an event possesses its own numerical quantity, which can be computed by the "Belief(X)" function where X is the event of interest.

Given a cause, we say that the probability of occurrence of one of its potential effects is the proportion of the belief in the given effect with respect to the sum of the beliefs in all the potential effects.

The more frequently we observe an event, its vivacity increases. And the less frequently we observe an event, its vivacity decreases.

Since the amount of belief in an event is the sum of the vivacities of its history, we can say that more observation yields higher probability and less observation yields lower probability.

@@<hr>
@@<div class="l_spacer"></div>
<039>
@@<h3><b>19. Attention</b></h3>

A person's attention is a pointer which points to an idea.

Ideas, which fill up the mind's mental space, stay inactive as long as they are being neglected. It is because their vivacity levels are zero by default.

When an external stimulus enters the person's sensory organ, an impression gets created. It sticks itself to the corresponding idea and breathes vivacity into its mouth, thereby making it alive.

At the same time, the impression attracts the mind's attenion and induces it to point to the idea. The impression soon dies out (due to the cooling down of the stimulus), but the attention remains for a while, cogitating the idea to which it was summoned.

@@<hr>
@@<div class="l_spacer"></div>
<040>
@@<h3><b>20. Attention's Job</b></h3>

A person's mind contains at least one active agent of cognition called "attention". An attention points itself to one idea at a time.

There may as well be multiple attentions navigating the person's mental space concurrently. If it is the case, we will refer to them as "primary attention", "secondary attention", and so on.

An attention performs two major jobs while it is pointing to an idea.

(1) It connects the idea with nearby ideas. In physical space, such a connection is called "contiguity". In conceptual space, such a connection is called "resemblance".

(2) It boosts up the vivacity level of the idea.

The more we pay attention to certain ideas, the more vivid they get. This, in turn, strengthens our belief in these ideas.

Ceremonies and rituals are important in human society because citizens need to share a set of common beliefs. Such activities basically "recharge" the vivacity levels of a set of ideas, ensuring that their connections are firmly fixed in our minds.

@@<hr>
@@<div class="l_spacer"></div>
<041>
@@<h3><b>21. Attention Shift</b></h3>

The dots you see here are ideas. Ideas can be connected to each other (denoted by line segments) if they are either contiguous in physical space or if they resemble each other in conceptual space.

The mind contains a self-moving agent called "attention" which constantly crawls the vast network of ideas and their connections, boosting their vivacity levels.

Each idea possesses its own "belief" number. This number tells us how strongly the mind believes in the existence of the idea.

When the attention encounters multiple alternative pathways through which it can move, it tends to move toward nearby ideas with higher beliefs than those with lower beliefs.

Over time, this tendency forces only a few strongly believed ideas to survive and all the other ideas to die out. It is reminiscent of gas clouds in space gradually being condensed into a few dense balls of mass (i.e. planets).

@@<hr>
@@<div class="l_spacer"></div>
<042>
@@<h3><b>22. Thinning of the Cloud</b></h3>

A person's mind is like a galaxy; it is a dense celestial body of innumerable ideas, all shining desperately to be embraced by the attention of the consciousness.

The mind must pay attention to an idea at least occasionally in order to keep it alive. Otherwise, its vivacity will decrease over time and, once it reaches zero, it will kill the idea.

Unfortunately, the mind can pay attention to only a small number of ideas at a time. So, even if the mind happens to have a vast cloud of ideas (due to a sudden appearance of a huge number of external stimuli, for instance), time will eventually do its job of "thinning out" such a cloud into a rigid skeleton of only a selected few ideas which are deemed way more important than others.

@@<hr>
@@<div class="l_spacer"></div>
<044>
@@<h3><b>23. Plenum and Vacuum</b></h3>

Hume's rejection of the infinite divisibility of matter in our minds eventually leads us to the conclusion that ideas must be discrete and finite in nature.

The reason is simple. A brain does not possess infinite processing power, so it cannot possibly perceive an infinite number of ideas.

Therefore, our mental space must be discrete (i.e. not continuous), and must be enclosed by finite boundaries beyond which no further ideas are conceivable (We cannot see things whiter than white itself, for instance).

What this means is that there must be a "minimum distance between ideas". No two distinct ideas can ever be closer than this.

If a pair of ideas separated by the minimum distance are connected to each other (either via the relation of contiguity or resemblance), we can define this interval as the most basic unit of "plenum" (i.e. filled space).

If a pair of ideas separated by the minimum distance are disjoint from each other, we can define this interval as the most basic unit of "vacuum" (i.e. empty space).

@@<hr>
@@<div class="l_spacer"></div>
<045>
@@<h3><b>24. Idea Field</b></h3>

Since ideas are discrete, we can imagine the mind as a vast field of ideas which are placed at regular intervals. The size of each interval is the minimum distance allowed between a pair of ideas.

Most of these ideas, however, are in sleep because their vivacity levels are zero. The mind cannot perceive them because they "do not exist".

External stimuli, which enter the mind via sense organs, give birth to impressions. These impressions, in turn, pump up the vivacity levels of their respective ideas.

The vivified ideas attract the mind's attention. The attention, then, connects these ideas by "smearing" their fluids of vivacity toward each other. If they are too far apart, however, there won't be enough vivacity to spread and the connection will fall short.









:d:Possible usage of Peter Gärdenfors' two-vector event model in the design of gameplay systems.
:k:Peter Gärdenfors, Cognitive Science, Causality, Causation, Causal Loops, Systems Thinking, System Dynamics, Game Design, Game Mechanics, Game Systems, Philosophy, Epistemology, Russell, Hume, Gameplay Systems Design, Technical Design, Lund University
:l:2024-07-28

[Game Design using Gärdenfors' Event Model] July 28, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Introduction</b></h3>

When developing a game, one of the biggest challenges that the developer often encounters is the problem of implementing complex gameplay systems.

A large-scale game project often involves various in-game events, such as instantaneous actions (e.g. malee attack), firing of projectiles, casting of spells (i.e. status conditions), area effects, upgrades, and many others. It is not so easy to make sure that all these events will coexist in harmony, due to undesired side effects which might be caused by factors such as: (1) Race condition, (2) Combination of multiple events, (3) Criteria for deciding whether an event should be applied or not, and so on.

Professor Gärdenfors, who is both a cognitive scientist and a philosopher at the University of Lund (Sweden), has introduced a structurally elegant model of events and their internal causal relations. It is called the "Two-Vector Model", and it leverages vector quantities as means of specifying the cause and effect of an event. In other words, his model defines an event as an instance of vector transformation.

What I have personally noticed is that his event model works beautifully in the context of game development. Many of the complexities which are prone to arise in gameplay systems can easily be mitigated (or even avoided) by applying the two-vector model, due to the fact that it lets us nicely encapsulate each gameplay event's causal relation as a simple algebraic operation.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>The Two-Vector Event Model</b></h3>

<049>

According to Professor Gärdenfors' definition of cause and effect, an event can be thought of as a vector function which takes a vector as the input and returns another vector as the output.

In the two-vector event model, the input is a "force vector" and the output is a "result vector". Imagine that there are two separate entities in the world - agent and patient. The agent is the one who causes the event, and the patient is the one who is being affected by the event. Once the event kicks in, the patient receives the force vector (aka "cause") that was emitted by the agent, and yields its own result vector (aka "effect").

The result vector should be able to represent any type of change, but the easiest way to understand it (from the point of view of classical mechanics) is to simply assume that it represents an offset in the patient's position. So if you (i.e. agent) hit a ball (i.e. patient) with the force of 1N and let it move by 1m, you may say that the force vector is (1N, 0N, 0N) and the result vector is (1m, 0m, 0m) in 3D space.

Gärdenfors, however, does not necessarily confine the force and result vectors solely to the physical domain. In his model of events, a "position" may as well refer to the quality of the patient (e.g. color, temperature, emotional state, etc), and a "force" may as well be considered a force which modifies the quality. For instance, an act of painting can be considered an application of a "color-changing force" to the patient, which subsequently pushes the patient's position in color space (e.g. RGB) to the desired color location.

<050>

Gärdenfors' event model differs from more traditional models of causation due to its nature of self-encapsulation. It has widely been presumed that the so-called "causation" is simply a relation between events, and that each event is more or less just a "snapshot" of how things look like at each moment. In Gärdenfors' model, on the other hand, each event contains its own cause-and-effect relation, defined in terms of agent, patient, force, and result. The agent and the force it emits can altogether be considered the "cause" of the event, while the patient and its result of receiving the emitted force can altogether be considered the "effect" of the event.

I think the most prominent advantage of modeling an event this way is that it allows us to apply the notion of causation inside the event itself rather than in terms of its relation with other events. In computer science, such a form of conceptualization nicely fits the OOP (Object-Oriented Programming) paradigm, where each object defines its own behaviors within its own body. So for example, in a typical object-oriented programming language such as Java or C#, it is oftentimes convenient to define "Event" as a class, and assume that its force-to-result vector transformation procedure (i.e. causal relation) will be implemented as the class's member function.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>An Alternative Interpretation</b></h3>

<051>

As a side note, I would like to briefly introduce yet another mathematical interpretation of an event. In his paper on event structure and force dynamics (See Fig 11 of {%a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X"%}"Event structure, force dynamics and verb semantics"{%/a%}), Gärdenfors explains Croft's alternative definition of causality. According to Croft, a cause-and-effect relation between forces can be explicated as the propagation of a "causal signal" across the dimension of causality. What's really interesting in this worldview is that it imagines "causality" as yet another dimension in spacetime, via which various worldly phenomena (i.e. events) establish causal links with one another.

This philosophically fascinating design, however, requires both the force vector and the result vector to reside in the same set of dimensions, thereby disallowing the result vector from identifying itself as part of its own hypothetical space which is of a different type from that of the force vector. This apparent lack of expressive freedom, I think, is one of the reasons why this unified spatial representation of causality is not so widely used.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>A Linguistic Interpretation</b></h3>

<052>

In {%a href="https://www.researchgate.net/publication/352868530_Causal_Reasoning_and_Event_Cognition_as_Evolutionary_Determinants_of_Language_Structure"%}"Causal Reasoning and Event Cognition as Evolutionary Determinants of Language Structure"{%/a%}, as well as the latter half of {%a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X"%}"Event structure, force dynamics and verb semantics"{%/a%}, Gärdenfors suggests a possible usage of his two-vector event model as a device for explaining sentence structures in our language. An "event", according to him, can be expressed as an English sentence because it has its own subject (agent), verb (force), and object (patient).

Both the agent and patient can be described by nouns, yet adjectives and prepositions can also be leveraged as "filters" for specifying them more precisely. For example, in conceptual space (aka "feature space" in machine learning and artifical intelligence), a noun can be imagined as a voluminous region (which encloses a cluster of data points that are associated with that noun) and an adjective can be imagined as a thin plane which partially intersects such a region. A combination between a noun and an adjective (e.g. "black cat", "white rose", or "wooden jar"), therefore, indicates the intersection (i.e. a plane segment) between the noun's region and the adjective's plane.

Similarly, a preposition may as well function as a filter because it specifies a region in physical space with respect to the physical location (and direction) of the observer's point of reference. It "sorts out" any object which does not fall within the specified region.

The force and result vectors are expressible in terms of verbs. Here, a verb (or a "verb phrase" in general) can be defined as a vector transformation which maps a region in space to another region in space. These regions are specified by the sentence's subject and object, respectively.

Based upon these observations, Professor Gärdenfors suggests that we formulate our language in terms of events and their force-to-result (i.e. cause-and-effect) relations. In other words, our language is based on the way we cogitate causal relations.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Application in Game Development</b></h3>

A potential application of Peter Gärdenfors' event model can be found in the development of video games. When designing a gameplay system, a developer often finds it hard to construct the cause-and-effect relations of various in-game events (e.g. Attack, Heal, Stun, Knockback, Poison, Teleport, etc) without introducing too many layers of complexity. Gärdenfors' nicely encapsulated model of events solves this problem, and I am here to demonstrate why.

First of all, we need to take a look at the generalized form of the two-vector model in order to be able to leverage it for game design purposes. It is illustrated below.

<053>

Previously, I have shown that an event can be summarized as a combination of a cause (i.e. agent and the force it emits) and an effect (i.e. patient and the result it generates from the received force). In general, however, an event does not necessarily have to involve exactly one agent, one patient, and one force vector.

Whenever I walk on my own, I am both the one who exerts the force of movement (agent) and the one who is being moved by that force (patient). And whenever I happen to be pushed by two people simultaneously, I should consider both of them as the agents of the "push" event. Their force vectors will have to be added up to yield the net force vector, which will then be used by the event to compute the result vector.

Let me show you a simple gameplay scenario to explain why the concept shown so far is useful for gameplay systems design. Suppose that there is a role-playing game in which the player is a fantasy warrior traveling in a dungeon. There are currently 3 characters nearby, one of them attacking the player (i.e. Attacker) and the other two healing the player (i.e. Healer A and Healer B). The player has a health bar which shows his current health. Each attacker decreases the health, and each healer increases the health.

<054>

In the two-vector model, it is necessary to represent this simultaneous presence of attacking/healing effects as a combination of force vectors. Imagine that there is a hypothetical space called "force space" in which all the contributing forces of the event reside (The idea of representing the force/result vectors in their own conceptual spaces is illustrated in {%a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X"%}"Event structure, force dynamics and verb semantics"{%/a%}). When an event kicks in, these contributing forces all add up to yield a single net force vector. This net force vector, then, gets mapped into its corresponding result vector. The result vector exists in another hypothetical space called "result space".

<055>

In the case of the player's health-changing event, we should consider the force space as the spectrum of all health-changing force values. So if the force is 0, you are doing nothing to the player's health. If the force is 1, you are increasing the player's health with the strength of 1 (This is what "healing" does). If the force is -1, you are decreasing the player's health with the strength of 1 (This is what "attacking" does). And so on.

The attacker applies the health-changing force of -1 to the player, while each of the two healers applies the health-changing force of 1 to the player. The net force is (-1) + 1 + 1 = 1, so we will conclude that the overall health-changing force that the player receives must be 1.

<056>

The health-changing event system, then, should be expected to take this net force vector (= 1) and transform it into its corresponding result vector which characterizes the change in the player's health (aka "ΔHealth"). Mathematically, such a process of transformation can be carried out by plugging the net force vector (as the input parameter) into the function called "transfer function", which basically shows us the one-to-one correspondence between force vectors and their result vectors.

Once the transformation part is complete, the only task remaining is to add the result vector (ΔHealth) to the player's current health. This is essentially what the player's health-changing event does whenever it executes itself.

But of course, one might be confused and say, "Dude, why do you overthink it? Just keep it simple. Simple is best. All you need to do is increase the player's health by 1 whenever a healer heals, and decrease it by 1 whenever an attacker attacks. You don't need such a fancy framework to do that!"

I am pretty sure that this is the exact kind of response which will be asserted a thousand times by a group of parrots unless I come up with a slightly more advanced example to show you the complexity of the issue. So here is an additional example.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Status Conditions</b></h3>

<057>

Suppose that there is also a wizard who is casting a spell on the player. This spell is called "heal-blocker spell", and while it is affecting the player, it prevents him from being healed. How shall we implement this this?

A naive approach is to put a conditional statement inside the the gameplay logic, such as: "IF (the player is being affected by a heal-blocker spell), THEN (do not heal the player)". This might be a decent solution for small games. If the game happens to involve a hundred (or even more) types of spells, however, a decent developer will agree that hard-coding their effects using a bunch of conditional statements is not an okay way to do it.

A much more scalable way of implementing a spell (aka "status condition") is to define it as a modifier of an event's transfer function.

<058>

The default transfer function of the player's health-changing event is the identity function ("f(x) = x"). It gracefully handles both the force of heal and the force of attack because, whenever the force is a positive number (heal), the health will change in the positive direction with the rate that is proportional to the magnitude of the force, and whenever the force is a negative number (damage), the health will change in the negative direction with the rate that is proportional to the magnitude of the force. This is exactly what we would expect the health-changing event to do every time it receives a force.

When the player is under the influence of the health-blocker spell, however, such a transfer function is no longer valid because the player shouldn't be healed when he receives a healing force. Therefore, we must zero out the right half of the transfer function to enforce such a condition. And how do we do that? There are multiple ways, but the easiest one is to "add" another function to the transfer function which, after the addition, will cancel out the healing behavior of the original transfer function.

This additive approach is quite elegant because it is incredibly easy to undo the process of addition. Whenever we add the spell, we simply add the spell's modifier function to the player's current transfer function. Whenever we remove the spell, we simply remove (subtract) the spell's modifier function from the player's current transfer function. Since subtraction is the exact inverse of addition, no information will be lost and all the external factors (i.e. anything that is not part of the spell) will be preserved no matter how many times we add/remove the spell to/from the player.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Thresholding</b></h3>

<059>

Another application of the two-vector event model can be found in success-or-fail (aka "binary") scenarios, such as trying to let the character jump up a steep hill in order to proceed to the next stage. Imagine that there is a hill right in front of the player, and that the player is trying to reach the top of the hill by jumping. The player's current altitude is 0, and it will be shifted up to 1 once he successfully reaches the top.

<060>

Just like we did in the previous example, we can use the two-vector event model for the problem of jumping. Unlike in the case of attacking and healing, though, we will now begin to assume that the force space refers to the range of "jump forces" (where high magnitudes denote powerful jumps and low magnitudes denote weak jumps), and that the result space refers to the change in the player's altitude after the jump.

The jump event has its own transfer function which is not an expression of proportionality between two variables, but a "threshold condition" which tells us how strong the player's jump must be in order to let him reach the top of the hill. In this example, at least the force of magnitude 2 is required to accomplish such a goal.

The main benefit of threshold-oriented gameplay scenarios (where you either CAN or CANNOT do something, not somewhere in between) is that it allows you to impose upon the player a specific set of keys which must be utilized in order to unlock his/her way out of the obstacle. If the hill were a smooth surface, for example, the player would've been able to climb it up by paying just a bit more effort and time. Under a strict yes-or-no condition, on the other hand (e.g. locked door, unreachable height, uncrossable river), it becomes possible to force the player to follow an absolute requirement such as: "You MUST have this item in your inventory in order to finish this task". This prevents the player from completing the whole game based solely upon brute-force and enough patience.

<061>

In his article on force dynamics (See {%a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X"%}"Event structure, force dynamics and verb semantics"{%/a%}), Gärdenfors shows us that an event's force vector can be classified into one of the following categories under the presence of a goal - "Enable", "Help", "Prevent", and "Despite". The "Enable" force, when added to the patient's current force vector, allows him/her to achieve the desired result which was unachievable before. The "Help" force is similar to the "Enable" force, except that its presence is not absolutely necessary because the patient is already able to achieve the desired result (with just a bit of additional time and effort). The "Prevent" force is the opposite of the "Enable" force because it disables the patient from achieving the goal which would have been achievable otherwise, and the "Despite" force is the opposite of the "Help" force.

Such categorization of forces is definitely feasible in a threshold-based scenario, such as the problem of jumping to reach the top of the steep hill. For example, if the player's initial jump force is only 1 and there is a "booster" item in the inventory which he/she can consume in order to add an extra boost of 1 to the jump force (which will achieve enough level of force to reach the top of the hill), we will be able to tell that this item is the "enabler" of the player's hill-mounting event. This way, we are able to sort various items, abilities, spells, and other numerous in-game factors into the four major categories (i.e. Enable, Help, Prevent, and Despite) and implement them appropriately based on how their presence will affect the progression of the game in binary (i.e. threshold-driven) gameplay scenarios.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Data-Driven Design</b></h3>

Many of you who have implemented large-scale gameplay systems may have heard of the term, "data-driven". It is one of the most popular design philosophies in game development, in which the game's rules are specified in the form of declarative statements (e.g. data tables, English sentences, block diagrams, etc) instead of being hard-coded as part of the game's script itself.

Gärdenfors' event model nicely fits the spirit of data-driven gameplay design, due to the fact that it allows us to fully describe an event and its causal relation in the form of a plain English sentence (i.e. a declarative statement), instead of a bunch of conditional and iterative statements which are intertwined with one another (See {%a href="https://www.researchgate.net/publication/352868530_Causal_Reasoning_and_Event_Cognition_as_Evolutionary_Determinants_of_Language_Structure"%}"Causal Reasoning and Event Cognition as Evolutionary Determinants of Language Structure"{%/a%}). Engineers who have studied a logic programming language (e.g. Prolog) will instantly grasp the beauty of this, as well as how neatly it is going to mitigate many of the design complexities which tend to arise in gameplay engineering.

As long as we manage to express gameplay events as English sentences, we will be able to summarize all gameplay rules simply as a list of sentences and hardly anything else.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Event as a List Processor</b></h3>

The examples I have shown so far are only one-dimensional - that is, each force space or result space is only a single number line, made up of a single variable (e.g. "change in health", "change in altitude", etc). However, this is just for the ease of visualization (because it is easier to draw 1D and 2D graphs than ones which are 3D, 4D, etc). In general, each force space or result space should be allowed to possess any number of dimensions, which may be spatial (x, y, z), temporal (t), or qualitative (e.g. color, temperature, health, mana, dexterity, experience, anger, happiness, attack strength, defense strength, and so forth).

Designing a transfer function which maps a multidimensional force vector to a multidimensional result vector is indeed a difficult thing to do. If you consider each vector as just an array of numbers (e.g. "int[]"), however, you will be able to tell that the two-vector event model is nothing more than a "list-mapping process" - a generic system which takes a list of numbers as the input, and generates another list of numbers as the output. One of the easiest ways of designing such a system is to treat each element of the output list as a linear combination of the elements of the input list. This lets the system's transfer function be constructed as a simple matrix multiplication, which is something your graphics card (GPU) can do extremely well.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Special Thanks</b></h3>

<062>

I would really like to thank Katarina Gyllenbäck for introducing the works of Professor Gärdenfors. I would have not had a chance to delve into his profound insights in the field of cognitive science, if she did not introduce his papers in her articles.

Katarina Gyllenbäck, who is both a narrative designer and a researcher of interactive media, has shown me a narrative-driven interpretation of Gärdenfors' two-vector event model. It is most thoroughly illustrated in her description of conceptual space in the article, {%a href="https://katarinagyllenback.com/2023/03/16/part-11-the-meaning-makers-space/"%}"Part 11, The Meaning-Maker's Space"{%/a%}.

To learn more about her areas of insight, please visit {%a href="https://thingspool.net/read-rec/page-2.html"%}Here{%/a%} to see my review of her writings on narrative design. Or, you may want to visit her website and read her vast collection of articles {%a href="https://katarinagyllenback.com"%}Here{%/a%}.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Bibliography</b></h3>

1. {%a href="https://www.sciencedirect.com/science/article/pii/S038800012300075X"%}Event structure, force dynamics and verb semantics{%/a%} by Peter Gärdenfors (This article most accurately summarizes the mathematical pattern behind the causal relations of the two-vector event model.)

2. {%a href="https://www.researchgate.net/publication/352868530_Causal_Reasoning_and_Event_Cognition_as_Evolutionary_Determinants_of_Language_Structure"%}Causal Reasoning and Event Cognition as Evolutionary Determinants of Language Structure{%/a%} by Peter Gärdenfors (This one most thoroughly describes the linguistic interpretation of the two-vector event model, explaining why an event can be considered a sentence and each element of the event can be considered a phrase such as a "noun phrase", "verb phrase", etc)

3. {%a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.584017/full"%}Primary Cognitive Categories Are Determined by Their Invariances{%/a%} by Peter Gärdenfors (This is a great introductory text to the idea of "Conceptual Space" - a hypothetical space which expresses the qualitative attributes of an object as a point in geometry. It also explains how a set of invariances in our domain of cognition (i.e. a dense cluster of sense-data) eventually manifest themselves in the form of a discrete entity called "object". This is one of the most foundational ideas in the study of artificial intelligence and machine learning (often referred to as "pattern recognition").)

4. {%a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.00630/full"%}Events and Causal Mappings Modeled in Conceptual Spaces{%/a%} by Peter Gärdenfors (This is a general overview of how the force and result vectors are related to one another in an event. In this paper, Professor Gärdenfors tells us various subleties that are involved in the dynamics of causality, such as the capacity of the human mind to perform interpolation between two force vectors (which means that the total domain of forces which can be formed by a set of basis force vectors is their convex hull), etc.)

5. {%a href="https://www.researchgate.net/publication/322314237_From_Sensations_to_Concepts_a_Proposal_for_Two_Learning_Processes"%}From Sensations to Concepts: a Proposal for Two Learning Processes{%/a%} by Peter Gärdenfors (This article introduces some of the experimental results which show us that, during early childhood development, young children do manage to learn how much an object (i.e. a cluster of data points) differs from another object (i.e. another cluster of data points), but not necessarily the direction (i.e. dimension) in which they differ. It is only later stages in life during which they acquire the ability to break down each object as a product of multiple dimensions and make comparisons based upon individual dimensions, not only in terms of the overall distance between two clusters of data).






:d:A computational interpretation of Good and Evil.
:k:Good and Evil, Dichotomy, Dualism, Binarism, Philosophy, Computational Philosophy, Computational Ethics, Ethics, Monotheism, Theology, Theism, Theosophy
:l:2024-07-30

[Good and Evil] July 30, 2024

This is a computational interpretation of Good and Evil, as well as how to model their dynamics using vector math. This is useful for technical systems design in the development of computer games.

@@<hr>
@@<div class="l_spacer"></div>
<018>
@@<h3><b>Heaven and Hell</b></h3>

Good and evil are measurable quantities.

There are three points in space - the center of Heaven, the center of Hell, and the center of the world.

The center of Heaven is the best possible state of the world; it is the pivot of pure good.

The center of Hell is the worst possible state of the world; it is the pivot of pure evil.

The center of the world belongs to neither of these two pivots. It represents the current state of the world, which is a mixture between both good and evil. Therefore, it is located somewhere between Heaven and Hell.

Both Heaven and Hell are perfect spheres. The radius of Heaven is the radius of its spherical event horizon (i.e. threshold beyond which the world can no longer escape from the volume of Heaven), and the radius of Hell is the radius of its spherical event horizon (i.e. threshold beyond which the world can no longer escape from the volume of Hell).

The amount of good in the world is its distance from Hell, minus its distance from Heaven. The farther away the world is from Hell, the more "good" it is. The closer the world is to Heaven, the more "good" it is.

The opposite scenario applies to the amount of evil in the world; it is the world's distance from Heaven, minus its distance from Hell.

@@<hr>
@@<div class="l_spacer"></div>
<019>
@@<h3><b>Goodness of Movement</b></h3>

There are three regions in space - Heaven, Hell, and the world in which we live. All three of them have their own center positions.

The world can either be moving toward Heaven, toward Hell, or somewhere in between. A movement directed toward Heaven is a "good movement", and a movement directed toward Hell is an "evil movement".

The world's movement is characterized by its velocity vector. This vector tells us how fast and in which direction the world is currently moving.

Suppose that there is a unit vector which starts from the center of the world and points directly to the center of Heaven. This indicates the best direction in which the world can ever move with respect to Heaven.

Furthermore, suppose that there is yet another unit vector which starts from the center of the world and points directly away from the center of Hell. This indicates the best direction in which the world can ever move with respect to Hell.

Combine these two unit vectors together and you will get the best direction of movement with respect to both Heaven and Hell.

If you compute the dot product between this vector and the world's velocity vector, you will obtain the degree of how good the world's velocity is.

@@<hr>
@@<div class="l_spacer"></div>
<020>
@@<h3><b>Thermostat's World</b></h3>

How to calculate the position of Heaven and Hell? It really depends on the definition of our world.

A world made up of a single thermostat dwells in a one-dimensional space which represents the full range of temperatures. The center of the world refers to the current temperature.

This one-dimensional space's lower edge indicates the lowest temperature that the thermostat can ever reach. It is one of the two centers of Hell (i.e. It is the center of the headquarter of Hell's ice department).

In contrast, the upper edge indicates the highest temperature that the thermostat can ever reach. This, too, is one of the two centers of Hell (i.e. It is the center of the headquarter of Hell's fire department).

The center of Heaven is the thermostat's most ideal temperature (i.e. Temperature that the thermostat is ordered to reach).

@@<hr>
@@<div class="l_spacer"></div>
<021>
@@<h3><b>Force of Morality</b></h3>

Measurement of the force of morality starts from the realization of our best desire.

The center of the world is constantly moving. The rate at which it simultaneously approaches Heaven and escapes Hell indicates the overall goodness of its movement.

The best velocity of the world is its most optimal direction of movement (That is, the direction which decreases the distance between the world and Heaven and increases the distance between the world and Hell as quickly as possible), scaled by the maximum possible speed of the world.

Our best desire is the direction which guides the world's current velocity to its best velocity as quickly as possible. If you multiply it by the world's overall amount of morality, you will get the force of morality which is currently pushing the world.

@@<hr>
@@<div class="l_spacer"></div>
<024>
@@<h3><b>Multiple Eras</b></h3>

The world's timeline consists of multiple eras. As time passes by, the world advances from one era to the next in a sequential manner.

Each era has its own Heaven and Hell, meaning that the amount of good (or evil) in the world is determined by the particular era in which we live.

Once the world enters Heaven 1 during Era 1, the era changes from Era 1 to Era 2. Then, once the world enters Heaven 2 during Era 2, the era changes from Era 2 to Era 3.

If Era 3 is the last of all eras, the world will finally merge with Heaven 3 (i.e. the final Heaven) after entering it.

On the other hand, entering the current era's Hell brings the world back to its previous era instead of the next.

As a side note, it is also feasible to connect the last era to the first era, thus forming an infinite loop of eras. In this case, time will be eternal and the world will keep on moving in its orbit forever.








:d:This a new way of designing biology-inspired emergent systems, based upon Glenn Puchtel's biocybernetic theory. It encompasses the construction of artificial life, artificial mind, virtual ecosystems, wetware, and other nature-inspired phenomena, based on ideas such as Systems Thinking and Unconventional Computing.
:k:Glenn Puchtel, Emergent Systems, Control Systems, Feedback Loop, Control Theory, Cybernetics, Biocybernetics, Swarm Intelligence, Software Design, Software Architecture, System Dynamics, Systems Thinking, Systems Engineering, Signal Processing, DSP, Communication Systems, Communication Network, Wetware, Unconventional Computing, Unconventional Cognition, Cognitive Science
:l:2024-08-02

[Emergent Systems based on Glenn Puchtel's Biocybernetic Theory] August 2, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Introduction</b></h3>

If you are a software engineer, you will probably agree that designing a well-organized system is a necessary step in the development of an application. Not that many engineers, however, agree that the system must be "emergent" in order for us to maximize its robustness as well as resilience.

It is probably true that emergence is not a necessary ingredient when it comes to creating a rather static, single-purpose program. When it comes to highly dynamic applications such as simulations and video games, though, we cannot help ourselves realizing that the sheer complexity of the system we are dealing with is not something which can be thoroughly grasped by the developer's intellect alone.

A large-scale application with its own adaptive behaviors is oftentimes too complex in nature, that the developer is inevitably led to the conclusion that the system must be broken down into smaller subsystems. Such a divide-and-conqure approach comes in handy especially when a fairly large team of developers are simultaneously working on the same codebase (Modularization helps us reduce interdependencies, for example).

There is a much more profound advantage in the aforementioned design philosophy, however, and it is often referred to as "emergence". As you might have already guessed, expressing a system as a group of multiple subsystems (rather than a single, monolithic blackbox) is a good idea not only because it makes it easy for developers to tackle each individual part separately, but also because the collective behavior of such subsystems makes room for what we would like to describe as "swarm intelligence" - an army of relatively dumb agents which, when coordinated under a common goal, display surprisingly robust and resilient phenomena (e.g. self-recovery of a multicellular organism).

Such a multi-agent system is called "emergent", due to the fact that its complexity is something which emerges out of simpler elements, rather than something which was carved like a rigid sculpture by the developer.

<a0_1>

Glenn Puchtel, who is an interdisciplinary software architect with expertise in system dynamics and cognitive science, has introduced a set of novel concepts in the design of emergent systems. Explained in the context of cybernetics, these concepts tell us that they can be used as building blocks of what may be termed "artificial biological organs" - modules that a cyborg would attach to its body in order to adapt itself to the surrounding environment.

In the following sections, I will be illustrating a collection of Glenn Puchtel's ideas which are indispensable for the construction of emergent systems. They are inspired by some of his major articles, which are listed below:

1. {%a href="#bibliography_1"%}"Cybernetic-Oriented Design (CyOD)"{%/a%}
2. {%a href="#bibliography_2"%}"Coordination without (direct) Communication"{%/a%}
3. {%a href="#bibliography_3"%}"Cybernetic Waves"{%/a%}
4. {%a href="#bibliography_4"%}"Reaction Networks"{%/a%}
5. {%a href="#bibliography_5"%}"Biological Models of Reactionary Networks"{%/a%}
6. {%a href="#bibliography_6"%}"Temporality"{%/a%}
7. {%a href="#bibliography_7"%}"Bio-Cybernetic Patterns"{%/a%}

These articles, however, are not the only ones he has written. If you want to read more of his works, please visit his LinkedIn newsletter: {%a href="https://www.linkedin.com/newsletters/cyborg-bio-cybernetics-6879770834110689280/"%}"Cyborg - (bio)cybernetics"{%/a%}.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>1. Separation of State and Behavior</b></h3>

Glenn Puchtel begins his introduction to cybernetic-oriented design with the notion of "separation of state and behavior" {%a href="#bibliography_1"%}[1]{%/a%}. Programmers who have learned OOP (Object Oriented Programming) will remember that a system (i.e. "object") is practically a blockbox which encapsulates both a behavior and a state. This is quite syntactically apparent in a class declaration, where the member functions characterize its behavior and the member variables characterize its state.

People who dislike OOP may insist that the very idea of wrapping both a behavior and a state inside the same container called "object" is a bad decision, since the mixture of these two drastically different elements is prone to spill nasty side effects such as race conditions and deadlocks.

This, however, is not really an intrinsic aspect of OOP, but rather a result of misunderstanding the way this paradigm should be handled. Most of its undesirable side effects can be attributed to the lack of enough separation between systems (objects), not the lack of a thick, gigantic wall which segregates everything into two global zones - one containing all the functions (behavior), and the other one containing all the state variables.

It is okay to let each system have its own behavior AND its own local state. After all, a system without any state is memoryless, and the range of actions that a memoryless system can take is extremely limited (since it is only able to react to the current input and none of the previous inputs).

One of the main sources of OOP's complexity problem is one's attempt to pack too many functionalities in a single object. Such a mistake can be prevented by making each object as simple as possible. However, we should also take care not to use this design approach as a means of justifying tight coupling (i.e. direct communication) among objects which are functionally closely related.

<a1_1>

Tight coupling might be okay to have if we are dealing with just a few objects. If there are too many of them, the overall architecture will start to degenerate into a jungle of entangled, disorderly web of communication.

<a1_2>

So, what's the solution? The key lies on Glenn Puchtel's concept of "coordination without direct communication" {%a href="#bibliography_2"%}[2]{%/a%}. I will explain it in detail throughout the upcoming sections.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>2. Indirect Communication</b></h3>

If you have looked at Glenn Puchtel's bio-inspired systems (i.e. biocybernetic systems) {%a href="#bibliography_5"%}[5]{%/a%}, you will be able to tell that they do not allow their biological subsystems (e.g. cells) to share information simply by directly invoking each other's member functions. Instead, these subsystems communicate via a "medium" - a chemical mixture which exists somewhere in space and decays over time. Such a medium continuously emits waves {%a href="#bibliography_3"%}[3]{%/a%}, which are signals waiting to be picked up by nearby systems (e.g. neighboring cells) and be processed according to their own behavioral logic (i.e. goal + rules).

<a2_1>

What I just mentioned is the essence of indirect communication. Instead of directly feeding signals to each other (which creates dependency), systems can instead talk to each other by means of a medium. A medium could be interpreted as some form of "shared state" among multiple systems, yet we should also be aware that it carries its own behavior as well (e.g. continual emission of waves, interaction with environmental factors, etc). Such a dual aspect, combined with the architect's demand for structural consistency, eventually leads us to conclude that the place which holds a medium is itself yet another system, composed of its own state and behavior.

(For a specific example, please refer to the component called "Pipe" in Glenn Puchtel's article, "Biological Models of Reactionary Networks" {%a href="#bibliography_5"%}[5]{%/a%}.)

<a2_2>

This is analogous to typical communication networks which we use daily. Our cellphones exchange data by means of cellphone towers, and out personal computers exchange data by means of servers and routers (which altogether constitute the internet). Two most widely known benefits of such a networking scheme are: (1) Reduction of the number of connections between nodes, and (2) Simplification of the overall topology of the connections.

In Glenn Puchtel's biocybernetic design philosophy, however, he reveals yet another major advantage of indirect communication - scoping of information.

In the "Space (scope)" and "Time" sections of his article, "Coordination without (direct) Communication" {%a href="#bibliography_2"%}[2]{%/a%}, Glenn Puchtel mentions that "giving individuals access to too much information may lead to sensory overload", suggesting that limiting the scope of information is crucial.

He says that it can be achieved by using a medium as the gateway of communication. As long as a medium occupies only a limited region in space and time, systems which communicate via such a medium will be guaranteed to receive information which is only relevant to the local region to which they belong.

The importance of scoping eventually leads us to the conclusion that the application as a whole should comprise multiple layers of scope, which necessitates the notion of a "hierarchy".

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>3. Hierarchy of Systems</b></h3>

When developing a large-scale application, a hierarchical worldview oftentime helps. The reason behind this is not difficult to grasp; whenever the system we are required to implement is too complex, we feel the necessity to break it down to a collection of simpler subsystems, devise each of them separately, and then join them together to let them collaborate as a whole.

For instance, a biological organism is a complex system which can be broken down to a number of major subsystems such as the digestive system, circulatory system, respiratory system, nervous system, immune system, and so on. Since these subsystems are still pretty complex, we still have to break them down to even simpler subsystems, etc, in a recursive (tree-like) manner. This means that a complex system should be hierarchical in structure.

<a3_1>

(According to Glenn Puchtel, "Complex systems organize themselves in hierarchical layers of abstraction-a structure achieved by encapsulating smaller, more specific systems within more extensive, more general systems" {%a href="#bibliography_1"%}[1]{%/a%}.)

In fact, we have already seen an example of hierarchical modeling in the previous section (i.e. "2. Indirect Communication"). When two systems are communicating through a medium, we may as well say that they are both physically bound to the same place to which the medium belongs. In other words, these two communicators should be deemed as two neighboring objects which are occupying the same region in space.

This is the simplest example of a hierarchy, in which the two communicating agents are the children of their common parent. In a way, therefore, one could claim that each branching point of a tree of systems is basically a place (i.e. a medium-provider) through which its subsystems are allowed to communicate, as though it is a LAN (Local Area Network).

<a3_2>

In general, a hierarchical breakdown of systems allows us to handle our problems via multiple layers of abstraction (where the root of the tree represents the most general (abstract) system, and each leaf of the tree represents the most specific (single-purpose) system). At the same time, it also lets systems communicate with one another by means of their parent systems, which means that they are able to transmit messages across multiple layers of the hierarchy.

("The result is the control or dynamic regulation of behavior between layers" - Glenn Puchtel {%a href="#bibliography_1"%}[1]{%/a%})

For example, suppose that we are modeling an animal's anatomy as a hierarchy of systems. And let us also suppose that cells are subsystems of a tissue and tissues are subsystems of a bloodstream (Note: I know that this is not an accurate reflection of how the body really works, but let's just ignore it for the sake of demonstration). If a cell wants to send a signal to another cell, it won't require a direct connection to that cell to do so. It will only release a chemical which will eventually be delivered to the other cell through the following steps:

(1) First, the cell's chemical will be absorbed by the surrounding tissue.
(2) The tissue will release the chemical to the adjacent bloodstream.
(3) The chemical in the bloodstream will be picked up by the other tissue.
(4) And finally, the other cell's receptor will receive the chemical's signal.

<a3_3>

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>4. Polymorphism</b></h3>

Glenn Puchtel's "separation of statics and dynamics" {%a href="#bibliography_1"%}[1]{%/a%} is an extremely useful concept not just for the sake of organization, but also for the sake of enabling polymorphism.

The foundation of polymorphism lies on the idea of decoupling. Glenn Puchtel mentions in his article that individual systems which are decoupled by knowledge "need not know about the other; only that stimulus introduction affects the state" {%a href="#bibliography_2"%}[2]{%/a%}. What he means by this is that the stimulus (information) can simply reveal its presence in a shared medium, exposing itself to nearby systems and hardly doing anything else. The individual systems, then, can pick up the stimulus from the shared medium and respond to it based on their own decision-making processes.

<a4_1>

The main advantage of polymorphism is that it enhances the modularity of systems. The sender of a stimulus does not have to care who the recipient is, or in which fashion the stimulus ought to be presented in order to fit the recipient's expectations. Those who want to receive it will receive it, and those who want to respond to it will respond to it. And the type of response is entirely up to the recipient's own behavior. The sender only needs to care about sending, and the recipient only needs to care about receiving.

Since a single type of stimulus is able to trigger different responses when detected by different types of recipients, the effects of its presence can be considered "polymorphic" - "poly" because it exhibits a one-to-many relationship (i.e. single input, multiple outputs), and "morphic" because the effects appear in distinct forms.

<a4_2>

A great example of polymorphism can be found in the case of ants and their pheromone-based communication. Glenn Puchtel says in his article that: "Just as the same pheromone elicits different behavior, whereby a worker ant might respond differently from a soldier ant, messages trigger receptors' behavior depending on their role" {%a href="#bibliography_2"%}[2]{%/a%}. Here, a pheromone is a stimulus (input signal) which triggers two separate responses when received by two different recipients (i.e. worker ant and solider ant).

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>5. Feedback Loops</b></h3>

Another benefit of indirect (medium-based) communication is that it allows us to create indirect feedback loops - i.e. long, circular chains of causality which control the flow of the overall system based on not only short-term effects, but also long-term effects (aka "Circular Causality", as mentioned by Glenn Puchtel in his introduction to cybernetic-oriented design {%a href="#bibliography_1"%}[1]{%/a%}). Such a phenomenon is indispensable for the design of dynamics systems which involve multiple causal loops that are intertwined with one another, such as an ecosystem, a marketplace, an electrical grid, and many others which appear in the study of System Dynamics (aka "Systems Thinking").

<a5_1>

A direct (i.e. internal) feedback loop is something which can easily be constructed within the anatomy of the system itself; those of you who have studied systems engineering will probably know how to design such a thing. All you need to do is branch off the output signal, feed it into a stream of time-delay elements (just 1 delay element for a first-order system, or 2 delay elements for a second-order system, etc), and then use that stream as part of the subsequent input of the system. This is an example of how a system can leverage part of its own history of outputs as means of calculating its current input.

<a5_2>

An indirect (i.e. external) feedback loop, on the other hand, requires a collaboration of multiple systems. The output of a system in such a loop first leaves the system, enters another system, leaves that system too, enters yet another system, and so on, until its effect eventually comes back to the input port of the original system. This is how you can simulate long-term effects in complex systems, such as too much population growth eventually leading to more deaths due to food shortage, and so on.

<a5_3>

The most obvious way of implementing indirect feedback loops is to first draw a CLD ({%a href="https://en.wikipedia.org/wiki/Causal_loop_diagram"%}Causal Loop Diagram{%/a%}) and then devise systematic components (e.g. stocks and flows) based on its graph structure. This approach, however, forces the overall architecture to be static (i.e. hard to modify) due to the way it tightly couples its individual subsystems with each other.

Indirect communication offers a nice solution to this lack of structural flexibility. As long as the individual systems communicate only via their "shared pool of information" (i.e. medium), we will be free to either add an additional system to the pool or remove an existing system from the pool without invoking undesirable side effects. This allows systems to dynamically reproduce or destroy themselves as though they are part of a living organism, allowing the overall architecture to continuously modify its shape (which is indispensable for adaptive, self-regulatory behaviors).

Besides, medium-based communication is far superior to predefined communication routes when it comes to the creation of indirect feedback loops, due to one simple reason; a shared medium allows its members to indirectly influence each other in any order/combination, as illustrated in the diagram below.

<a5_4>

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>6. Divisibility means Resilience</b></h3>

Expressing the overall system as an assembly of numerous subsystems has yet another advantage to offer; it is the sense of resilience.

In the "Weight [measure]" section of his article, "Coordination without (direct) Communication", Glenn Puchtel mentions that "emergent systems favor small, lightweight, almost negligible parts; losing any part does not adversely affect the whole" {%a href="#bibliography_2"%}[2]{%/a%}.

What he means by this is that a system which is divisible in nature (i.e. able to cut some of its parts off and still manage to function) is capable of sacrificing small portions of itself for larger gains - a sign of flexibility. If the system were a single, inseparable unit, any risk which involves its loss would cost the total annihilation of the system and would have to be avoided entirely.

<a6_1>

In general, a divisible system is robust because it is composed of rearrangeable parts. Such a system can grow, shrink, and change its shape as needed, and is able to accept relatively minor risks (e.g. A lion hunts down a giraffe despite the risk of being injured, since it can regrow its damaged tissues).

(Look at "Apoptosis" in Glenn Puchtel's "Coordination without (direct) Communication" {%a href="#bibliography_2"%}[2]{%/a%}.)

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>7. Goal</b></h3>

There is one major factor in systems design which I haven't mentioned yet. It is "goal" - a sense of purpose. A system needs a goal to serve its designated role; otherwise there is no point in designing a system, unless all we want is an engineering equivalent of "art for art's sake".

<a7_1>

A goal is what drives the system's control mechanism. A goal-oriented system constantly strives to keep its current state as close to the desired state as possible; for instance, a thermostat's "goal" is to minimize the difference between its current temperature and the desired temperature.

("A goal-oriented system must actively intervene to achieve or maintain its goal" - Glenn Puchtel {%a href="#bibliography_1"%}[1]{%/a%})

For a simple system such as a thermostat, the goal is easy to define. It only requires us to state simple numerical relations, such as: "The difference between X and X0 should be kept minimal", and so on. In the case of a complex system whose goal is way too complicated to be fully explicated in such a manner, however, we need a somewhat more advanced way of defining goals. From my point of view, one of the best ways of illustrating a complex goal is to decompose it into a list of more specific goals (aka "subgoals"), decompose each of them into even more specific goals, and so on, thereby creating a hierarchy of goals. This is similar to the so-called "behavior tree" in video games.

What's interesting in this model of reasoning is that it is structurally analogous to a hierarchical arrangement of systems. In fact, this happy correlation is due to the intrinsic one-to-one correspondence between each system and the goal it is expected to serve (e.g. The root system serves the root goal, the left subtree's system serves the left subtree's goal, etc).

<a7_2>

Here is an example. An organism's goal is to survive. In this case, "organism" is the root system and "survive" is the root goal. The problem is that the words "organism" and "survive" are so broad in scope, that they fail to delineate all the necessary details.

Therefore, we must repeatedly break them down into more specific components, up until the moment at which we finally feel assured that everything is broken down to a set of "atoms" which do not demand further conceptual decomposition. In computer engineering, the atoms are primitive data types (e.g. char, int, float) and machine level instructions (e.g. MOV, ADD, MUL). In electrical engineering, the atoms are basic circuit components (e.g. transistors, capacitors, inductors).

In this example, the "survive" goal can be defined as a compound of 2 subgoals - "eat" and "breathe". Serving the "survive" goal is the same thing as serving both the "eat" and "breathe" goals. 

The hierarchy of systems can be expected to mirror the hierarchy of goals. Since the "survive" goal is the parent of its 2 child goals ("eat" and "breathe"), the organism (whose goal is to "survive") should be considered the parent of its 2 child systems which serve the "eat" and "breathe" goals, respectively. The first one is the digestive system, and the second one is the respiratory system.

<a7_3>

The presence of a hierarchy of goals helps us design the hierarchy of systems because these two have a direct one-to-one relationship (i.e. they are structurally identical). All we have to do is look at each of the goals and construct a system which fulfills that goal.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Bibliography</b></h3>

1. {%a id="bibliography_1" href="https://www.linkedin.com/pulse/cybernetic-oriented-design-cyod-glenn-puchtel/"%}Cybernetic-Oriented Design (CyOD){%/a%} by Glenn Puchtel (This introductory article outlines pretty much all the main concepts in the design of emergent systems, including: (1) Separation of state and behavior (which enables polymorphism), (2) Hierarchical arrangement of individual subsystems (which is a neat way of breaking down a complex system into a collaborative network of simpler systems), (3) Back-and-forth interaction between state and behavior, which gives rise to feedback loops, and so on.)

2. {%a id="bibliography_2" href="https://www.linkedin.com/pulse/coordination-without-direct-communication-glenn-puchtel/"%}Coordination without (direct) Communication{%/a%} by Glenn Puchtel (Reading this article is crucial for understanding the nature of emergence and how its full potential might be leveraged. Here, the author suggests "indirect communication" (i.e. communication by means of a medium, rather than by means of a direct and instantaneous feeding of signals) as a method of letting individual subsystems collaborate with each other in an implicit manner, which eventually reveals highly emergent patterns such as those we can find in cellular automata.)

3. {%a id="bibliography_3" href="https://www.linkedin.com/pulse/cybernetic-waves-glenn-puchtel/"%}Cybernetic Waves{%/a%} by Glenn Puchtel (This article suggests a chemistry-inspired method of designing a signal-transmitting medium in a cybernetic communication network. The author tell us that a "medium", a mixture of multiple chemical substances of varying saturations, along with several physical parameters such as temperature and pressure, can be placed in the environment, which in turn will emit waves (signals) to the surroundings for some limited duration.)

4. {%a id="bibliography_4" href="https://www.linkedin.com/pulse/reaction-networks-glenn-puchtel/"%}Reaction Networks{%/a%} by Glenn Puchtel (Here, the author directly shows us how to define a chemical mixture in a software simulation and use it as a medium of communication between virtual biological modules. He also suggests specific ways of interpreting the content of such a mixture for the purpose of evaluating/modifying the surrounding environment (aka "conditions" and "cures"). Additionally, the very last diagram of the article deserves special attention, since it summarizes the grand cycle of information flow in a cybernetic-oriented system. In this diagram, "signals" is where an organism receives information, "rules" and "states" are where the organism evaluates and stores the received information, and "actions" is where the organism emits actions based on the result of evaluation. The "world" is the outside environment, which is governed by its own environmental factors such as chemical mixtures (media). These factors continuously emit waves, which the organism's receptors then receive as "signals".)

5. {%a id="bibliography_5" href="https://www.linkedin.com/pulse/biological-models-reactionary-networks-glenn-puchtel/"%}Biological Models of Reactionary Networks{%/a%} by Glenn Puchtel (This one explains in detail the specific building blocks of the author's (bio)cybernetic systems architecture, including nodes, edges, pipes, rules, applicators, kits, and others. Nodes are basically the entry/exit points of signals, edges are transmitters of signals, and pipes/applicators are the ones which make decisions based on the received signals.)

6. {%a id="bibliography_6" href="https://www.linkedin.com/pulse/temporality-glenn-puchtel/"%}Temporality{%/a%} by Glenn Puchtel (This article illustrates the inner workings of time-related cybernetic components (e.g. temporals) using specific code examples. By doing this, the author proves us that these components are highly useful for simulating the ways in which signals vary their intensity levels as time passes by - an extremely crucial concept for implementing time delays in the system's feedback mechanism, as well as for implementing gradual memory decay.)

7. {%a id="bibliography_7" href="https://www.linkedin.com/pulse/cybernetic-patterns-glenn-puchtel/"%}Bio-Cybernetic Patterns{%/a%} by Glenn Puchtel (This is the graphical summary of the major building blocks in cybernetic-oriented design. They resemble digital circuit components in some sense, yet are much more functionally abstract. Within this collection, "pipe" is probably the most notable component because it represents the very concept of "controlled transmission of signals" - an ever-recurring theme in the activity of neurons, protein receptors, and other biological signal-processors.)









:d:A new way of designing gameplay systems, based on a force-exchange network and functional force-vector transformation logic. This was inspired by the two-vector event model of Peter Gärdenfors (Professor at Lund University), as well as the emergent system architecture concepts invented by Glenn Puchtel (Principal software architect at GRUBBRR).
:k:Game Design, Gameplay System, Game System, Technical Game Design, Gameplay Engineering, Peter Gärdenfors, Glenn Puchtel, Emergent Systems, Network Theory, Exchange Network, Game Mechanics, Lund University, GRUBBRR, Software Architecture, Emergent Gameplay, Pathfinding Network
:l:2024-08-13

[Force-Exchange Network for Gameplay Systems] August 13, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Introduction</b></h3>

Designing a gameplay system is a fairly complicated job, due to the way in which game mechanics typically work. A game usually involves a variety of factors, which are intertwined with one another in all sorts of dynamic ways. Worse still, a game designer who is not well-versed in software engineering is prone to further compound this kind of complexity by making requests which compel the engineers to break the general rule of the game by introducing special cases, thus convoluting the codebase.

If engineers ever happen to decide that a large portion of the codebase must be refactored in order to meet the designer's unexpected request, they are likely to be accused of "not willing to deliver the product on time". Eventually, those who are deemed "productive" are ones who duct-tape their way around the designer's irrational expectations and shove the accuring tech debt under the rug.

Therefore, engineers who are intelligent enough to comprehend the seriousness of this problem will be quick to realize that there are two ways of solving it; it is either (1) Work with a decent designer who at least has a STEM mindset, or (2) Start developing a system which is so robust, that even the craziest design request cannot ruin it.

The first solution works only if the engineers are able to choose which designer to work with. Unfortunately, this does not usually happen unless they are part of a small indie game studio (in which case the boundary between a designer and an engineer would be pretty blurry anyways).

The second solution, on the other hand, works at least to some extent as long as those in charge of shaping the overall architecture of the system are engineers, not "coders". A person who knows how to write code in more than a myriad of programming languages and has memorized a bunch of IT terminologies may look impressive on the outside, yet it does not indicate his/her competence as an engineer who is capable of thinking in terms of systems.

Coming up with a gameplay system which meets every single design request might be an impossible task. However, we can still minimize the necessity of refactoring the whole system if we make sure that it is as versatile as possible in the first place. The goal of engineering is to mitigate the issue of complexity, not necessarily to get rid of it entirely (because such a goal is too idealistic to be achievable).

<b0_1>

Therefore, it my belief that beginning a game development project with a highly robust gameplay system is a crucial step to take in order to prevent the occurrence of tech debt as much as possible. And for such a purpose, I have come up with a new model of gameplay systems which I decided to refer to as "Force-Exchange Network".

In a force-exchange network, actors (i.e. gameplay agents) dispatch force vectors to one another, which travel across the network of places (i.e. spatial zones) and eventually reach their recipients. Those recipients, then, apply these force vectors to their state vectors and respond by dispatching their own force vectors.

Some of the key benefits of this design can be outlined as the following:

(1) The componentization of events in the form of vector quantities (e.g. force vectors, result vectors) helps us specify game rules as vector transformation functions (aka "transfer functions") instead of a complicated set of conditional statements.

(2) Since events communicate their effects via a medium (i.e. force-exchange network), it is easy for the system to intervene with the game's cause-and-effect relations and modify them as needed. In a "peace zone", for example, the developer can simply turn off all damage-causing forces by letting the communication network simply dismiss them during the routing process.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Background Information</b></h3>

I drew inspirations for this model of gameplay from two people.

@@<b>(1) Peter Gärdenfors</b>

Professor Gärdenfors is a cognitive scientist and a philosopher at Lund University (Lund, Sweden), whose two-vector model of causality and the idea of "forces in conceptual space" helped me model gameplay events as instances of vector transformation. Please read {%a href="https://thingspool.net/morsels/page-4.html"%}Game Design using Gärdenfors' Event Model{%/a%} to learn more about the way his ideas contributed to the computational modeling of events and their causal relations.

@@<b>(2) Glenn Puchtel</b>

Glenn Puchtel is a principal software architect/engineer at GRUBBRR (Boca Raton, Florida), whose ideas in bio-inspired emergent systems (i.e. biocybernetics, wetware, etc) as well as their architectural implications inspired me to construct a network-based topology of the gameplay system. Please read {%a href="https://thingspool.net/morsels/page-6.html"%}Emergent Systems based on Glenn Puchtel's Biocybernetic Theory{%/a%} to learn more about the way his ideas contributed to the formation of the force-exchange network model.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>1. Events</b></h3>

The force-exchange network model of gameplay starts with a core concept called "events". Everything which happens in our game is an event, and events form chains of causality based upon their cause-and-effect relations.

Suppose that there are actors (i.e. gameplay agents) in the game world. Actors experience events, yet these events are causally bound to one another in terms of forces and their results. A force triggers an event, the event applies the force to the actor (thereby generating a result), and the result, in turn, emits a force which triggers yet another event, and so on.

<b1_1>

When an actor receives a force from another actor, it applies that force to itself and generates the appropriate result. This force-to-result conversion process, as a whole, is basically what an "event" is. Such a representation of an event can be described as an example of the "two-vector model of causality" in Professor Gärdenfors' theory in cognitive science.

<b1_2>

What is really important, though, is the inner workings of the event itself. In {%a href="https://thingspool.net/morsels/page-4.html"%}Game Design using Gärdenfors' Event Model{%/a%}, I have mentioned that an event can be interpreted as a process of transforming the incoming force vector to its corresponding result vector, as well as that such transformation can be achieved by means of a transfer function (i.e. a list of one-to-one mappings between scalar values).

In addition, what is happening inside an actor as it undergoes an event should also be noted. An actor first receives an incoming force vector, transforms it into the result vector (via the transfer function), and accumulates that result vector in its own persistent vector called "state vector". This special vector represents the current state of the actor, and can be considered the cumulative sum of all the result vectors produced so far.

<b1_3>

The following code demonstrates how force vectors, result vectors, and state vectors can be implemented. They are all subtypes of IAbstractVector, which is a generic data type containing a list of numerical values. The "ForceVectorComponentIndex" enum indicates the meaning of each coordinate in a force vector, the "ResultVectorComponentIndex" enum indicates the meaning of each coordinate in a result vector, and so on. In this example, the 4th coordinate of the incoming force vector characterizes the healing/damaging force, which is responsible for influencing the 4th coordinate of the result vector which characterizes the change in the actor's health.

#$
public enum ForceVectorComponentIndex
{
    PositionForceX,
    PositionForceY,
    RadiusForce,
    HealthForce,
}

public enum ResultVectorComponentIndex
{
    PositionChangeX,
    PositionChangeY,
    RadiusChange,
    HealthChange,
}

public enum StateVectorComponentIndex
{
    PositionX,
    PositionY,
    Radius,
    Health,
}

public interface IAbstractVector
{
    int[] Components { get; }
}

public class ForceVector : IAbstractVector { ... }
public class ResultVector : IAbstractVector { ... }
public class StateVector : IAbstractVector { ... }
#$

And the code below shows how a transfer function can be implemented. Here, "MinForceValue" is the staring value of the x-axis of the transfer function, and "TransferValues" are the list of f(x) values corresponding to the values in the x-axis (if we suppose that f(x) is the mathematical notation denoting a transfer function). "ForceVectorComponentIndex" indicates the type of force the function's x-axis represents, and "ResultVectorComponentIndex" indicates the type of result the function's y-axis represents.

Processing of an event is essentially the same thing as executing the "ApplyForceToState" function of its TransferFunction object.

#$
public class TransferFunction
{
    public int[] TransferValues;
    public int MinForceValue;
    public ForceVectorComponentIndex ForceVectorComponentIndex;
    public ResultVectorComponentIndex ResultVectorComponentIndex;

    public TransferFunction(...)
    {
        ...
    }

    public AddModifier(TransferFunction modifier)
    {
        if (modifier.ForceVectorComponentIndex != ForceVectorComponentIndex)
            throw new Exception("Force vector component indices do not match.");
        if (modifier.ResultVectorComponentIndex != ResultVectorComponentIndex)
            throw new Exception("Result vector component indices do not match.");

        int N = TransferValues.Length;
        for (int i = 0; i < N; ++i)
            TransferValues[i] += modifier.TransferValues[i];
    }

    public RemoveModifier(TransferFunction modifier)
    {
        if (modifier.ForceVectorComponentIndex != ForceVectorComponentIndex)
            throw new Exception("Force vector component indices do not match.");
        if (modifier.ResultVectorComponentIndex != ResultVectorComponentIndex)
            throw new Exception("Result vector component indices do not match.");

        int N = TransferValues.Length;
        for (int i = 0; i < N; ++i)
            TransferValues[i] -= modifier.TransferValues[i];
    }

    public void ApplyForceToState(ForceVector forceVector, StateVector stateVector)
    {
        int forceValue = forceVector.Components[ForceVectorComponentIndex];
        stateVector.Components[ResultVectorComponentIndex] += TransferValues[forceValue - MinForceValue];
    }
}
#$

After processing the events and adding their results to the state vector, the actor then runs its own behavioral logic and emits outgoing forces, which will then be received/processed by other actors. The other actors, then, may decide to send their own outgoing forces to the aforementioned actor, and so on. This back-and-forth transmission of forces allows the system to give birth to complex chains of causality, without requiring the architect to configure them manually. Such chains simply "emerge" out of where the actors are located and what the characteristics of their transfer functions are.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>2. Force Routers</b></h3>

There is a reason why my proposed model of gameplay systems is called "Force-Exchange Network". It is because network-oriented communication is the heart of what makes this model work.

When an actor emits a force vector, how shall we make sure that it will be received by the intended recipients? Unless every actor is sending force to everyone else all the time, we must attach some metadata to the force vector for the purpose of guiding it to its proper destination, such as the target location, places it has already visited so far (so as not to visit the same place over and over again), etc.

The following code shows a wrapper class which will works as a "parcel" for delivering force vectors. This class is simply called "Force", and is where the force vector and its metadata are packaged together. This is what an actor actually sends to other actors whenever it "emits a force".

#$
public class Force
{
    public HashSet<Place> VisitedPlaces;
    public int TargetPositionX;
    public int TargetPositionY;
    public int TargetRadius;
    public ForceVector ForceVector;
}
#$

So, we have this thing called "Force" which is analogous to a letter in a mail delivery service. But how to deliver it to its designated location? In order to answer this question, we must first picture the game world as a collection of spatial entities and then proceed to interpret them as nodes in a communication network.

First of all, let us imagine that the game world consists of a number of places, where a "place" is a fixed region in space. Each place contains a number of actors in it, who are responsible for exchanging forces with one another.

Spatially speaking, a place is an area which encloses its actors. From a communication point of view, though, a place is something more than that. Since an actor communicates with another actor "through space", we ought to imagine a place not as a static region, but as a "force router" which works as a medium of signal transmission (This concept is thoroughly explained in the "Indirect Communication" section of {%a href="https://thingspool.net/morsels/page-6.html"%}Emergent Systems based on Glenn Puchtel's Biocybernetic Theory{%/a%}).

<b2_1>

A place acts as a "cellphone tower" in this respect. It "routes" forces to their rightful recipients, just like a cellphone tower routes voice signals to their receivers' mobile devices. One of the main benefits of this indirect means of communication is that it prevents tight coupling among the actors themselves.

<b2_2>

What if the sender and recipient are in two different places? In this case, the sender's place should first route the sender's force to the recipient's place. The recipient's place, then, will route the received to the recipient, thereby completing the line of delivery.

<b2_3>

The following snippet shows the code implementation of the "Place" data structure. From a topological point of view, each place is a node in a graph with its own edges to its adjacent places (i.e. "neighboringPlaces") as well as a set of actors it contains. The four numbers, "boundaryX1", "boundaryY1", "boundaryX2", and "boundaryY2", refer to the place's spatial boundaries.

Whenever a place receives a force, its "RouteForce" function gets called. This function compares the force's target region with the spatial regions of the place's constituent actors as well as neighboring places, and routes the force to every one of them whose region intersects that of the target region (because anyone who lies outside of the target region is not supposed to receive the force).

#$
public abstract class Place
{
    private HashSet<Actor> actors;
    private HashSet<Place> neighboringPlaces;

    private int boundaryX1;
    private int boundaryY1;
    private int boundaryX2;
    private int boundaryY2;

    public Place(...)
    {
        ...
    }

    public void RouteForce(Force force)
    {
        force.VisitedPlaces.Add(this);
        ApplyFilter(force);

        foreach (Actor actor in actors)
        {
            actor.ReceiveIncomingForce(force);
        }

        foreach (Place neighboringPlace in neighboringPlaces)
        {
            if (!force.VisitedPlaces.Contains(neighboringPlace))
            {
                int forceX1 = force.TargetPositionX - force.TargetRadius;
                int forceY1 = force.TargetPositionY - force.TargetRadius;
                int forceX2 = force.TargetPositionX + force.TargetRadius;
                int forceY2 = force.TargetPositionY + force.TargetRadius;
                if ((forceX1 <= boundaryX2 && forceX2 >= boundaryX1) &&
                    (forceY1 <= boundaryY2 && forceY2 >= boundaryY1))
                {
                    neighboringPlace.RouteForce(force);
                }
            }
        }
    }

    public void Update()
    {
        // Re-evaluate and rearrange actors (based on their current positions).
        ...
    }

    private abstract void ApplyFilter(Force force);
}
#$

The game world, as a whole, may as well be considered a graph which is made out of "Place" nodes. Such a world is fairly convenient to devise, both manually and procedurally, since each place can be thought of either a room or a hallway of a dungeon, etc. Besides, the center of each place may also serve as a pathfinding node, from which a set of more granular pathfinding nodes can branch out.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>3. Force Filters</b></h3>

One may ask, "Why do we even need a network-based communication scheme for exchanging forces? Can't an actor just look up other actors and send forces directly to them?"

And indeed, it is a feasible option. However, there are reasons why I insist that transmitting forces via a network of routers (i.e. places) is a much better idea from an architectural standpoint than direct actor-to-actor force transmission.

The first reason is that the game may not necessarily be single-threaded, single-player, and completely synchronous. It might be an online game which is supposed to be played across multiple servers, in which case the network-based topology accurately reflects the way in which things ought to be arranged. Each server machine, for instance, may serve as a separate "place" in this game, being an actual router of signals inside a real computer network. And even if the game is single-player, modern computing devices often expect us to leverage their power of multithreading (by means of worker threads, etc). In this case, allocating the jobs of different places (i.e. force routers) to different threads becomes a feasible option as long as these places possess their own asynchronous message queues, etc.

The second reason is that using a place as a medium of force transmission allows us to create places with their own ways of intervening with the forces (aka "force filters"). When designing a video game, we often feel that the system ought to be able to change the manners in which the actors interact with each other based on where they are located (e.g. "The game must disable damage effects if the players are in a peace zone", etc). Force filters easily fulfill this kind of expectation by filtering forces via custom methods before they reach their recipients.

<b3_1>

The following code shows how different types of places are able to have their own filters. The "ApplyFilter" method is what applies the place's force filter to any force it happens to be routing. Each subclass of "Place", which represents a custom place type, includes its own definition of "ApplyFilter", allowing it to filter forces in its own way.

#$
public class RegularPlace : Place
{
    private void ApplyFilter(Force force)
    {
    }
}

public class DamageDisabledPlace : Place
{
    private void ApplyFilter(Force force)
    {
        if (force.ForceVector.Components[(int)ForceVectorComponentIndex.HealthForce] < 0)
            force.ForceVector.Components[(int)ForceVectorComponentIndex.HealthForce] = 0
    }
}

public class MechanicalForceDisabledPlace : Place
{
    private void ApplyFilter(Force force)
    {
        force.ForceVector.Components[(int)ForceVectorComponentIndex.PositionForceX] = 0;
        force.ForceVector.Components[(int)ForceVectorComponentIndex.PositionForceY] = 0;
    }
}
#$

"RegularPlace" is a place which does not intercept forces at all; it simply lets forces pass through the vacuum. "DamageDisabledPlace" is some kind of "peace zone" where none of the actors are able to hurt each other (because all negative (damaging) health forces are clamped to 0). "MechanicalForceDisabledPlace" is a chunk of space in which all mechanical interactions are disabled (e.g. collision, knockback, etc), which means that all actors can simply pass through one another like ghosts.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>4. Status Conditions</b></h3>

In {%a href="https://thingspool.net/morsels/page-4.html"%}Game Design using Gärdenfors' Event Model{%/a%}, I have mentioned that an actor's status condition (e.g. spell, upgrade, etc) can be implemented as a modification to the event's transfer function. The simplest approach is to model each status condition as a function which temporarily gets added to the event's original transfer function, thereby "warping" the way in which the event transforms the incoming force vector.

<058>

The question of when to add or remove a status condition, though, presents us with a major technical challenge. A brute-force implementation is to just let actors identify each other and directly invoke each other's "AddStatusCondition(...)" or "RemoveStatusCondition(...)" methods, and so forth, but such a direct solution violates the architectural elegance of the network-based communication protocol.

A significantly better solution is to let the addition and removal of a status condition be part of a force vector. This allows each actor to apply a status condition to another actor by sending a force vector, instead of relying on other means. The image below shows an example layout of a force vector which supports status conditions. The "Add-force" component refers to the number of times the corresponding status condition should be added to the recipient actor, and the "Remove-force" component refers to the number of times the corresponding status condition should be removed from the recipient actor.

<b4_1>

And here is the augmented version of the "ForceVectorComponentIndex" enum. As you can see from the code below, each separate type of status condition has its own pair of force vector components - AddForce and RemoveForce.

#$
public enum ForceVectorComponentIndex
{
    PositionForceX,
    PositionForceY,
    RadiusForce,
    HealthForce,
    StatusConditionAddForce_HealBlocker,
    StatusConditionRemoveForce_HealBlocker,
    StatusConditionAddForce_Stun,
    StatusConditionRemoveForce_Stun,
    StatusConditionAddForce_Poison,
    StatusConditionRemoveForce_Poison,
    StatusConditionAddForce_Freeze,
    StatusConditionRemoveForce_Freeze,
}
#$

A little bit of change in the event-processing logic will be required to make this design work. Previously, we were merely assuming that the event takes the incoming force vector, plugs it into the transfer function, and computes the corresponding result vector. When status conditions are involved, however, the incoming force vector's components which are related to status conditions must be identified/processed via a different procedure. Instead of going through the transfer function, these components will turn themselves into "modifier functions" which will then be added to (or subtracted from) the transfer function.

<b4_2>

The code below is how a status condition should be implemented. Each status condition consists of two parts - is modifier function and expiration time. The modifier function is structurally the same as a transfer function (hence the reason why its type is "TransferFunction"), except that its role is to modify an existing transfer function instead of serving as one. The expiration time is for cases in which the status condition is supposed to be automatically removed from the actor after a certain duration of time, without requiring it to be removed explicitly. The "StatusConditionFactory" class is basically a lookup table for each status condition type's modifier function and lifespan.

#$
public class StatusCondition
{
    public TransferFunction Modifier;
    public float ExpirationTime;
}

public static class StatusConditionFactory
{
    public static Dictionary<ForceVectorComponentIndex, () => StatusCondition> GenerationMethods = {
        {ForceVectorComponentIndex.StatusConditionAddForce_HealBlocker,
            () => new StatusCondition(...)},
        {ForceVectorComponentIndex.StatusConditionRemoveForce_HealBlocker,
            () => new StatusCondition(...)},
        {ForceVectorComponentIndex.StatusConditionAddForce_Stun,
            () => new StatusCondition(...)},
        {ForceVectorComponentIndex.StatusConditionRemoveForce_Stun,
            () => new StatusCondition(...)},
        {ForceVectorComponentIndex.StatusConditionAddForce_Poison,
            () => new StatusCondition(...)},
        {ForceVectorComponentIndex.StatusConditionRemoveForce_Poison,
            () => new StatusCondition(...)},
        {ForceVectorComponentIndex.StatusConditionAddForce_Freeze,
            () => new StatusCondition(...)},
        {ForceVectorComponentIndex.StatusConditionRemoveForce_Freeze,
            () => new StatusCondition(...)},
    };
}
#$

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>5. Anatomy of an Actor</b></h3>

The diagram below illustrates the overall structure of an actor (i.e. gameplay agent). First, it receives all the incoming forces and adds them together, yielding the net incoming force vector. This single vector contains the composite effect of all the incoming forces which the actor happened to receive at that particular moment in time.

This net vector, then, undergoes two different processes. First, some of its components which are related to the addition/removal of status conditions will trigger the system to add/subtract the appropriate "modifier functions" to/from the transfer functions whose dimension types match, as well as setting timers to handle their expiration. Secondly, the rest of the vector components will be applied to the (possibly modified) transfer functions, yield their corresponding result vectors.

The individual result vectors, then, will eventually be added to the actor's state vector (because, after all, result vectors represent changes in the state vector; they are the differentials). The state vector is the current state of the actor, which means that its components reflect the actor's current x position, current y position, current size in radius, current health, etc. The only "state" outside of this vector is the set of ongoing status conditions that the actor has in itself.

Based on the latest state vector, the actor's "force emitters" then execute themselves and generate the outgoing forces. They can be pretty much any sorts of behavioral commands, such as: "Attack every enemy in range", "Heal the closest friend in front of you", "Apply the slow-down status condition to every enemy in front of you", "Turn yourself toward the closest enemy", and so on.

Note that it is definitely possible to generate an outgoing force whose recipient is the sender itself. This kind of force comes in handy when the actor wants to move or rotate itself (or apply a status condition to itself).

<b5_1>

The following code is how an actor can be implemented as a class. Whenever it updates, it processes the incoming force vectors, applies them to the state vector, emits outgoing force vectors, and gets rid of expired status conditions.

#$
public class Actor
{
    private Place place;
    
    private ForceVector netIncomingForceVectorPending;
    private StateVector stateVector;

    private HashSet<TransferFunction> transferFunctions;
    private HashSet<ForceEmitter> forceEmitters;

    private HashSet<StatusCondition> statusConditions;
    private List<StatusCondition> removePendingStatusConditions;

    public int PositionX => stateVector.Components[(int)StateVectorComponentIndex.PositionX];
    public int PositionY => stateVector.Components[(int)StateVectorComponentIndex.PositionY];
    public int Radius => stateVector.Components[(int)StateVectorComponentIndex.Radius];

    public Actor(...)
    {
        ...
    }

    public void ReceiveIncomingForce(Force incomingForce)
    {
        int dx = incomingForce.TargetPositionX - PositionX;
        int dy = incomingForce.TargetPositionY - PositionY;
        int thresDist = incomingForce.TargetRadius + Radius;

        // Receive the incoming force only if the actor intersects the target region.
        if (dx*dx + dy*dy <= thresDist*thresDist)
        {
            netIncomingForceVectorPending += incomingForce.ForceVector;
        }
    }

    public void SendOutgoingForce(Force outgoingForce)
    {
        place.RouteForce(outgoingForce);
    }

    public void Update()
    {
        foreach (TransferFunction transferFunction in transferFunctions)
        {
            int N = netIncomingForceVectorPending.Components.Length;
            for (int i = 0; i < N; ++i)
            {
                if (StatusConditionFactory.GenerationMethods.TryGetValue((ForceVectorComponentIndex)i, out var method))
                {
                    int numStatusConditionsToAdd = netIncomingForceVectorPending.Components[i];
                    for (int j = 0; j < numStatusConditionsToAdd; ++j)
                    {
                        AddStatusCondition(method());
                    }
                }
            }
            transferFunction.ApplyForceToState(netIncomingForceVectorPending, stateVector);
        }

        foreach (ForceEmitter forceEmitter in forceEmitters)
        {
            Force outgoingForce = forceEmitter.GenerateOutgoingForce(actor, stateVector);
            SendOutgoingForce(outgoingForce);
        }

        for (int i = 0; i < netIncomingForceVectorPending.Components.Length; ++i)
        {
            netIncomingForceVectorPending.Components[i] = 0;
        }

        foreach (StatusCondition statusCondition in statusConditions)
        {
            if (statusCondition.ExpirationTime > Time.time)
                removePendingStatusConditions.Add(statusCondition);
        }

        foreach (StatusCondition removePendingStatusCondition in removePendingStatusConditions)
        {
            RemoveStatusCondition(removePendingStatusCondition);
        }
        removePendingStatusConditions.Clear();
    }

    private void AddStatusCondition(StatusCondition statusCondition)
    {
        statusConditions.Add(statusCondition);

        foreach (TransferFunction transferFunction in transferFunctions)
        {
            if ((transferFunction.ForceVectorComponentIndex == statusCondition.TransferFunction.ForceVectorComponentIndex) &&
                (transferFunction.ResultVectorComponentIndex == statusCondition.TransferFunction.ResultVectorComponentIndex))
            {
                transferFunction.AddModifier(statusCondition.Modifier);
            }
        }
    }

    private void RemoveStatusCondition(StatusCondition statusCondition)
    {
        statusConditions.Remove(statusCondition);

        foreach (TransferFunction transferFunction in transferFunctions)
        {
            if ((transferFunction.ForceVectorComponentIndex == statusCondition.TransferFunction.ForceVectorComponentIndex) &&
                (transferFunction.ResultVectorComponentIndex == statusCondition.TransferFunction.ResultVectorComponentIndex))
            {
                transferFunction.RemoveModifier(statusCondition.Modifier);
            }
        }
    }
}

public class ForceEmitter
{
    public Force GenerateOutgoingForce(Actor actor, StateVector stateVector)
    {
        ...
    }
}
#$

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>6. Future Implementation</b></h3>

The schematics and code shown so far, of course, do not cover the full picture of the force-exchange network system. There are many more things which ought to be added to make it usable for gameplay purposes. Here are some of the future implementations which I think may be necessary:

@@<b>(1) Force with delay (e.g. projectile)</b>

In many cases, forces are not instantaneous; they need time to propagate. Therefore, it may be desirable to attach an additional "delay" attribute to the Force object, telling the Place objects that they must wait for a certain duration of time before routing it to either the recipient or another place.

@@<b>(2) Function-based force emission logic</b>

I have not demonstrated any logic for generating outgoing forces. Technically speaking, it is not impossible for an actor to simply look up other actors (via references to its current place, neighboring places, etc), go through a custom logic (with a bunch of hard-coded conditional/iterative statements), and decide what forces to emit. Although such an approach is straightforward, it is far more prone to error and complexity than the way in which incoming forces are being processed. Thus, we will probably need an outgoing-force equivalent of the "transfer function".

@@<b>(3) Place with its own force-emitting behaviors</b>

Just like actors are capable of emitting forces, places may need to be able to emit forces as well. A sauna, for example, is a place which constantly emits "heat force" to all the actors in it. Also, this kind of logic lets us devise places with their own "force fields" (e.g. gravity), as opposed to places which act like complete vaccum in outer space.

@@<b>(4) Observation by means of "notification forces"</b>

While it is definitely possible to let actors directly perceive each other's presence based on references (i.e. reference to the current place + reference to neighboring places + reference to each place's constituent actors, etc), such direct dependency is not so desirable from an architectural point of view. Also, it makes it hard to implement visibility-warping gameplay features such as stealth-mode, and so on. Thus, it may be better to let each actor emit not only state-changing forces such as "knockback", "heal", "damage", etc, but also "notification forces" which notify the actor's presence. This lets the process of observing other actors be passive (i.e. You just wait to receive notification forces instead of actively searching for other actors on your own).











:d:How to use state machine networks (multi-state machines) to design dynamics systems, including gameplay systems, industrial simulations, scientific applications, cellular automata, and many others.
:k:Game Design, Technical Game Design, Game Mechanics, State Machine, FSM, Finite State Machine, Multi State Machine, Systems Engineering, Electrical Engineering, Electronics, Digital Circuits, Digital Logic, Digital Design, Verilog, HDL, Boolean Algebra, Systems Design, Cellular Automata, Circuit Simulator
:l:2024-08-18

[Technical Design using Multi-State Machines] August 18, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Introduction</b></h3>

There are many ways of conceptualizing a system. In computer science, it is typical to picture it as a state machine. One of the main downsides of this approach, however, is the ensuing complexity.

A fairly small system with only a few number of possible states is pretty easy to model as a state machine; all we need to do is come up with a set of states (i.e. nodes in a graph), and a set of transitions between states (i.e. edges between the nodes). This lets us devise the entire system simply by drawing a state diagram.

A system with an enormous number of states, on the other hand, is not something which we are able to design in terms of a single state machine without pulling our hair off out of sheer confusion. While it is technically possible to formultae even an extremely complex system (such as a whole computer) as an FSM (Finite State Machine) in which the current state of the system is represented as one of the nodes in a graph, such a way of modeling the system is prohibitively convoluted.

Therefore, it usually makes more sense to imagine a complex (non-trivial) system not as a single state machine, but as a collection of multiple state machines (i.e. modules), each of which keeps track of its own state. This design philosophy also aligns itself with the OOP (Object Oriented Programming) paradigm, where each object is a state machine with its own set of states (i.e. all possible permutations of the member variables) and state transitions (i.e. all possible ways in which the member variables can change their values).

Such a multitude of state machines, though, can operate as parts of one underlying system only if they are able to influence each other - that is, they must be able to communicate. If not, they will be a mere juxtaposition of completely isolated systems.

<c00>

The question is, "What kind of communication"? There are innumerable ways of sharing information with one another, yet some of them are far more efficient than others. Thus, a wise designer of a system should be able to choose an optimal (or at least a nearly optimal) means of letting state machines affect each other's state.

In this article, I will showcase a new method of communication between state machines which I consider as fairly optimized for a wide variety of applications in system design, such as video games, cellular automata, circuit simulators, and many others.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Multi-State Machine</b></h3>

Since I am personally more familiar with game development than any other area of engineering, it is probably appropriate for me to start with an example which takes place in a video game.

Suppose that there is a top-down game in which the player is going on an adventure in a maze-like world. There are walls preventing him from moving freely, so he is forced to use doors. Unfortunately, some of the doors are locked, which means that the player must figure out how to unlock them in order to get to the other side of the wall.

The image below is an example of a locked door. This door stays open only when the nearby button is being pressed, and immediately closes itself as soon as the button is released. So the only way for our player to cross the door is to first place a box right in front of the button to press it. Unless somebody moves it away, the box will continue pressing the button, and the player will be free to cross the door.

<c01>

How shall we implement such a game mechanic? One of the most intuitive ways, obviously, is to model each individual object as a state machine. For instance, the door is a state machine with two states called "Closed" and "Open", and the button is a state machine with two states called "Released" and "Pressed".

The diagram below shows how the button "talks" to the door, so as to make the aforementioned rule work. Whenever the door is closed and the button is pressed, the button tells the door to open up. And whenever the door is open and the button is released, the button tells the door to close.

<c02>

What we are witnessing here is a pair of state machines - the door and the button. Both of them have their own states and state transitions, and they run concurrently (i.e. both the door's state and button's state are active at the same time). Yet they are not disjoint from each other; the button's state influences the door's state, thereby forming a cause-and-effect relation.

We may refer to such a network of state machines as a "multi-state machine", since the whole thing behaves as though it is a state machine with multiple concurrent states.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Logic without Logic Gates</b></h3>

Let us take a look at a slightly more involved scenario. Suppose that there are two buttons called "button A" and "button B", and that there is a door which stays open only when both of these buttons are simultaneously being pressed (not just one of them, but both). This is an AND relation in boolean logic, and the image below shows an electric circuit analogy of such a logical operation. Here, the two buttons are analogous to two electrical switches which are connected in series. Only when both of them are pressed, the whole circuit becomes conductive and enables itself to activate (open) the door.

<c03>

An engineer might be tempted to implement this using a logic gate (i.e. AND gate), which is a sound approach in a general computing environment in which binary logic operators (such as &&, ||) are some of the most primitive building blocks of expressions. The temptation gets even larger when the engineer is developing a game which is meant to be executed not on a personal computing device (e.g. PC or mobile), but on a piece of programmable hardware, such as FPGA (Field-Programmable Gate Array), wherein logic is being configured by establishing connections upon a vast 2D array of pre-embedded logic gates.

Outside of the realm of digital design, however, logic gates may not be the most appropriate elements to use. A video game, for example, is often filled with a myriad of wild mechanics which are not so easily explicable in terms of binary logic operators, such as dialogues, spatial reasoning (e.g. rush hour puzzles), various causal chains, and many others.

In order to encompass most (if not all) of all these non-boolean mechanics in our design protocol, therefore, we must come up with a framework which is more versatile than a mere assembly of binary logic elements. In my opinion, such a framework can easily be discovered when we stop thinking in terms of logic gates (e.g. AND, OR, NOT, etc) and begin to design the whole system using state machines and their transition rules only.

The diagram shown below is an example of how the aforementioend "AND" logic can be implementated by introducing an intermediatry state machine called "ButtonSeq" (i.e. "Button Sequence"), whose current state indicates the most recently ongoing sequence of button-presses. For instance,

(1) "None" means that neither button A nor button B is being pressed.
(2) "A" means that only button A is being pressed.
(3) "AB" means that button A was pressed first, button B was pressed second, and that they both have been being pressed since then.

<c04>

What you can see here is that both the "AB" and "BA" states trigger the door's transition from "Closed" to "Open". This means that, whenever button A and button B are both being pressed (regardless of the order in which they were initially pressed), the door is forced to be open.

In this example alone, we are already seeing an advantage of designing mechanics in terms of state machines instead of logic gates. Logical operators such as "AND", since they are commutative (i.e. they don't care about the order of the input parameters), are insufficient for handling cases in which the order of pressing these two buttons matters, and so forth. Imagine that, inside a game studio, the game designer comes over to the engineering team, gets mad, and says,

"Hey, the way you implemented my design was wrong. When I said that pressing button A and button B should open up the door, what I meant was that pressing button A AND THEN pressing button B should open up the door. I never said that pressing them the other way around should give the same result. What you guys made is clearly a bug. The entire level won't make any sense at all if you do not fix this immediately!"

Try making the requested adjustment by rearranging logic gates; it is fairly complicated to do so. If you configure things in terms of state machines, on the other hand, you will have an easier life because state machines are much more malleable than logic gates.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Rule Dictionary</b></h3>

But of course, I can totally feel the sentiment that the above example somehow appears to "overcomplicate" a fairly simple piece of logic by not leveraging built-in logical operators. However, there is an enormous benefit in trying to formulate everything solely in terms of state machines; it is what I would call "structural uniformity".

In the model we have been seeing so far, there has really been only a single type of semantic building blocks - state transition rules. Every one of these rules, without an exception, can be interpreted as a mapping of two states (i.e. "current state" and "trigger state") into another state (i.e. "next state"). In case of the state "AB" triggering the door to open, for instance, we can say that it is due to a rule in which "current state" is "Closed", "trigger state" is "AB" (because "AB" is the thing that triggers the door to open), and "next state" is "Open".

<c05>

And if we gather all such rules in one place called "rule dictionary", we may claim that this dictionary alone fully describes the behavior of the whole system. This kind of simplicity is extremely helpful especially if we are aiming to implement it as a custom electronic module (in the form of an ASIC (Application-Specific Integrated Circuit), etc); less variety in the data structure means less complexity in the hardware.

<c06>

The most challenging part is the implementation of the rules. It really depends on the hardware, but let us begin with the assumption that we are developing an application for a general-purpose computer (e.g. PC or mobile). The most brute-force approach is to execute the rules as a bunch of conditional statements, like the ones displayed below.

#$
if (ButtonSeq.CurrState == "None" && ButtonA.CurrState == "Pressed")
    ButtonSeq.NextState = "A";
if (ButtonSeq.CurrState == "A" && ButtonA.CurrState == "Released")
    ButtonSeq.NextState = "None";
if (ButtonSeq.CurrState == "A" && ButtonB.CurrState == "Pressed")
    ButtonSeq.NextState = "AB";
if (ButtonSeq.CurrState == "AB" && ButtonA.CurrState == "Released")
    ButtonSeq.NextState = "B";
if (ButtonSeq.CurrState == "AB" && ButtonB.CurrState == "Released")
    ButtonSeq.NextState = "A";
...
if (ButtonSeq.CurrState == "AB" && Door.CurrState == "Closed")
    Door.NextState = "Open";
if (ButtonSeq.CurrState == "BA" && Door.CurrState == "Closed")
    Door.NextState = "Open";
if (ButtonSeq.CurrState == "None" && Door.CurrState == "Open")
    Door.NextState = "Closed";
if (ButtonSeq.CurrState == "A" && Door.CurrState == "Open")
    Door.NextState = "Closed";
if (ButtonSeq.CurrState == "B" && Door.CurrState == "Open")
    Door.NextState = "Closed";
#$

It is not difficult to interpret the meaning of what is going on here. If ButtonSeq's state is "None" and ButtonA's state is "Pressed", ButtonSeq's state must change into "A", and so on. Each IF statement here is an individual state transition rule.

Of course, this is not the best way of implementing the rules. It is both hard-coded (which means it is not data-driven) and inefficient (because it introduces a long sequence of statements that the program needs to visit), so obviously we need a better approach.

A better (although not the best) way is to define the rules as entries in a nested dictionary which, in C# programming language, could be written as:

#$
Dictionary<CurrStateOwner,
    Dictionary<CurrStateName,
        Dictionary<TriggerStateOwner,
            Dictionary<TriggerStateName,
                NextState>>>> RuleDictionary;
#$

"CurrStateOwner" is the state machine whose "current state" is to be examined, and "TriggerStateOwner" is the state machine whose "trigger state" is to be examined. The rule dictionary is the data structure which maps these two states (i.e. "current state" and "trigger state") to the desired future state of "CurrStateOwner".

If we represent the rule dictionary of the previous two-button scenario in JSON format, it will be written as:

#$
"RuleDictionary": {
    "ButtonSeq": {
        "None": {
            "ButtonA": {
                "Pressed": "A"
            },
            "ButtonB": {
                "Pressed": "B"
            }
        },
        "A": {
            "ButtonA": {
                "Released": "None"
            },
            "ButtonB": {
                "Pressed": "AB"
            }
        },
        "AB": {
            "ButtonA": {
                "Released": "B"
            },
            "ButtonB": {
                "Released": "A"
            }
        },
        ...
    },
    "Door": {
        "Closed": {
            "ButtonSeq": {
                "AB": "Open"
            },
            "ButtonSeq": {
                "BA": "Open"
            }
        },
        "Open": {
            "ButtonSeq": {
                "None": "Closed"
            },
            "ButtonSeq": {
                "A": "Closed"
            },
            "ButtonSeq": {
                "B": "Closed"
            }
        }
    }
}
#$

While a nested dictionary is a pretty complicated and computationally inefficient data structure, one of its upsides is that it is extremely easy to use. Whenever the program happens to run the update loop of the "ButtonSeq" object, for instance, all it has to do is access the "ButtonSeq" entry of "RuleDictionary" to look up all the rules which belong to the "ButtonSeq" object. And within that "ButtonSeq" entry, all it has to do is access the inner entry which corresponds to the current state of the "ButtonSeq" object to look up all "trigger states" which may potentially determine the object's future state.

This makes a nested dictionary a great choice for quick data access. A more optimal means of implementing this dictionary is to come up with a number of relational data tables (like in SQL) and chain them together by means of primary/secondary keys, etc.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Templates for Rules</b></h3>

Syntactic redundancy, though, is going to ensue if we construct all the rules on a case-by-case manner. Imagine that there are 100 doors inside the game, each of which opens up only when a unique pair of buttons are simultaneously being pressed. There are numerous buttons (at least 50 or so), from which 100 such unique combinations are to be made for those doors. Indeed, we do not want to manually type all the rules for a hundred "ButtonSeq" objects. Fortunately, we know that we can abstract out the logic of "ButtonSeq" (i.e. AND operator) and turn it into a template which can be replicated as many times as we want.

In order for this to happen, we must first turn all the relevant states and their transitions into generic, parameterizable entities, like the ones shown below.

<c07>

The idea is that, once they are all parameterized, we will be able to write a function which automatically creates a set of rules based on the given parameters. Such a function works as a template for generating rules, and is demonstrated in the Javascript code below:

#$
const buttonA = {
    name: "buttonA",
    states: {
        "0": "Released",
        "1": "Pressed",
    },
};
const buttonB = {
    name: "buttonB",
    states: {
        "0": "Released",
        "1": "Pressed",
    },
};
const door = {
    name: "door",
    states: {
        "0": "Closed",
        "1": "Open",
    },
};

function AND(output, input0, input1)
{
    return `{
        "AND(${output.name}, ${input0.name}, ${input1.name})": {
            "null": {
                "${input0.name}": {
                    "${input0.states["1"]}": "0"
                },
                "${input1.name}": {
                    "${input1.states["1"]}": "1"
                },
            },
            "0": {
                "${input0.name}": {
                    "${input0.states["0"]}": "null"
                },
                "${input1.name}": {
                    "${input1.states["1"]}": "01"
                },
            },
            "01": {
                "${input0.name}": {
                    "${input0.states["0"]}": "1"
                },
                "${input1.name}": {
                    "${input1.states["0"]}": "0"
                },
            },
            ...
        },
        "${output.name}": {
            "${output.states["0"]}": {
                "AND(${output.name}, ${input0.name}, ${input1.name})": {
                    "01": "${output.states["1"]}"
                },
                "AND(${output.name}, ${input0.name}, ${input1.name})": {
                    "10": "${output.states["1"]}"
                },
            },
            "${output.states["1"]}": {
                "AND(${output.name}, ${input0.name}, ${input1.name})": {
                    "null": "${output.states["0"]}"
                },
                "AND(${output.name}, ${input0.name}, ${input1.name})": {
                    "0": "${output.states["0"]}"
                },
                "AND(${output.name}, ${input0.name}, ${input1.name})": {
                    "1": "${output.states["0"]}"
                },
            }
        }
    }`;
}

const rule_door = AND(door, buttonA, buttonB);
#$

Here, the code just genereated the two-button rule (i.e. "Both button A and B must be pressed in order to open the door") by calling the "AND" function, which took the door as the operator's output parameter and the two buttons (A and B) as the operator's two input parameters.

Leveraging this function-based rule instantiator, the developer is able to quickly come up with a wide spectrum of complex rules simply by calling functions. For example, take a look at the scenario below, expressed in the form of an electric circuit. It tells us that there are two doors (A and B) and three buttons (A, B, and C); door A opens up only when button A and B are both being pressed, whereas door B opens up only when button B and C are both being pressed.

<c08>

By calling the "AND" function twice, each time with a unique combination of arguments, one can easily create two sets of rules which are different yet share the same type of logic (i.e. AND operator). These two sets of rules, when combined as a whole, will fully depict the above scenario. The following code shows us how to do it.

#$
const rules_doorA = AND(doorA, buttonA, buttonB);
const rules_doorB = AND(doorB, buttonB, buttonC);

const ruleDictionary = `{
    ${rules_doorA},
    ${rules_doorB}
}`;
#$

Here is a slightly trickier example. What if there is a door which opens up only when all 3 of the buttons are being pressed, not just 2? This may baffle the designer a bit because the AND function only takes two inputs. This problem, though, is not hard to solve. All we need is an intermediary object (i.e. "wire") whose state is the result of applying the AND function to the first two buttons (A and B). This intermediary object, then, can be fed as an input parameter to another AND function along with the third button (C). The result of this AND function will be equivalent to the result of applying the AND operator to all three buttons (A, B, and C).

<c09>

And the snippet below is the code implementation of what I just mentioned.

#$
const wire = {
    name: "wire",
    states: {
        "0": "LowVoltage",
        "1": "HighVoltage",
    },
};

const rules_wire = AND(wire, buttonA, buttonB);
const rules_door = AND(doorB, wire, buttonC);

const ruleDictionary = `{
    ${rules_wire},
    ${rules_door}
}`;
#$

Such a dynamic rule-generation script, as those of you with expertise in embedded systems may have noticed, closely resembles a hardware description language such as Verilog, since it is composed of a set of declarative statements which tell us the input-output relations between modules.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Race Conditions</b></h3>

One may question the feasibility of this multi-state design scheme by saying, "What if the system accidentally triggers an undesirable race condition? For instance, in your second example, I noticed that it is possible to cause a race condition when ButtonSeq's current state is 'None' and both button A and B are pressed at the same exact time. What should ButtonSeq's next state be - A, or B? This kind of ambiguity makes me question the legitimacy of what you are expounding here."

In a parallel or concurrent (e.g. multi-threaded) environment, such edge cases will be detrimental to the system if they are not being handled properly. They may even cause two or more mutually exclusive states to be simultaneously active! I will deal with this problem near the end of this article (i.e. in the "Race Conditions in Parallel Processing" section).

If the system is simply running on a single-threaded environment, though, we have nothing to worry about. When the system's update loop runs the update procedure of one of the state machines, it will simply scan the rule dictionary from top to bottom and apply the first rule which happens to trigger the state to change. If the rule which makes ButtonSeq respond to button A is listed BEFORE the rule which makes ButtonSeq respond to button B, simultaneously pressing both button A and B will make ButtonSeq respond to button A only. No fatal error will arise, and no one but an over-enthusiastic QA will say anything about it.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Compression of Rule Data</b></h3>

The JSON representation of the rule dictionary, which I have demonstrated above, is a bit too inefficient both in terms of memory and speed. Thus, it is desirable to format the dictionary in a more compact manner.

There are a number of things we ought to do to make it happen. First, we should reference each individual state with a binary code instead of a string of characters (as shown in the table below). This greatly reduces the amount of data needed for referencing the states. Furthermore, it helps us significantly speed up the dictionary's lookup process because a numerical value can directly be used as an index in the hash table (whereas a string requires the application to compute its hash code first).

<c10>

Secondly, we can even get rid of the nested dictionary data structure altogether if we express all the rules using two binary sequences (illustrated in the picture above), which may be referred to as "Rule Ranges" and "Rules", respectively.

The "Rule Ranges" sequence lists all the states of the system and their corresponding ranges in the "Rules" sequence (specified in terms of memory addresses). The "Rules" sequence lists all the rules which correspond to the given CurrState, each of which is denoted by the (TriggerState, NextState) pair.

When the system updates a state machine whose current state is 0101, for instance, it first looks up the range which corresponds to the entry 0101 in the "Rule Ranges" sequence (which corresponds to the range [start1, end1]). It then jumps into the "Rules" sequence, scans the entries in the range [start1, end1], and checks to see if the state 0010 or 0100 is currently active (Note: A state is "active" if it is the current state of one of the state machines). If 0010 is active, the program will change the current state of the machine from 0101 to 0110. If 0100 is active instead, the program will change the current state of the machine from 0101 to 0111.

Such raw-data representation of rules helps us minimize the size of its storage space, minimize the frequency of cache-miss (because more compactness of data means more data can be packed into the cache), and prevent potential incompatibility between ways in which different computing envirionments (e.g. operating systems) tend to handle data (because raw binary data does not make assumptions on how the environment will interpret it; the "rules engine" will simply interpret the data based upon its own custom way, regardless of on which platform it is running).

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Parallel Processing</b></h3>

Versatility and compactness are not the only advantages of the multi-state model I have shown so far. Another major advantage of it is its ability to run on a parallel computing environment, given a custom hardware module.

In order to explain what sort of hardware it is, I must first introduce a hypothetical electronic component called "state activation indicator". It is a 1-bit memory storage (similar to a D Flip-Flop) which tells us whether each state is active or not. So, for example, if there is a state activation indicator which indicates the activation status of the door's "Closed" state, we say that the door's current state is "Closed" if and only if the indicator's output bit is currently 1.

<c11>

This means that the "current state" of the whole system, which is really the ensemble of all the current states of its constituent state machines, is a binary string of length N containing M number of 1s (where 'N' is the total number of all possible states of all the state machines, and 'M' is the number of the state machines themselves). The figure below demonstrates how this data representation works.

<c12>

The system's current state can be characterized by an array of state activation indicators. Similarly, the system's next state (i.e. pending future state which is to be assigned back to the current state by the end of the application's clock cycle), too, can be characterized by an array of state activation indicators.

The image below is the hardware implementation that is necessary for the parallel multi-state machine to work. Between the two bit arrays which respectively represent the system's current state and next state, there is a two-dimensional grid of wire intersections, formed by both horizontally and vertically oriented electrical wires (i.e. conductors) which are placed at regular intervals. Each black dot denotes connection between the two intersecting wires (which means that if the value of one of the wires is 1, the value of the other wire will be 1 as well), and each green dot refers to the presence of an AND logic gate between the two intersecting wires. The result of this logic gate gets communicated through the green line.

<c13>

Here, each green dot (AND logic gate) corresponds to a state transition rule; it takes a "current state" and a "trigger state" as a pair of inputs, and yields the next state as the output. Imagine the green parts of the image above as the ones which are dynamically configured on a programmable hardware module (by means of tri-state buffers, transisters, etc), while the black parts are static and cannot be modified. Installing a rule dictionary, thus, is the same thing as establishing a bunch of green wirings between the grid's intersection points and their corresponding next-state memory input ports.

Once the wiring process is done, all we need to do is set the initial state of the system by pre-configuring the bits of CurrState and then starting the clock to initiate the cyclic flow of signals between CurrState and NextState. This will let the system update all of its states in parallel, without requiring the CPU to visit every individual state machine and update it sequentially.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Race Conditions in Parallel Processing</b></h3>

The aforementioned parallel implementation, though, is prone to introduce race conditions which may inadvertently make a state machine carry two or more concurrent states (which is inappropriate for a state machine). Therefore, we must add some kind of "postprocessing" stage to the end of the state transition cycle to resolve race conditions. For such a purpose, let me introduce yet another hypothetical module called "tie-breaker". It is an electronic component which takes a bit sequence as an input, leaves only a single "1" in it (if there are multiple of them), and returns the resulting sequence as the output.

<c14>

Then, when the system's clock cycle (update loop) ends and its NextState is to be transferred back to CurrState, the system will need to feed each group of states in NextState (corresponding to each individual state machine) to a tie-breaker and assign its result to the respective group in CurrState, instead of simply copying the bits from NextState to CurrState directly. It is demonstrated in the image below.

<c15>

This is how the system can prevent any potential race conditions; each tie-breaker ensures that each state machine contains only ONE current state.








:d:How to express a relation in an undirected graph, without using hypergraphs or any other advanced concepts? This article explains a purely node-based method of representing any n-ary relations. This will be useful for implementing custom hardware for relational computing systems, such as an optimized Prolog interpreter.
:k:Relation, Arity, Discrete Mathematics, Directed Graph, Undirected Graph, Hypergraph, Hyperedge, Hypernode, Hypervertex, Supergraph, Graph Theory, Circuit Theory, Relational Database, Topology, Prolog, Logic Programming, Knowledge Graph, GraphQL
:l:2024-08-22

[On Nodal Representation of Relations] August 22, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Introduction</b></h3>

Relations are everywhere - from discrete mathematics to computer science, from topology to data science, and innumerable other subjects. In popular areas of research such as ML (Machine Learning) and AI (Artificial Intelligence), too, relations serve as some of the most essential components of logic.

One of the problems in the study of relations, though, is the matter of representing them in a visually intelligible way. While it is possible to express any n-ary relations (for any positive integer 'n') in the form of algebraic notations (such as "xRy", "R(x,y)") or relational data table entries (such as those in an SQL database), expressing them in terms of graphical elements has been a major challenge.

When it comes to graphically depicting unary and binary relations, graph theory comes in handy. As long as we suppose that a relation is an edge and a member of a relation is a vertex, it can simply be assumed that a binary relation is an edge which connects its two members. If the relation is symmetric, it will be a undirected edge (because direction won't matter). if the relation is asymmetric, it will be a directed edge.

Similarly, an unary relation could be rendered as though it is a special case of a binary relation - that is, an edge which connects its sole member to an entity which symbolizes the absence of the other member (i.e. null).

<d01>

Now, how about ternary relations, quaternary relations, and other n-ary relations, whose arities are higher than two? It is technically possible to use the language of hypergraph (instead of just plain old graph) in which we may be free to express a relation as a hyperedge (i.e. edge with more than two vertices), and so on. We may even be able to describe the asymmetry of such a relation using the idea of a directed hyperedge.

However, a graphical representation of a hypergraph is visually too intimidating, as it no longer allows us to draw a relation simply as a line segment and instead forces us to draw colored regions, bundles of curved arrows, and a myriad of other complex visual elements. This will bloat the viewer's mind with too much abstract information.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Relation as a Composition of Atoms</b></h3>

In order to be able to draw n-ary relations in a simple, highly recognizable visual language, we must abstain from utilizing conceptual models which are way too abstruse for laymen to understand.

A hypergraph is not something which a person whose area of expertise does not considerably intersect with the domain of pure mathematics is expected to grasp easily. A graph, on the other hand, is something pretty much anyone is able to comprehend at least on a superficial level.

Therefore, it will be helpful for expressive purposes if we somehow figure out a way to represent any arbitrary n-ary relations using the rudimentary graph elements only (i.e. edges and vertices), without leveraging additional layers of abstraction. Also, it will be even more desirable to avoid directed edges and only employ undirected edges (i.e. line segments with no arrow signs whatsoever) so as to ensure utmost structural simplicity.

<d02>

For the purpose of accomplishing the above goals, let me first introduce two basic building blocks - negative nodes and positive nodes (shown above). These two types of nodes, when combined, will be able to constitute the body of any n-ary relation. I will explain the underlying reason shortly.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Unary and Binary Relations in an Undirected Graph</b></h3>

The picture below is our first example of how negative and positive nodes can be used as building blocks of a relation. What you are seeing here is a binary asymmetric relation with two member variables: X and Y. If we were using a directed graph, we would only need a directed edge (i.e. arrow) to depict such a relation. Since we are trying to describe it using an undirected graph, however, we need at least two distinct types of connected nodes between X and Y to express the sense of directionality from X to Y (I am assuming here that the relation's "direction" goes from negative to positive).

<d03>

What if there is no Y, and we are only left with X? There may be two opposing scenarios (demonstrated in the image below).

If X were connected to the negative side, the pair of nodes would be a quasi-unary relation which "points away from" X (i.e. points from X to nowhere). If X were connected to the positive side, the pair of nodes would be a quasi-unary relation which "points toward" X (i.e. points from nowhere to X). In both cases, the pair of nodes characterizes an unary relation whose sole member is X. Its "direction" in regard to X is a highly nuanced feature whose usage depends on the application.

<d04>

What if we do have both X and Y as members of the relation, yet both of them are attached to only one of the relation's pair of nodes? The image below illustrates two possible scenarios.

If X and Y were connected to the negative side, the pair of nodes would be a quasi-binary relation which "points away from" both X and Y (i.e. points from X,Y to nowhere). If X and Y were connected to the positive side, the pair of nodes would be a quasi-binary relation which "points toward" both X and Y (i.e. points from nowhere to X,Y). In both cases, the pair of nodes characterizes a binary symmetric relation whose members are X and Y (It is symmetric because there is nothing which enforces an order between X and Y). Just like in the previous case, its "direction" in regard to X and Y is a highly nuanced feature whose usage depends on the application.

<d05>

In the language of bidirected graphs, these two cases may be interpreted as an "introverted edge" and an "extraverted edge", respectively (See {%a href="https://en.wikipedia.org/wiki/Bidirected_graph"%}Bidirected Graph{%/a%}).

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Symmetric Relations</b></h3>

So far, I have only demonstrated relations which possess their own directions (i.e. asymmetry). It is, however, also feasible to construct a symmetric relation if we model it as though it is a pair of asymmetric relations pointing in two opposite directions. Its unary and binary examples are showcased below.

<d06>

Just like before, we are presuming here that each relation is a connected pair of dual particles (i.e. negative node and positive node). Recall the previous examples, and see how the new ones differ from them.

Suppose that X is the only member of the relation. If X is connected to both the negative and positive sides, we will be able to say that the relation points both toward and away from X. This means that it is an unary relation which associates itself with X without specifying any particular direction in regard to it.

Suppose, on the other hand, that X and Y are both members of the relation. If X and Y are simultaneously connected to both the negative and positive sides, we will be able to say that the relation points both toward and away from X and Y. This means that it is a binary relation which associates itself with X and Y without specifying any particular direction between them.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>N-Ary Relations in an Undirected Graph</b></h3>

The main benefit of using dual (negative and positive) nodes as means of formulating relations can be found in the case of n-ary relations, where 'n' is an arbitrary integer which is greater than 2.

Traditionally, it has been presumed that only binary relations can adequately be rendered in the standard graph notation (i.e. edges and vertices). What I would like to suggest in this article is that, by means of negative and positive nodes, it is possible to represent even an n-ary relation (where n > 2) in a plain undirected graph (That is, by drawing only dots and lines on a sheet of paper, along with a bit of symbolic characters such as '-', '+', and alphanumerics).

The image below shows three examples of the idea I just mentioned, the first two being ternary relations and the last one being a quaternary relation.

The first case depicts an instance of "merge" - i.e. a scenario in which two separate things (X1,X2) merge into one thing (Y). The order of the things which are being merged does not matter, so this relation is symmetric with respect to X1 and X2.

The second case depicts an instance of "split" - i.e. a scenario in which one thing (X) splits into two things (Y1,Y2). The order of the things which are being produced by the split does not matter, so this relation is symmetric with respect to Y1 and Y2.

The third case depicts an instance of "exchange" - i.e. a scenario in which two things (X1,X2) interact with each other, exchange resources, and end up turning themselves into a pair of things (Y1,Y2) which are somewhat different from the previous two. Since this particular act of exchange does not imply a sense of directionality from one thing to the other, this relation is symmetric with respect to both the former pair (X1,X2) and the latter pair (Y1,Y2).

<d07>

What about n-ary relations which require a fixed order between its members? The image below illustrates ways in which such a strict sense of asymmetry can be achieved.

The first case indicates an instance of "split", yet the outcome of the split is a vector of two components (Y1,Y2) instead of a set. Here, the order between Y1 and Y2 is being enforced by the order in which the two positive nodes are connected with respect to the negative node.

The second case indicates an instance of "merge", yet the ingredients of the merging process is a vector of two components (X1,X2) instead of a set. Here, the order between X1 and X2 is being enforced by the order in which the two negative nodes are connected with respect to the positive node.

<d08>

In general, it is definitely feasible to construct any arbitrary n-ary relation and selectively make parts of its list of members either symmetric or asymmetric, by means of dual (negative and positive) nodes connected in series. The number of connected negative (or positive) nodes will imply the number of members in a group whose order is strictly enforced (as though they are elements of a vector), and the number of parallel connections to each negative (or positive) node will imply the number of members in a group whose order does not matter (as though they are elements of a set).

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Practical Applications</b></h3>

Visualization is not the only advantage which can be gained by the aforementioned methodology. One of the upsides of modeling a concept in the form of a plain undirected graph (instead of something fancy such as hypergraph) is that it can easily be translated into a physical equivalent such as an electric circuit (which is useful for applications in science/engineering).

A vertex or a connected set of vertices, for example, can be modeled as a circuit component (e.g. logic gate, amplifier, buffer, arithmetic module, memory module, etc), and an undirected edge can be modeled as a wire which establishes an electrical connection between a pair of circuit components.

If engineers happen to desire to create a specialized computational system (such as a custom circuit) which uses relations as its basic unit of computation, therefore, they will find the nodal representation of n-ary relations to be a helpful means of devising a piece of hardware in which each relation is implemented not as a piece of abstract data inside a random-access array, but as a real physical device which occupies an area on the circuit board.

This kind of implementation will be optimal for interpreting a logic programming language such as Prolog, which is based upon relations and their mappings (i.e. horn clauses).

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Further Readings</b></h3>

1. {%a href="https://thingspool.net/reality/page-4.html"%}The Origin of Reality - Volume 4{%/a%}
2. {%a href="https://thingspool.net/reality/page-7.html"%}The Origin of Reality - Volume 7{%/a%}
3. {%a href="https://thingspool.net/metaphysics/page-59.html"%}연결의 종류 (Korean){%/a%}
4. {%a href="https://thingspool.net/metaphysics/page-61.html"%}이진법적 공간의 배열 (Korean){%/a%}
5. {%a href="https://thingspool.net/metaphysics/page-62.html"%}공간의 근본적 형태들 (Korean){%/a%}








:d:This article explains how a game can be made in Prolog by leveraging the hidden power of logic programming. Inspired by the elegance of data-driven design and declarative programming paradigms, I will be explaining a new way of designing game rules which can at the same time be used as the source code of the gameplay system itself.
:k:Prolog, Logic Programming, Declarative Programming, Data Driven Design, GM Tool, Technical Game Design, Gameplay Systems, Emergent Systems, Game Design, Game Development, Game Systems Design, Game Mechanics, Systems Engineering, Design Patterns, Simulations, Computer Science, Computer Engineering, Software Engineering
:l:2024-08-25

[Game Programming in Prolog - Part 1] August 25, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Introduction</b></h3>

As a fan of unconventional programming paradigms, I enjoy learning new programming languages which are drastically different from the typical object-oriented ones such as C#, Java, and the like. The most iconic of them are LISP (which is a powerful language for both functional programming as well as metalinguistic patterns in software development) and Prolog (which is one of the most popular languages in logic programming). Learning these languages is quite hard, compared to being acquainted with usual C-style imperative languages such as Ruby and Python, yet it has turned out to be one of the most effective ways of exercising one's brain.

By the time I started learning LISP via MIT's 1986 lecture series called "SICP (Structure and Interpretation of Computer Programs)" back in 2018, I was already quite familiar with some of its core concepts (such as lambda expressions, higher-order functions, etc) because they were already integrated as some of the main features of C#, which was the language I was using all the time as a Unity game developer. Also, my academic background in electrical engineering (signal processing in particular) helped me easily grasp the idea of "stream processing" which appeared in the latter half of the lecture series. Thus, learning LISP and its functional design patterns was not as difficult as I imagined it to be.

A major intellectual challenge, however, struck me when I began to study Prolog - the famous logic programming language which is notorious for its esoteric syntax. The grammar itself did not appear to be complicated at all; it was just as minimal as that of LISP. The way in which programming had to be done in Prolog, though, was stressful enough to fry the engine of my brain. The way it approached data structures (such as lists) and algorithms based upon mathematical relations was something so revolutionarily novel to me, that it seriously opened up a new horizon in my faculty of computational reasoning.

While Prolog's approach in software development was quite alien to me, I managed to notice a number of familiar associations between Prolog and many useful topics in engineering. I discovered, for example, that the so-called "relational databases" (e.g. MySQL) are named so not because they comprise data tables which are related to each other via references, but because each row of a data table can be considered an n-ary predicate (where 'n' is the number of columns in the table) in Prolog's syntax. Besides, I found out that the input/output behavior of each digital circuit component (e.g. logic gate) could be implemented as an n-ary relation (where 'n' is the total number of the input/output ports combined), implying that an "object", whether it be a piece of hardware or a piece of pure data in memory, may as well be defined as a relation in logic programming (just like an object may as well be defined as a function in functional programming). Furthermore, the declarative nature of Prolog strongly convinced me that it must be optimal for data-driven design.

These realizations soon led me to contemplate upon the notion that, maybe, logic programming has a great deal of potential in the design and implementation of highly complex systems, such as a video game's core gameplay mechanics. I began to ask myself, "Will it be possible to develop an entire game using the grammar of logic programming?"

Indeed, there are reasons why most game developers just stick to general-purpose programming languages (such as C#) for making games, aside from purely experimental purposes. Implementing an entire game based on Prolog, for instance, is perhaps too much of a challenge for those who are not hardcore mathematicians. Also, Prolog may not be the best language to use for parts of the project which are not necessarily made of a complex web of relations, such as simple I/O modules, graphics modules, audio modules, physics modules, and the like.

However, I believe that at least the core mechanics of a game can definitely be implemented using the language of Prolog, and that we will be able to solve a plethora of complex design problems by doing so. It is because a gameplay system which is structured in terms of a set of declarative statements will be far more robust, modular, and free of confusing edge cases (e.g. race conditions) than an imperative system.

For this alternative methodology to be successful, one must start by designing the system in terms of logical relations/predicates only, and nothing else (That is, no functions, no structs, no classes, no interfaces, no state variables, etc). This will allow us to construct a gameplay system which is purely driven by the soul of Prolog.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>World and Actors</b></h3>

The core idea in Prolog-based game programming is to utilize relations as the most primitive building blocks of the system, just like basic circuit components (e.g. resistors, transistors, capacitors, inductors, etc) are the most primitive building blocks of an electric circuit. It is sensible, therefore, to start this journey by considering the most rudimentary relations (e.g. unary and binary) first, and see if these elements can serve as the most essential nuts and bolts of the game.

<e01>

Suppose that we are designing a game, and that the game consists of two major parts - world and actors (see the image above). The world is a scene in which everything is supposed to happen, and actors are objects which belong to the world. Examples of actors include "players", "enemies", "obstacles", "items", and pretty much any discrete entities which have their own names and attributes. Actors are able to interact with each other (as well as with themselves), from which various events occur. What we refer to as "gameplay" is a chain of such events.

We will begin formulating a gameplay system based off of this conceptual backbone. All you need to remember is that there is a world, and that the world contains a number of actors, each of which possesses its own state and behavior.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Tags</b></h3>

First of all, let us identify each individual actor with a unique name. If there are two actors in the world, for instance, we will simply assume that the name "actor1" and "actor2" will be used to indicate the first and second actors, respectively.

<e02>

The first piece of logic I will illustrate is the idea of tags. A tag is a keyword which, when attached to an actor, describes what the actor stands for. When an actor has the tag "bread" attached to it, for example, we should be able to tell that the actor is a piece of bread.

The Prolog code below assigns the tag "bread" to both actor1 and actor2, in the form of unary predicates (The tag "bread" itself is an unary relation, and "bread(actor1)" & "bread(actor2)" are two separate instances of it). This implies that both actor1 and actor2 are pieces of bread.

#$
bread(actor1).
bread(actor2).
#$

<e03>

An actor can have multiple tags as well. However, one may feel that it is a bit too tedious to manually assign a bunch of tags to each individual actor. For example, let us say that every piece of bread must also be labeled as flammable and decomposable. This means that, whenever an actor is associated with the tag "bread", we are obliged to always ensure that it is also associated with the tag "flammable" and "decomposable". Manually attaching these two additional tags to every "bread" actor is way too cumbersome and error-prone. Fortunately, the following pair of horn clauses neatly solve this problem. They enforce the following two rules:

(1) Whenever tag "bread" is assigned to actor X, tag "flammable" will automatically be assigned to actor X.
(2) Whenever tag "bread" is assigned to actor X, tag "decomposable" will automatically be assigned to actor X.

#$
flammable(X) :- bread(X).
decomposable(X) :- bread(X).
#$

<e04>

These horn clauses, therefore, serve as part of the game's "config data" - a list of data entries in the game's technical design document (like the ones you would see on a spreadsheet) telling us the characteristics of each individual character type, skill type, mission type, and so forth. The tags called "flammable" and "decomposable" in our case, for instance, are characteristics which belong to the type-specifier called "bread", meaning that any actor which can be identified as "bread" is a composition of two properties called "flammable" and "decomposable".

A decent analogy can be found in Unity game engine, where we may create a prefab called "Bread" with two components in it - "Flammable" and "Decomposable". Or, in a general object-oriented programming environment, "Bread" may stand for the name of a class which implements two interfaces called "IFlammable" and "IDecomposable".

In a way, therefore, horn clauses in Prolog play the role of data type definitions.

<e05>

Aside from these pre-configured tags (which all rely on the presence of the tag "bread"), one may as well attach a custom tag to an actor as needed. For example, imagine that a wizard happened to enchant actor2 (i.e. the second piece of bread). This means that, unlike actor1 which is an ordinary piece of bread, actor2 must be an "enchanted" piece of bread which is required to have the tag "enchanted" attached to it for the purpose of showing us that it has been enchanted. The code below ensures that this is the case.

#$
enchanted(actor2).
#$

<e06>

The tags "flammable" and "decomposable" are characteristics of all pieces of bread, whereas the tag "enchanted" is a characteristic of only special pieces of bread which have been enchanted by a wizard.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Relationships</b></h3>

So far, we have been using tags for specifying the characteristics of each individual actor. In a gameplay system, however, we also need to specify relationships between actors, such as ways in which they interact, etc.

In an ecosystem, predators chase preys and preys run from predators. In a dating simulator, a guy tries to flirt with girls and girls reject him. In a social simulator (such as The Sims), people are either friends or enemies of each other, or somewhere in between. In the game of chess, a bishop devours a rook diagonally and a rook devours a bishop orthogonally. These are all relationships out of which the game's dynamics emerge.

Defining actor-to-actor relationships in Prolog is pretty straightforward. Just like an unary predicate can be used to characterize a single actor, a binary predicate can be used to characterize a relationship between a pair of actors. And by means of a horn clause, such a relationship can be dynamically deduced from a set of requisite conditions.

The following code is an example of a relationship. Suppose that there is a third actor called "actor3", and that we have declared it as a human (by attaching the tag "human" to it). Since a human is able to eat a piece of bread, we can confidently assert that "X can eat Y if X is a human and Y is a piece of bread". Here, "X can eat Y" is a relationship which holds whenever X is associated with tag "human" and Y is associated with tag "bread".

#$
human(actor3).
canEat(X, Y) :- human(X), bread(Y).
#$

<e07>

Here is another example. Since a piece of bread is decomposable (because anything which is identified as "bread" must also be identified as "decomposable"), we know that microbes such as fungi are capable of spoiling it. If there is an actor with the tag "fungus" attached to it, therefore, we will be able to tell that it must be able to spoil any other actor which is "decomposable". This is yet another case of a relationship between two types of actors; it is a relationship which says, "X can spoil Y if X is a fungus and Y is decomposable". The following code shows its definition.

#$
fungus(actor4).
canSpoil(X, Y) :- fungus(X), decomposable(Y).
#$

<e08>

There is something still missing here, though. While I have demonstrated that it is possible to assign characteristics to individual actors as well as their mutual connections (i.e. relationships), I have not shown yet how to make these characteristics change over time. They all have been static so far, and the declarative nature of Prolog does not seem to offer an easy solution to make things dynamic.

If we want to create a game rather than a fixed landscape of how things are shaped permanently, we better let them move and interact as time goes by. In the next part of the series, I will explain how the game loop shall be conceptualized in Prolog.

(Will be continued in {%a href="https://thingspool.net/morsels/page-11.html"%}Part 2{%/a%})









:d:This article explains how a game can be made in Prolog by leveraging the hidden power of logic programming. Inspired by the elegance of data-driven design and declarative programming paradigms, I will be explaining a new way of designing gameplay systems which is way more robust, concise, and error-free than traditional methods.
:k:Prolog, Logic Programming, Declarative Programming, Data Driven Design, GM Tool, Technical Game Design, Gameplay Systems, Emergent Systems, Game Design, Game Development, Game Systems Design, Game Mechanics, Systems Engineering, Design Patterns, Simulations, Computer Science, Computer Engineering, Software Engineering, Ludology, Game Science, LISP, Functional Programming
:l:2024-08-29

[Game Programming in Prolog - Part 2] August 29, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Before You Read...</b></h3>

This is Part 2 of the series, "Game Programming in Prolog". In order to understand what is going on in this article, please read {%a href="https://thingspool.net/morsels/page-10.html"%}Part 1{%/a%} first.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Numerical Attributes</b></h3>

So far, I have been demonstrating ways in which we can assign tags and relationships to each of the gameplay agents (aka "actors"). The key takeaway is to use predicates to specify them, as well as leverage the power of logical relations for letting the program automatically instantiate such predicates.

By the same spirit, we are also able to assign a numerical attribute to an actor. Suppose that an actor called "actor3" is tagged "human", and that we would like to ensure that every human actor has an attribute named "numLegs" which indicates the person's number of legs (i.e. 2). The following horn clause, then, will fulfill this objective.

#$
numLegs(X, 2) :- human(X).
#$

<e09>

The binary predicate, "numLegs", is a numerical attribute of every human actor which tells us that the actor's number of legs is 2. This differs from a simple tag (i.e. keyword) in the sense that it also contains a number. This allows us to specify different "numLegs" values to different species of actors, like the ones shown below.

#$
numLegs(X, 2) :- human(X).
numLegs(X, 4) :- dog(X).
numLegs(X, 4) :- cat(X).
numLegs(X, 3) :- martianTripod(X).
#$

<e10>

If every martian tripod were a faithful reader of George Orwell and happened to interpret every single phrase of his novel "Animal Farm" in the most blatantly literal manner, it would be reasonable to conclude that a martian tripod is likely to protect four-legged creatures and kill two-legged creatures ("Four Legs Good, Two Legs Bad"). These behavioral patterns can be implemented using horn clauses, which are illustrated below.

#$
shouldProtect(X, Y) :- martianTripod(X), numLegs(Y, 4).
shouldKill(X, Y) :- martianTripod(X), numLegs(Y, 2).
#$

<e11>

And of course, it is equally feasible to devise a numerical attribute which involves multiple actors, similar to the concept of relationship I have demonstrated before. For instance, imagine that a dog's degree of loyalty to a human being is 6, while a cat's degree of loyalty to a human being is only 2. These two numerical relationships can be modeled as two slightly different ternary relations, like the ones shown below.

#$
loyalty(X, Y, 6) :- dog(X), human(Y).
loyalty(X, Y, 2) :- cat(X), human(Y).
#$

<e12>*

This sort of reasoning can be expanded indefinitely. For example, one may as well define a numerical attribute which carries not just a single number, but multiple numbers (i.e. vector quantity). One may also define a relationship which involves not just two actors, but three or more actors, such as: "This girl hates her boyfriend for showing too much affection to the other girl", etc.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>State and Time</b></h3>

Things have been looking good so far. We know how to create attributes and relationships, as well as how to assign them to our gameplay agents, and so forth. However, we cannot make a game out of these building blocks alone.

What has been missing here is a sense of change over time. We want actors to move, interact, and make impacts upon the world as well as upon themselves. What we've got so far, instead, is a mere snapshot of how things are related to each other; there is no moving part at all.

So, how to turn this static world into something dynamic? First of all, let us recall the way in which an imperative programming language would approach this problem. In a typical imperative language such as C, C++, or Java, creating a sense of change is simple and straightforward.

Suppose that there is a clock which ticks at regular intervals. Every time it ticks, it calls a function called "Update". If there is an actor who is supposed to get hungrier and hungrier as time passes by, all we have to do in an imperative language is to access the actor's state variable called "hunger" and increase its value whenever the "Update" function runs (See the code below).

#$
void Update(Actor x)
{
    x.hunger = x.hunger + 1;
}
#$

This kind of logic is possible because the variable we are dealing with (i.e. hunger) is a state variable; we are allowed to assign a new value to it at any moment.

In a declarative language such as Prolog, unfortunately, we cannot just declare a state variable and modify its value whenever we want to. Logical relations are timeless beings; they exist outside of the realm of time, which means that it is nonsensical to try to associate them with variables which are bound to certain points in time.

What do we do, then? In order to mimic state transition in logic programming, we must approach the concept of time from a different angle. Rather than trying to directly manipulate the current state of the game while it is running, we ought to instead define a set of time-invariant statements which tell us how the past and present are related.

<e13>

The figure above illustrates the core difference between the imperative and declarative means of running the game. Suppose that the game's state is being recorded in the computer's memory space (e.g. RAM), which is just an array of data slots.

In the imperative case, there is one chunk of data called "state". The game looks up this chunk of data, computes the new state, and overwrites this new state on top of the original chunk of data. This is what the assignment operator (i.e. "=") does in an imperative language.

In the declarative case, on the other hand, direct mutation of data is prohibited. At the beginning of each frame, the game first accesses the chunk of data at which the previous state was located. It computes the new state based on the previous state, and allocates this new state to a location which is currently not being used. The system does not tamper with the previous state; it simply appends the new state to the history of states without erasing or modifying the existing data.

<e14>

The main advantage of this approach is that it gracefully prevents race conditions. Since it does not "change" any existing piece of data, it never has to worry about inadvertently disrupting another computational process which may have been accessing the same location in memory.

Of course, continually adding new copies of the game's state without deleting anything is too wasteful. Such an ever-growing list of states (which altogether constitute the game's "history"), unless the gameplay is either turn-based or very short in duration, is likely to eat up too much space in memory. This is clearly not desirable.

Such a problem, however, can easily be mitigated by limiting the maximum duration of time through which an event's effect is able to propagate. For example, if the game's current state is entirely determined by events which happened only up to N steps back in time, it will imply that the system only needs to retain the memory of N previous states (which corredpond to the N previous time steps) and nothing older than that. As you can see in the image below, this means that memory slots which are sufficiently old can simply be recycled for other purposes.

<e15>

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>State Transition</b></h3>

So, how do we implement such a declarative state transition mechanic in the language of Prolog?

Let us first examine how the functional paradigm would approach this problem. In functional programming (e.g. LISP), the game's "Update" function simply needs to take the previous state of the game as the input, instantiate the new state based off of the given previous state, and return this new state as the output. The returned output will then be appended to the game's state history as the most recent state, and the game loop (which is another function which is responsible for calling the "Update" function) will call the "Update" function once again, and again, and again, and so on, thereby periodically updating the game (For more details, please read: {%a href="https://thingspool.net/software-development/page-21.html"%}Functional Programming for Game Development{%/a%}).

In logic programming (e.g. Prolog), on the other hand, we cannot use such a functional methodology because functions are not a thing here. Instead, we must specify relations between the current and previous states, in a manner which resembles that of the so-called "difference equations" in mathematics.

In order to demonstrate how it works, let me first augment the syntax of Prolog a bit by introducing a number of additional symbols. These are not part of the standard Prolog (which means whichever Prolog interpreter you use won't be able to recognize them), so please keep that in mind. Any Prolog code you are going to see from now on should be taken as pseudocode, meant to serve as a mere proof of concept.

The snippet below is a list of notations which will be used to illustrate the game's temporal relations.

#$
n = (current time step)
n-1 = (previous time step)
X[n] = (X at the current time step)
X[n-1] = (X at the previous time step)
X = (X at any time)
#$

Suppose that the game loop keeps track of time in discrete time steps, beginning with n = 0 and periodically incrementing it one by one (i.e. n = 1, n = 2, n = 3, etc). The symbol 'n' refers to the game's current time step, which means that 'n-1' refers to the previous time step, 'n-2' refers to the previous-previous time step (i.e. two steps back in time), and so on.

What's important here is the bracketed notation (e.g. "X[n]"). So far, I have only shown Prolog statements which involved timeless entities. Things like "actor1", "actor2", and "actor3", for instance, involved no concept of time whatsoever. Thus, there was no need to state their associations with respect to time.

When it comes to indicating an actor at a specific point in time, on the other hand, we can no longer just stick to a simple notation such as "X" because, if we do, we will be referring to the presence of the actor throughout the entirety of time. So in this case, we ought to attach an additional time parameter to the actor's identifier (e.g. "X[n]" for current X, "X[n-1]" for previous X, etc).

<e16>

Before elaborating further, let me first introduce a couple of arithmetic relations which I will be using quite frequently from now on (See the snippet below). The "equal(...)" relation holds whenever its parameters are equal in value, meaning that "equal(3, 3)" and "equal(5, 5)" are TRUE, whereas "equal(1, 2)" and "equal(3, 5)" are FALSE. The "add(...)" relation holds whenever the sum of its first two parameters yields the value of its last parameter, meaning that "add(2, 2, 4)" and "add(3, 4, 7)" are TRUE, whereas "add(1, 2, 5)" and "add(4, 0, 6)" are FALSE. The "multiply(...)" relation works in a similar fashion.

#$
equal(A, B) = (TRUE if A = B)
add(A, B, C) = (TRUE if A + B = C)
multiply(A, B, C) = (TRUE if AB = C)
#$

I will now explain how time-dependent relations can be used to implement gameplay dynamics. Imagine that there is an actor which is tagged as "human". Also, let us assume that this actor was spawned at some point in time. What we want here is to let this actor have its own state variable called "hunger", which starts at 0 (when the actor spawns) and increments itself by 1 every time the clock ticks.

The code below shows how it can be formulated in terms of executable rules.

#$
hunger(X[n], 0) :- human(X[n]), spawnTime(X, n).
hunger(X[n], Curr) :- hunger(X[n-1], Prev), add(Prev, 1, Curr).
#$

<e17>

The first horn clause says that, if there is a human actor who just spawned right at the present moment (i.e. "n"), we must initialize its hunger to 0. This clause gets executed only once when the actor spawns, since it is the only moment at which "spawnTime(X, n)" can be evaluated as TRUE (Note: "spawnTime(X, n)" basically asks the question, "Is the current time the same as X's spawn time?").

The second horn clause says that, if there was an actor which had a state variable named "hunger" during the previous time step, its current hunger must be 1 greater than the previous hunger. This clause gets executed each time the clock ticks (i.e. whenever the time step increments by 1). It does NOT get executed during the moment at which the actor spawns, since "X[n-1]" is nonexistent during that time.

The first clause initializes the hunger, and the second clause periodically increments the hunger (because a human being is supposed to get hungrier and hungrier as time goes by).

But of course, the game will be pretty boring if all we can do is watch a human character starve. If we are to design a life simulator (like The Sims), for instance, there better be a way to quench the person's hunger by letting him/her eat some food.

Here is a bit of a trouble, though. We already have a rule which tells us that the hunger must increase by 1 each time the clock ticks. If we add a new rule which describes how much the hunger must go down when the actor eats food, this new rule will be incompatible with the existing one because it is logically contradictory to have two different horn clauses which are both trying to define the same piece of data (i.e. "X[n]") simultaneously.

There is a pretty neat solution to this, fortunately. All we have to do is separately compute the amount of natural increment in hunger (aka "naturalChangeInHunger") and the amount of reduction in hunger due to the act of eating (aka "digestiveChangeInHunger"), and then combine them together into a single differential. Their implementations are shown below.

#$
naturalChangeInHunger(X[n], 1) :- hunger(X[n], _).
digestiveChangeInHunger(X[n], ChangeInHunger) :- hunger(X[n], _), eat(X[n], Food[n]), calories(Food[n], NumCalories), multiply(NumCalories, -1, ChangeInHunger).
digestiveChangeInHunger(X[n], 0) :- hunger(X[n], _), !eat(X[n], Food[n]).
#$

<e18>

The first clause is easy to understand; it simply states that the natural change in hunger is always 1 (i.e. If the actor doesn't do anything, it naturally gets hungrier by the degree of 1 after each time step). The meaning of the second/third clauses is that, if an actor is currently eating some food, its hunger must be going down by the number of calories in the food (or 0 if the actor is not eating anything. This negatory relation is denoted by "!eat(...)").

One of the notable benefits of such mutually independent clauses is that they can run in parallel (by means of multi-threading or even GPU-based programs such as "compute shaders"). This provides us with yet another reason why logic programming is a great paradigm for gameplay engineering.

Anyways, once the application obtains the results of "naturalChangeInHunger" and "digestiveChangeInHunger", the only remaining step is to sum up these two results (which will be carried out by the predicate called "netChangeInHunger") and then add this sum to the actor's hunger, just as shown below.

#$
netChangeInHunger(X[n], NetChange) :- naturalChangeInHunger(X[n], Change1), digestiveChangeInHunger(X[n], Change2), add(Change1, Change2, NetChange).

hunger(X[n], Curr) :- hunger(X[n-1], Prev), netChangeInHunger(X[n-1], NetChange), add(Prev, NetChange, Curr).
#$

<e19>

(Will be continued in {%a href="https://thingspool.net/morsels/page-12.html"%}Part 3{%/a%})









:d:This article explains how you can use the language of Prolog to develop a game. It leverages the built-in syntactic elegance of Prolog, as well as its data-driven and declarative design philosophy.
:k:Prolog, Logic Programming, Declarative Programming, Functional Programming, Data Driven, LISP, Scheme, MIT Scheme, Metalinguistic, Systems Engineering, Signals And Systems, Linear Systems, DSP, Digital Signal Processing, Game Design, Game Mechanics, Game Programming, Game Development, Ludology, Game Science, Game Research, AI, Artificial Intelligence, Discrete Math, Computer Science, Data Structure, Algorithm, Theory Of Relativity, Concurrent Programming, Multithreading, Parallel Computing
:l:2024-09-02

[Game Programming in Prolog - Part 3] September 2, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Before You Read...</b></h3>

This is Part 3 of the series, "Game Programming in Prolog". In order to understand what is going on in this article, please read {%a href="https://thingspool.net/morsels/page-10.html"%}Part 1{%/a%} first.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Causality</b></h3>

So far, I have shown how the parameterization of time steps can indeed be a powerful tool for triggering state transitions in the system. In order to better understand the reasoning behind this, however, we ought to take a step back and really try to see the overall picture of what is going on behind the formulas.

Whenever we are dealing with the system's state and its means of transition, we are essentially thinking in terms of causes and their corresponding effects. A horn clause, for instance, is a rule which tells us what kind of effect can be generated based upon a given set of causes, just as illustrated below.

#$
effect(...) :- cause1(...), cause2(...), cause3(...).
#$

<e20>

A nice way of visualizing such a causal phenomenon is to imagine each of the predicates (i.e. either a cause or effect) as a point in spacetime.

Spacetime is simply a dimensional representation of the game's history; it consists of spatial axes as well as a time axis. When you throw a rock in the upward direction, for example, you will be able to plot its trajectory in spacetime as a parabola (because it will rise, stop, and fall).

Here is the catch, though. An effect is a product of causes, yet such an effect itself may as well be a cause of another effect. Thus, it does not quite make sense to try to establish a strict distinction between causes and effects. Depending on how our causal chains are interwoven, an effect could as well be identified as a cause and vice versa.

A unifying terminology between cause and effect is "event". Our spacetime is filled with events; each point in spacetime is an event, and an directed line segment (i.e. arrow) between two points in spacetime is a causal connection which leads one event to another. The code below is an example of a horn clause which defines "event3" as a product of two such connections (one between "event1" and "event3", and the other one between "event2" and "event3"). Here, "event1" and "event2" are the causes of "event3", and "event3" is the effect of "event1" and "event2".

#$
event3(...[n]) :- event1(...[n-1]), event2(...[n-2]).
#$

<e21>

In general, our gameplay system can be thought of as a collection of causal rules, each of which specifies a type of event which can be generated out of a set of preceding events. These rules, as they get applied to the game loop over and over as time elapses (in a periodic manner), gradually map out the fabric of causal connections in spacetime, revealing us the full picture of which events are related to which. This implies that the full history of the gameplay itself can be modeled as an "event graph" - an instance of a DAG (Directed Acyclic Graph), which is reminiscent of the so-called "blockchain", "hashgraph", and other event sourcing protocols.

"But," someone might say, "But! Don't you think that not all predicates represent events? If a horn clause happens to contain a relation called 'bread(X)', for instance, it must be obvious that this particular relation simply serves as a tag which declares that X is a piece of bread. It is by no means an event; it is just an indicator, and nothing more than that."

Such a line of thought definitely makes sense from a layman's point of view. An indicative relation such as "bread(X)", as it comprises a simple noun, is indeed something which feels hardly anything more than a mere semantic reference. From a strictly spacetime-oriented perspective, however, one must be able to consider every logical predicate as an event, even if it happens to serve as an identifier.

Let me show you an example. Suppose that there is an arbitrary hydrogen atom called "X". The identity of such an atom can be described by the code below:

#$
hydrogen(X).
#$

<e22>

What this relation really means, though, is: "There is an atom called 'X' which can be identified as 'hydrogen' at every moment of its existence". Thus, a more comprehensive means of expressing this relation would be the one shown below:

#$
hydrogen(X[n]) :- spawnTime(X, n).
hydrogen(X[n]) :- hydrogen(X[n-1]).
#$

<e23>

The true meaning of "hydrogen(X)" is that there is a chain of events in spacetime which consistently keep telling us that there has been a hydrogen atom called "X", whose line of existence began at X's moment of birth (aka "spawnTime") and has henceforth been growing itself through the passage in time. The first horn clause establishes the base case (i.e. first occurrence of the "hydrogen" event), and the second horn clause establishes the recursive case which generates the succeeding chain of "hydrogen" events.

In a way, therefore, a simple name tag such as "hydrogen(X)" can be interpreted as a connected sequence of points (events) in spacetime. The key takeaway here is that the very concept of "object" (i.e. a distinct body of existence) itself should be understood as a line in the hyperdimensional geometry of our universe, similiar to what physicists refer to as a "world line" - a four-dimensional path of an object in spacetime.

And of course, when individual atoms bond with one another, they form a molecule. Such a molecule, too, can be considered a discrete object with its own line of existence in spacetime.

An example case is demonstrated below. When two hydrogen atoms (X, Y) and an oxygen atom (Z) bond, they altogether form a water molecule. In this case, the birth of the water molecule (which is an event) may as well be considered an effect which was produced by the four causes listed below:

(1) The existence of the first hydrogen atom X at time n-1.
(2) The existence of the second hydrogen atom Y at time n-1.
(3) The existence of the oxygen atom Z at time n-1.
(4) The bonding of the aforementioned three atoms at time n-1.

#$
water(X[n], Y[n], Z[n]) :- hydrogen(X[n-1]), hydrogen(Y[n-1]), oxygen(Z[n-1]), bond(X[n-1], Y[n-1], Z[n-1]).
#$

<e24>

The inverse scenario of the bonding process is the act of split, which in this case can be depicted as the splitting of the water molecule into its individual component atoms (2 hydrogens and 1 oxygen). This, too, should be able to be rendered as a set of causal connections between events, just as shown below.

#$
hydrogen(X[n]) :- water(X[n-1], Y[n-1], Z[n-1]), split(X[n-1], Y[n-1], Z[n-1]).
hydrogen(Y[n]) :- water(X[n-1], Y[n-1], Z[n-1]), split(X[n-1], Y[n-1], Z[n-1]).
oxygen(Z[n]) :- water(X[n-1], Y[n-1], Z[n-1]), split(X[n-1], Y[n-1], Z[n-1]).
#$

<e25>

A Prolog-based gameplay system is extremely straightforward in nature, if you think about it for a second. In this virtual universe, everything is an event and events are causally related to each other via horn clauses (aka "rules").

As time passes by, events which are sufficiently old (i.e. so old that they no longer influence any of the future events) get discarded because they are obsolete. Meanwhile, the game loop keeps ticking its clock, generating new events in spacetime (i.e. those which belong to time 'n'). These new events get registered to the memory, while the oldest ones get thrown away. This means that there is a "window of remembrance" in spacetime which covers events that are fairly recent (see the image below).

<e26>

Events which fall within this window are the ones which are being kept in the computer's memory. As old events get discarded (i.e. exit the window through the left edge), their corresponding memory slots get freed up. And as new events get created (i.e. enter the window through the right edge), they get stored in these freed up slots. Thus, the same array of memory slots get recycled over and over again, just like in any other dynamic memory allocation system. This proves that Prolog is a sound choice even from the perspective of memory optimization.

(Note: If you force every event to occupy the same exact amount of space in memory, you will be able to simplify the allocation scheme even further because the system will only need to keep track of free slot indices, not how large those free slots are.)

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>On Theory of Relativity</b></h3>

Here is something I would like to point out before proceeding to the next chapter. The model of spacetime which I have explicated by far is a strictly Newtonian one, meaning that every "present event" is nicely aligned within the same exact time step (i.e. "n"). This is because all present events get generated by the gameplay system in a completely synchronous (aka "lockstep") manner. Here, time is an independent variable and we can easily tell which events are simultanous with each other and which ones are not. If two or more events belong to the same time step, they must be considered simultaneous.

<e27>

When a multitude of game loops (i.e. threads) are running concurrently, however, we can no longer easily tell which events are simultaneous with which. Besides, the presence of multiple game loops implies the presence of multiple clocks running independently, suggesting that we cannot even be sure which events belong to the "present moment" and which ones do not (because there would be more than a single frame of reference in time).

<e28>

Furthermore, depending on the order in which the game loops update themselves and which clusters of events they happen to be updating at each clock cycle, there is likely to be some kind of "propagation delay" among the events' forces of influence (due to the limit in the speed of light - the maximum rate at which information travels in space).

Such lack of simultaneity in events and their causal connections inevitably forces us to reimagine space as a fabric of causal relations (i.e. event graph), rather than a fixed coordinate system (i.e. Euclidean space) in which the notion of time can simply be expressed as an independent dimension.

This apparent lack of absolute synchronicity among events, introduced by the coexistence of multiple concurrent event-generating processes of the universe, reminds us of two analogous topics in academia, one of which belongs to computer science and the other one of which belongs to modern physics.

In computer science, the aforementioned notion of concurrency is considered the main source of many time-related algorithmic errors such as race conditions, where the evil can be attributed to the misordering of operations (which is quite common in imperative programming) which may have been caused not only by the programmer's coding mistake, but also by the lack of certainty in the speed at which the result of operation gets broadcasted from thread to thread.

In modern physics, concurrency of events and their apparent lack of ability to sync up instantly (due to the fact that their waves of influence cannot move faster than light) comprise one of the core pillars of Einstein's Theory of Relativity, where space and time can be "warped" based on the way in which events are causally connected.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Lockstep</b></h3>

In the case of a single-threaded gameplay system, though, we do not have to worry about any of the aforementioned perils of asynchronicity. As long as our Prolog program sticks its mode of operation to a single game loop, we will be safe from any of the bizarre relativistic effects such as time dilation, etc. This also means that we may imagine the game world as a simple Newtonian model of spacetime, in which the spatiotemporal location of every event can be specified in terms of Cartesian coordinates.

The only potential source of error in a single-threaded Prolog application is the very logical ambiguity in the code itself, and nothing else. As long as the rules (i.e. horn clauses) are formulated in a manner which won't allow a loophole (such as unintended reliance on the order of operation), everything is going to be fine. An example of such an undesirable loophole is demonstrated below.

#$
carryUmbrella(Person[n]) :- rainy(City[n]), resident(Person[n], City[n]).
rainy(City[n]) :- cloudy(City[n-1]).
#$

<e29>*

This scenario tells us that, whenever a city gets cloudy, it must be rainy at the next time step. This is fine so far.

It also tells us that a rainy city must instantly make its residents carry umbrellas without any time delay. But alas! The rule which enforces the carrying of umbrellas is written BEFORE the rule which updates the "rainy" status of the city, which means that, by the time the city gets tagged as "rainy", the "carryUmbrella" rule would have already been examined and ignored.

This is the kind of race condition which may occur if we do not take sufficient care when ordering horn clauses whose lefthand and righthand sides both reference the same time step (i.e. "n"). There are multiple ways of preventing this kind of error, such as:

(1) Forcing the Prolog interpreter to scan the list of rules twice instead of just once, so that the "carryUmbrella" predicate will be recognized and be activated during the second scan (because the "rainy" predicate would have been activated by the end of the first scan), or,
(2) Just writing the rules in the correct order.

Both of these solutions will work, although they both involve their own tradeoffs. The first solution allows the two horn clauses to be written in any order, yet the necessity of scanning the whole code multiple times decreases the efficiency of the program. The second solution is great for efficiency, yet it requires extra care when writing the code.

(Will be continued in {%a href="https://thingspool.net/morsels/page-13.html"%}Part 4{%/a%})










:d:This article explains how to code in Prolog to make a game. By reading this, you will learn how the data-driven (declarative) grammar of Prolog will help developers create gameplay systems which are way more robust than their imperative counterparts, such as ones written in C++, C#, or Java.
:k:Prolog, Logic Programming, Declarative Programming, Data Driven, Design Patterns, Programming Paradigm, Metacircular, Metalanguage, DSP, Signal Processing, Game Design, Game Mechanics, Game Programming, Gamedev, Game Science, Games Study, AI, Artificial Intelligence, Expert System, Game Systems, Gameplay Systems, Technical Design
:l:2024-09-04

[Game Programming in Prolog - Part 4] September 4, 2024

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Before You Read...</b></h3>

This is Part 4 of the series, "Game Programming in Prolog". In order to understand what is going on in this article, please read {%a href="https://thingspool.net/morsels/page-10.html"%}Part 1{%/a%} first.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Space</b></h3>

A playable game must have its own space and time. It is because space without time is a mere snapshot, and time without space is a mere point.

In the last article, I explained the nature of time and how its progression could be conceptualized in the language of Prolog. However, I have not explained yet how to express the concept of space in Prolog. Since spatial reasoning is an indispensable part of almost all video games (except ones that are entirely text-based), being able to construct spatial elements and let them spatially interact with one another is crucial for the design of game mechanics.

So, how to express the idea of space in Prolog? In order to achieve this goal, we must first unlearn the traditional methods in computational geometry which are only appropriate for the imperative paradigm. Then, we ought to figure out how to model space as a collection of relations such as position, proximity, distance, direction, etc.

Let us first start with a few actors, assuming that we are able to specify each actor's spawn time as well as spawn position (The notation "<x, y>" refers to a vector quantity). Suppose that there are three of them, named "actor1", "actor2", and "actor3", respectively. Their declarations are displayed in the following code.

#$
spawnTime(actor1, 0).
spawnPosition(actor1, <0, 0>).
spawnTime(actor2, 0).
spawnPosition(actor2, <1, 0>).
spawnTime(actor3, 1).
spawnPosition(actor3, <2, 0>).
#$

<e30>

In this scenario, actor1 was born at time 0 and position <0, 0>, actor2 was born at time 0 and position <1, 0>, and actor3 was born at time 1 and position <2, 0>. This allows us to locate these three actors not just in time but also in space.

Of course, the statements above only show us where they were located when they were born. If there is no additional rule, we will be forced to assume that the positions of these three actors will simply be "undefined" shortly after they were born (because there won't be any "position(...)" predicate which will correspond to future time steps).

Thus, further elaboration is needed in order to ensure their spatial persistence. The following code illustrates how it can be achieved.

#$
position(X[n], P) :- spawnTime(X, n), spawnPosition(X, P).
position(X[n], P) :- move(X[n-1], P).
position(X[n], P) :- !move(X[n-1], _), position(X[n-1], P).
#$

<e31>

The first horn clause defines the base case, which says, "If actor X is created right now at position P, its current position must be P". This makes sense, doesn't it? If something is initially placed somewhere and absolutely no time has passed since then, we must be able to assert that its current position is identical to its initial position.

The second and third horn clauses specify the two alternative cases of recursion (This is an example of the so-called "decision tree").

The second horn clause tells us that if X moved to P during the previous time step, its current position must be P. Here, the "move" relation is plays the role of a differential which gets accumulated (integrated) into X's position as time elapses. This horn clause, thus, can be thought of as an accumulator (integrator) which produces the cumulative sum of the input stream of moves.

The third horn clause defines the "fallback" condition, which tells us that X's position should stay as it was before if X did not move at all. This, together with the second clause, ensures that X's position is always defined regardless of whether X has moved during the preceding time step or not. If X moved, the second clause would be activated. If X did NOT move, the third clause would be activated instead. This is Prolog's way of implementing the IF-ELSE logic.

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Spatial Reasoning</b></h3>

This sounds reasonable so far. However, the rules above only tell us how to update an object's position based on its momentum. They do not tell us how the momentum itself shall be generated in the first place.

What makes an object move? There really are innumerable forces which may contribute to its motion, so it is probably a bit too cumbersome to list them all here. Thus, I will first begin with one of the most common means of triggering a movement in a video game - a keyboard-based character control.

The logic I am going to expound here is pretty simple. Suppose that there is a main character I want to control in a top-down 2D game. If I were to play it on a PC, I would expect myself to be able to control the character's left, right, up, and down movements by pressing the four arrow keys on the keyboard (i.e. "left arrow", "right arrow", "up arrow", and "down arrow").

In order to implement this mechanic, we ought to first let Prolog know what we mean by "left", "right", "up", and "down". The semantics of these four words are specified below.

#$
left(P1, P2) :- x(P1, X1), x(P2, X2), y(P1, Y1), y(P2, Y2), equal(Y1, Y2), add(X1, C, X2), greaterThan(C, 0).
right(P1, P2) :- x(P1, X1), x(P2, X2), y(P1, Y1), y(P2, Y2), equal(Y1, Y2), add(X2, C, X1), greaterThan(C, 0).
up(P1, P2) :- x(P1, X1), x(P2, X2), y(P1, Y1), y(P2, Y2), equal(X1, X2), add(Y1, C, Y2), greaterThan(C, 0).
down(P1, P2) :- x(P1, X1), x(P2, X2), y(P1, Y1), y(P2, Y2), equal(X1, X2), add(Y2, C, Y1), greaterThan(C, 0).
#$

<e32>

The "x(P, Xc)" relation asserts that the x-component of position P is Xc, and the "y(P, Yc)" relation asserts that the y-component of position P is Yc. This means that, in all of the four horn clauses listed above, the following definitions hold:

#$
P1 = <X1, Y1>
P2 = <X2, Y2>
#$

The idea here is to identify the type of direction which can be implied by a pair of positions: P1 and P2. For example, if it is possible to make P1 identical to P2 by adding a positive number to P1's x-component, we can say that P1 is to the left of P2. Or, if it is possible to make P2 identical to P1 by adding a positive number to P2's x-component, we can say that P1 is to the right of P2.

These four directional relations, however, are not descriptive enough to define all the spatial properties we need. For instance, they only tell us about directions; they do not involve the concept of distance whatsoever (which means that they cannot answer questions such as, "How much do I have to travel from P2 to the left to reach P1?", etc).

A slight revision ought to be made to fill out the missing information. The following code shows the same set of horn clauses demonstrated above, except that they are now equipped with the third parameter ("C") which tells us how far apart P1 is from P2.

So, for example, if you write the predicate "left(P1, pos2, c)", with the value of "pos2" and "c" specified and the value of "P1" being left as unknown, it will compute the value of P1 which is to the left of "pos2" by the distance of "c".

#$
left(P1, P2, C) :- x(P1, X1), x(P2, X2), y(P1, Y1), y(P2, Y2), equal(Y1, Y2), add(X1, C, X2), greaterThan(C, 0).
right(P1, P2, C) :- x(P1, X1), x(P2, X2), y(P1, Y1), y(P2, Y2), equal(Y1, Y2), add(X2, C, X1), greaterThan(C, 0).
up(P1, P2, C) :- x(P1, X1), x(P2, X2), y(P1, Y1), y(P2, Y2), equal(X1, X2), add(Y1, C, Y2), greaterThan(C, 0).
down(P1, P2, C) :- x(P1, X1), x(P2, X2), y(P1, Y1), y(P2, Y2), equal(X1, X2), add(Y2, C, Y1), greaterThan(C, 0).
#$

<e33>

Now that we've got all the necessary ingredients, let us devise the keyboard-based control logic. Suppose that Prolog is equipped with a special library called "IO", which provides built-in predicates for the computer's input/output signals such as "key pressed", "mouse button pressed", "mouse position", "speaker volume", and so on. "IO::keyPressed(X)", for instance, will be interpreted as TRUE whenever key X is being pressed.

With this hypothetical library in mind, we can devise motion-related control flags as predicates, like the ones written below.

#$
moveLeft(X[n]) :- player(X[n]), IO::keyPressed(IO::leftKey).
moveRight(X[n]) :- player(X[n]), IO::keyPressed(IO::rightKey).
moveUp(X[n]) :- player(X[n]), IO::keyPressed(IO::upKey).
moveDown(X[n]) :- player(X[n]), IO::keyPressed(IO::downKey).
#$

<e34>

The basic idea is that there are four different commands for the player's movement - "moveLeft", "moveRight", "moveUp", and "moveDown". Whenever "moveLeft" gets triggered, the player character (i.e. any actor which is labeled as "player") should move to the left, and whenever "moveRight" gets triggered, the player character should move to the right, and... you get the idea.

And as you can see from the above horn clauses, the keyboard's left, right, up, and down arrow keys will respectively trigger these four commands, thereby letting the player move in four different directions.

But of course, these commands are not going to do anything unless we give them specific instructions as to the actual movement of the character.

You may recall that I have previously introduced the "move(...)" predicate as means of changing the actor's current position. "move(X[n], NewPos)", for example, will generate a force which will set the position of actor "X" to the value of "NewPos" by the moment at which the time step shifts from "n" to "n+1" (i.e. when the clock ticks).

So in order to construct the movement logic, all we need to do is come up with rules which will set the value of the "move(...)" predicate to TRUE whenever the right set of conditions are satisfied. In our case, such rules can be specified as the ones listed below.

#$
move(X[n], NewPos) :- moveLeft(X[n]), position(X[n], P), left(NewPos, P, 1).
move(X[n], NewPos) :- moveRight(X[n]), position(X[n], P), right(NewPos, P, 1).
move(X[n], NewPos) :- moveUp(X[n]), position(X[n], P), up(NewPos, P, 1).
move(X[n], NewPos) :- moveDown(X[n]), position(X[n], P), down(NewPos, P, 1).
#$

<e35>*

The four horn clauses above are responsible for handling the four movement commands (i.e. "moveLeft", "moveRight", "moveUp", and "moveDown").

Let us take a look at the first one. If the "moveLeft" command is raised and actor X is currently located at P, "left(NewPos, P, 1)" will find the position "NewPos" which is located to left of P by the distance of 1 (because that's the only value of "NewPos" which makes "left(...)" true). This position, then, will be the destination of actor X in its subsequent motion. The other 3 horn clauses work the same way, just in different directions.

There is something missing here, though. The rules stated so far do let us control our player using the arrow keys, yet there is no limit when it comes to locomotive freedom. We can move the character in any way we want, which is not the kind of mechanic we would like to have in a game where the presence of movement constraint is crucial (e.g. maze escape game).

A simple way to fix this flaw is to add an extra predicate to each horn clause to check whether the destination (i.e. NewPos) is blocked by an obstacle. This check can be done by trying to find an actor which is labeled as an "obstacle" and is located at the given position. If such an actor is found, the position must be considered "blocked". The "positionBlocked(...)" predicate in the following code carries out this task, which will be used for the purpose of allowing the player's move only if the destination is NOT being blocked (i.e. "!positionBlocked(NewPos)").

#$
positionBlocked(P) :- obstacle(X[n]), position(X[n], P).

move(X[n], NewPos) :- moveLeft(X[n]), position(X[n], P), left(NewPos, P, 1), !positionBlocked(NewPos).
move(X[n], NewPos) :- moveRight(X[n]), position(X[n], P), right(NewPos, P, 1), !positionBlocked(NewPos).
move(X[n], NewPos) :- moveUp(X[n]), position(X[n], P), up(NewPos, P, 1), !positionBlocked(NewPos).
move(X[n], NewPos) :- moveDown(X[n]), position(X[n], P), down(NewPos, P, 1), !positionBlocked(NewPos).
#$

<e36>

@@<hr>
@@<div class="l_spacer"></div>
@@<h3><b>Spatial Race Conditions</b></h3>

There is something fishy in the solution I just illustrated, however. As you may have already noticed, a problem arises when the player and an obstacle happen to be moving to the same exact location at the same exact time, in which case they will overlap. This is clearly undesirable because the player is not supposed to be able to penetrate through an obstacle.

<e37>

This bug, of course, is caused by the fact that the rules are only checking the obstacle's current location. In order to prevent the player and the obstacle from colliding due to simultaneous movement, we must make sure that no obstacle is going to be present at the player's destination not only during the current time step, but also during the next time step. The following code amends the rules to meet this requirement.

#$
currOrNextPositionBlocked(P) :- obstacle(X[n]), position(X[n], P).
currOrNextPositionBlocked(P) :- obstacle(X[n]), move(X[n], P).

move(X[n], NewPos) :- moveLeft(X[n]), position(X[n], P), left(NewPos, P, 1), !currOrNextPositionBlocked(NewPos).
move(X[n], NewPos) :- moveRight(X[n]), position(X[n], P), right(NewPos, P, 1), !currOrNextPositionBlocked(NewPos).
move(X[n], NewPos) :- moveUp(X[n]), position(X[n], P), up(NewPos, P, 1), !currOrNextPositionBlocked(NewPos).
move(X[n], NewPos) :- moveDown(X[n]), position(X[n], P), down(NewPos, P, 1), !currOrNextPositionBlocked(NewPos).
#$

Still, however, we cannot fully guarantee that nothing buggy is going to occur. What if the obstacle's intention to move to the player's destination (i.e. "move(...)") gets activated AFTER the "currOrNextPositionBlocked(...)" check has already been done? In such a case, collision may ensue.

Of course, just as I have mentioned in the last article, this kind of subtlety (which is due to the order of operation) may be circumvented by letting the Prolog interpreter scan the rules twice instead of just once, etc. But again, it is computationally expensive.

A more elegant solution is to introduce an interleaving mechanism to the game loop, forcing it to update only half of the world's 2D array of positions at every even-numbered time step, and the other half at every odd-numbered time step. Each half consists of positions which are guaranteed not to immediately influence each other during the current time step. A graphical depiction of this technique is shown below.

(Side Note: I have previously suggested a similar idea in: {%a href="https://thingspool.net/software-development/page-4.html"%}Parallel Adjacent-Cell Modification Support for General-Purpose Cellular Automata{%/a%})

<e38>

And the following lines are the corresponding code implementation. Note that "mod(A, B, C)" computes C based upon the rule "A % B = C", and "xor(A, B, C)" computes C based upon the rule "A ^ B = C".

#$
inEvenTimeSlot(P) :- x(P, Px), y(P, Py), mod(Px, 2, Mx), mod(Py, 2, My), xor(Mx, My, 0).
inOddTimeSlot(P) :- x(P, Px), y(P, Py), mod(Px, 2, Mx), mod(Py, 2, My), xor(Mx, My, 1).
shouldUpdatePosition(P) :- mod(n, 2, 0), inEvenTimeSlot(P).
shouldUpdatePosition(P) :- mod(n, 2, 1), inOddTimeSlot(P).

...
position(X[n], P) :- move(X[n-1], P), position(X[n-1], PrevPos), shouldUpdatePosition(PrevPos).
#$

(Will be continued in Part 5)